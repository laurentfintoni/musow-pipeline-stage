{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- do i need a way to reconstruct tweets from available data? are there other fields I should be saving to do that? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "# For sending GET requests from the API\n",
    "import requests\n",
    "# For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "#To add wait time between requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LF token\n",
    "#os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAADLsTwEAAAAAEAceYKKDFE0DUoWANpylf37zVuA%3DN1Arq9ZoC2LNMbKYjTtcZ2pk2C5rV806OWrxPzoP7MQgoWAuz6'\n",
    "\n",
    "#Marilena token\n",
    "os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAAJgsNAEAAAAAQcsgbUnOJJmqmU483%2F8x6n9V1i8%3Df0qaEo9cV1sWP4eyNQ6E9s8BiRjvFTSN9mSqithe8uIXSNP68x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep and functions for headers, URL, endpoint connection\n",
    "\n",
    "def auth():\n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "def create_url(keyword, start_date, end_date, max_results):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source,entities',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv function\n",
    "\n",
    "def append_to_csv(json_response, fileName):\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    \n",
    "    #setup usernames via includes\n",
    "    username = {user['id']: user['username'] for user in json_response['includes']['users']}\n",
    "    \n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "\n",
    "        # 1. Username\n",
    "        author_id = tweet['author_id']\n",
    "        user = username[author_id]\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 4. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        #5. URLs \n",
    "        if ('entities' in tweet) and ('urls' in tweet['entities']):\n",
    "            for url in tweet['entities']['urls']:\n",
    "                url = [url['expanded_url'] for url in tweet['entities']['urls'] if 'twitter.com' not in url['expanded_url']]\n",
    "                url = ', '.join(url)\n",
    "        else:\n",
    "            url = \" \"\n",
    "        \n",
    "        #6. Tweet text\n",
    "        text = tweet['text'] \n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [user, created_at, lang, like_count, quote_count, reply_count, retweet_count, text, url]\n",
    "\n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1    \n",
    "    \n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_search(input_keyword, start, end, file_name, mresults, mcount):\n",
    "    bearer_token = auth()\n",
    "    headers = create_headers(bearer_token)\n",
    "    keyword = input_keyword\n",
    "    start_list = start\n",
    "\n",
    "    end_list =  end\n",
    "\n",
    "    max_results = mresults\n",
    "\n",
    "    #Total number of tweets we collected from the loop\n",
    "    total_tweets = 0\n",
    "\n",
    "    # Create file\n",
    "    csvFile = open(f'/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/TWITTER_SEARCHES/{file_name}.csv', \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "    csvWriter.writerow(['user', 'created_at', 'lang', 'like_count', 'quote_count', 'reply_count','retweet_count','tweet', 'url'])\n",
    "    csvFile.close()\n",
    "\n",
    "    for i in range(0,len(start_list)):\n",
    "\n",
    "        # Inputs\n",
    "        count = 0 # Counting tweets per time period\n",
    "        max_count = mcount # Max tweets per time period\n",
    "        flag = True\n",
    "        next_token = None\n",
    "        \n",
    "        # Check if flag is true\n",
    "        while flag:\n",
    "            # Check if max_count reached\n",
    "            if count >= max_count:\n",
    "                break\n",
    "            print(\"-------------------\")\n",
    "            print(\"Token: \", next_token)\n",
    "            url = create_url(keyword, start_list[i],end_list[i], max_results)\n",
    "            json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "            result_count = json_response['meta']['result_count']\n",
    "\n",
    "            if 'next_token' in json_response['meta']:\n",
    "                # Save the token to use for next call\n",
    "                next_token = json_response['meta']['next_token']\n",
    "                print(\"Next Token: \", next_token)\n",
    "                if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                    print(\"Start Date: \", start_list[i])\n",
    "                    append_to_csv(json_response, f'/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/TWITTER_SEARCHES/{file_name}.csv')\n",
    "                    count += result_count\n",
    "                    total_tweets += result_count\n",
    "                    print(\"Total # of Tweets added: \", total_tweets)\n",
    "                    print(\"-------------------\")\n",
    "                    time.sleep(5)                \n",
    "            # If no next token exists\n",
    "            else:\n",
    "                if result_count is not None and result_count > 0:\n",
    "                    print(\"-------------------\")\n",
    "                    print(\"Start Date: \", start_list[i])\n",
    "                    append_to_csv(json_response, f'/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/TWITTER_SEARCHES/{file_name}.csv')\n",
    "                    count += result_count\n",
    "                    total_tweets += result_count\n",
    "                    print(\"Total # of Tweets added: \", total_tweets)\n",
    "                    print(\"-------------------\")\n",
    "                    time.sleep(5)\n",
    "                \n",
    "                #Since this is the final request, turn flag to false to move to the next time period.\n",
    "                flag = False\n",
    "                next_token = None\n",
    "            time.sleep(5)\n",
    "    print(\"Total number of results: \", total_tweets)\n",
    "    #csv to pickle, remove all entries w/ no url, remove duplicate urls \n",
    "    pickle_df = pd.read_csv(f'/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/TWITTER_SEARCHES/{file_name}.csv', keep_default_na=False, dtype={\"user\": \"string\", \"lang\": \"string\", \"tweet\": \"string\", \"url\": \"string\"})\n",
    "    pickle_df = pickle_df[pickle_df.url != ' ']\n",
    "    pickle_df = pickle_df.drop_duplicates(['url'], keep='last')\n",
    "    pickle_df.to_pickle(f'/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/TWITTER_SEARCHES/{file_name}.pkl')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_search(\"\\\"music history\\\" -is:retweet\", ['2021-01-01T00:00:00.000Z', '2021-02-01T00:00:00.000Z', '2021-03-01T00:00:00.000Z', '2021-04-01T00:00:00.000Z', '2021-05-01T00:00:00.000Z', '2021-06-01T00:00:00.000Z', '2021-07-01T00:00:00.000Z', '2021-08-01T00:00:00.000Z', '2021-09-01T00:00:00.000Z', '2021-10-01T00:00:00.000Z', '2021-11-01T00:00:00.000Z', '2021-12-01T00:00:00.000Z'], ['2021-01-31T00:00:00.000Z', '2021-02-28T00:00:00.000Z', '2021-03-31T00:00:00.000Z', '2021-04-30T00:00:00.000Z', '2021-05-31T00:00:00.000Z', '2021-06-30T00:00:00.000Z', '2021-07-31T00:00:00.000Z', '2021-08-31T00:00:00.000Z', '2021-09-30T00:00:00.000Z', '2021-10-31T00:00:00.000Z', '2021-11-30T00:00:00.000Z', '2021-12-31T00:00:00.000Z'], 'test', 100, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNFINISHED/TEST STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs for request\n",
    "\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "keyword = \"\\\"music oral history\\\" -is:retweet\"\n",
    "start_time = \"2021-01-01T00:00:00.000Z\"\n",
    "end_time = \"2021-06-30T23:59:59.000Z\"\n",
    "max_results = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test response\n",
    "url = create_url(keyword, start_time, end_time, max_results)\n",
    "json_response = connect_to_endpoint(url[0], headers, url[1])\n",
    "print(json.dumps(json_response, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe function\n",
    "\n",
    "def append_to_pd(json):\n",
    "    \n",
    "    #username setup\n",
    "    username = {user['id']: user['username'] for user in json_response['includes']['users']}\n",
    "    \n",
    "    #1. username\n",
    "    author_id = [tweet['author_id'] for tweet in json['data']]\n",
    "    user = [username[id] for id in author_id]\n",
    "\n",
    "    # 2. Time created\n",
    "    created_at = [dateutil.parser.parse(tweet['created_at']) for tweet in json['data']]\n",
    "\n",
    "    # 3. Language\n",
    "    lang = [tweet['lang'] for tweet in json_response['data']]\n",
    "\n",
    "    # 4. Tweet metrics\n",
    "    retweet_count = [tweet['public_metrics']['retweet_count'] for tweet in json['data']]\n",
    "    reply_count = [tweet['public_metrics']['reply_count'] for tweet in json['data']]\n",
    "    like_count = [tweet['public_metrics']['like_count'] for tweet in json['data']]\n",
    "    quote_count = [tweet['public_metrics']['quote_count'] for tweet in json['data']]\n",
    "\n",
    "    # 5. Tweet text\n",
    "    text = [tweet['text'] for tweet in json['data']]\n",
    "    \n",
    "    # 6. URL \n",
    "    url = []\n",
    "    for tweet in json['data']:\n",
    "        if ('entities' in tweet) and ('urls' in tweet['entities']):\n",
    "            for link in tweet['entities']['urls']:\n",
    "                url.append(link['expanded_url'])\n",
    "        else:\n",
    "            url.append('')\n",
    "    \n",
    "    # Create df and append everything \n",
    "    dataframe = pd.DataFrame(columns=['User', 'Created', 'Language', 'Likes', 'Quotes', 'Replies', 'RTs', 'Tweet', 'URL'])\n",
    "    dataframe['User'] = pd.Series(user).astype('string')\n",
    "    dataframe['Created'] = pd.Series(created_at)\n",
    "    dataframe['Language'] = pd.Series(lang).astype('string')\n",
    "    dataframe['Likes'] = pd.Series(like_count)\n",
    "    dataframe['Quotes'] = pd.Series(quote_count)\n",
    "    dataframe['Replies'] = pd.Series(reply_count)\n",
    "    dataframe['RTs'] = pd.Series(retweet_count)\n",
    "    dataframe['Tweet'] = pd.Series(text).astype('string')\n",
    "    dataframe['URL'] = pd.Series(url).astype('string')   \n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch tweets over period and output to csv \n",
    "\n",
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "keyword = \"\\\"cultural heritage\\\" -is:retweet\"\n",
    "start_list =    ['2021-01-01T00:00:00.000Z',\n",
    "                 '2021-02-01T00:00:00.000Z',\n",
    "                 '2021-03-01T00:00:00.000Z',\n",
    "                 '2021-04-01T00:00:00.000Z',\n",
    "                 '2021-05-01T00:00:00.000Z',\n",
    "                 '2021-06-01T00:00:00.000Z']\n",
    "\n",
    "end_list =      ['2021-01-31T23:59:59.000Z',\n",
    "                 '2021-02-28T23:59:59.000Z',\n",
    "                 '2021-03-31T23:59:59.000Z',\n",
    "                 '2021-04-30T23:59:59.000Z',\n",
    "                 '2021-05-31T23:59:59.000Z',\n",
    "                 '2021-06-30T23:59:59.000Z']\n",
    "\n",
    "max_results = 500\n",
    "\n",
    "#Total number of tweets we collected from the loop\n",
    "total_tweets = 0\n",
    "\n",
    "# Create file\n",
    "csvFile = open(\"data.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "#Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "csvWriter.writerow(['user', 'created_at', 'lang', 'like_count', 'quote_count', 'reply_count','retweet_count','tweet', 'url'])\n",
    "csvFile.close()\n",
    "\n",
    "for i in range(0,len(start_list)):\n",
    "\n",
    "    # Inputs\n",
    "    count = 0 # Counting tweets per time period\n",
    "    max_count = 1000 # Max tweets per time period\n",
    "    flag = True\n",
    "    next_token = None\n",
    "    \n",
    "    # Check if flag is true\n",
    "    while flag:\n",
    "        # Check if max_count reached\n",
    "        if count >= max_count:\n",
    "            break\n",
    "        print(\"-------------------\")\n",
    "        print(\"Token: \", next_token)\n",
    "        url = create_url(keyword, start_list[i],end_list[i], max_results)\n",
    "        json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "        result_count = json_response['meta']['result_count']\n",
    "\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            # Save the token to use for next call\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            print(\"Next Token: \", next_token)\n",
    "            if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                print(\"Start Date: \", start_list[i])\n",
    "                append_to_csv(json_response, \"data.csv\")\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(5)                \n",
    "        # If no next token exists\n",
    "        else:\n",
    "            if result_count is not None and result_count > 0:\n",
    "                print(\"-------------------\")\n",
    "                print(\"Start Date: \", start_list[i])\n",
    "                append_to_csv(json_response, \"data.csv\")\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(5)\n",
    "            \n",
    "            #Since this is the final request, turn flag to false to move to the next time period.\n",
    "            flag = False\n",
    "            next_token = None\n",
    "        time.sleep(5)\n",
    "print(\"Total number of results: \", total_tweets)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
