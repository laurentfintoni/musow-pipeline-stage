{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- supervised ce? \n",
    "- replicate w/ twitter dfs? \n",
    "- try different types of scoring in logreg? different test sizes?\n",
    "- export model and reuse w/ predictions (use additions to test against base set)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports + path\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "import pickle\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from pandas_profiling import ProfileReport\n",
    "path = '/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: \n",
    "- create DFs for trainin set using musoW and MJI spreadsheets\n",
    "- two versions: a base one and one w/ additions from ismir dataset (for musoW) and from additional research (for MJI)\n",
    "- the baseline difference we want to train the LogReg model on is that musoW is focused on music archives w/ datasets while MJI is focused on all sorts of music archives, regardless of dataset inclusion\n",
    "- to avoid polluting the two sets we check for duplicates at all possible stages to ensure that the MJI set does not have anything that currently exists in musoW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read mji csv and grab needed columns\n",
    "df_mji = pd.read_csv(path+'MJI/MJI_data.csv', keep_default_na=False, dtype='string')\n",
    "df_mji_small = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "df_mji_small['Title'] = df_mji['Title'].str.lower().str.strip()\n",
    "df_mji_small['Description'] = df_mji['Description'].str.lower().str.strip()\n",
    "df_mji_small['URL'] = df_mji['URL'].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read musow json dump and grab needed columns\n",
    "with open(path+'MUSOW/musow_name_desc_url_cat.json') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "musow_names = [result['name']['value'].strip().lower() for result in data['results']['bindings']]\n",
    "musow_desc = [result['description']['value'].strip().lower() for result in data['results']['bindings']]\n",
    "musow_url = [result['url']['value'].strip().lower() for result in data['results']['bindings']]\n",
    "df_musow = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "df_musow['Title'] = musow_names\n",
    "df_musow['Description'] = musow_desc\n",
    "df_musow['URL'] = musow_url\n",
    "df_musow = df_musow.astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove musow duplicates from MJI set \n",
    "mji_training_set = df_mji_small[~df_mji_small['Title'].isin(df_musow['Title'])].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create positive and negative sets w/o additions, add target column \n",
    "positive_df = df_musow.copy()\n",
    "positive_df['Target'] = '1'\n",
    "negative_df = mji_training_set.copy()\n",
    "negative_df['Target'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create positive and negative sets w/ additions, add target column \n",
    "ismir_df = pd.read_pickle(path+'GH_PICKLES/ismir.pkl')\n",
    "ismir_df = ismir_df[~ismir_df['Title'].isin(df_musow['Title'])].dropna() \n",
    "positive_df_adds = pd.concat([df_musow, ismir_df]).reset_index(drop=True)\n",
    "positive_df_adds = positive_df_adds.drop_duplicates(['Title'], keep='last')\n",
    "positive_df_adds['Target'] = '1'\n",
    "mji_additions_1 = pd.read_csv(path+'MJI/MJI_additions_for_LR.csv')\n",
    "mji_additions_1['Title'] = mji_additions_1['Title'].str.lower().str.strip()\n",
    "mji_additions_1['Description'] = mji_additions_1['Description'].str.lower().str.strip()\n",
    "mji_additions_1['URL'] = mji_additions_1['URL'].str.lower().str.strip()\n",
    "mji_additions_1 = mji_additions_1[~mji_additions_1['Title'].isin(df_musow['Title'])].dropna()\n",
    "mji_additions_1 = mji_additions_1[~mji_additions_1['Title'].isin(mji_training_set['Title'])].dropna()\n",
    "negative_df_adds = pd.concat([mji_training_set, mji_additions_1]).reset_index(drop=True)\n",
    "negative_df_adds = negative_df_adds.drop_duplicates(['Title'], keep='last')\n",
    "negative_df_adds['Target'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a prediction set for later w/ only additions\n",
    "#ismir_df['Target'] = '1'\n",
    "#mji_additions_1['Target'] = '0'\n",
    "prediction_set = pd.concat([ismir_df, mji_additions_1]).reset_index(drop=True)\n",
    "prediction_set.to_pickle(path+'LOGREG_RELEVANCE/base_prediction_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge both sets into one, create two training sets (one base, one extended w/ additional data), pickle for reuse\n",
    "training_set = pd.concat([positive_df, negative_df])\n",
    "training_set['Target'] = training_set['Target'].astype('int')\n",
    "training_set = training_set.reset_index(drop=True)\n",
    "training_set_adds = pd.concat([positive_df_adds, negative_df_adds])\n",
    "training_set_adds['Target'] = training_set_adds['Target'].astype('int')\n",
    "training_set_adds = training_set_adds.reset_index(drop=True)\n",
    "training_set.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset.pkl')\n",
    "training_set_adds.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_extended.pkl')\n",
    "\n",
    "#random sample positive df to same length as negative, create alt extended set w/ even balance, pickle \n",
    "positive_df_adds_2 = positive_df_adds.sample(n=267, random_state=1)\n",
    "training_set_even = pd.concat([positive_df_adds_2, negative_df_adds])\n",
    "training_set_even['Target'] = training_set_even['Target'].astype('int')\n",
    "training_set_even = training_set_even.reset_index(drop=True)\n",
    "training_set_even.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_even.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create another version of base/extended sets with combined features, pickle for reuse\n",
    "\n",
    "training_set_comb = training_set.copy()\n",
    "training_set_adds_comb = training_set_adds.copy()\n",
    "training_set_comb_even = training_set_even.copy()\n",
    "\n",
    "#create combined columns for desc+headline and desc+headline_url\n",
    "def tokenize_url(url:str):\n",
    "    url=url.replace(\"https\",\"\")\n",
    "    url=url.replace(\"http\",\"\")\n",
    "    url=url.replace(\"www\",\"\")   \n",
    "    url=re.sub(\"(\\W|_)+\",\" \",url)\n",
    "    return url\n",
    "\n",
    "#create tokenized URL field\n",
    "training_set_comb['tokenized_url']=training_set_comb['URL'].apply(lambda x:tokenize_url(x))\n",
    "#description + tokenized url\n",
    "training_set_comb['text_desc_headline_url'] = training_set_comb['Description'] + ' '+ training_set_comb['Title']+\" \" + training_set_comb['tokenized_url']\n",
    "training_set_comb.drop(['tokenized_url', 'Title', 'Description', 'URL'], inplace=True, axis=1)\n",
    "\n",
    "#create tokenized URL field\n",
    "training_set_adds_comb['tokenized_url']=training_set_adds_comb['URL'].apply(lambda x:tokenize_url(x))\n",
    "#description + tokenized url\n",
    "training_set_adds_comb['text_desc_headline_url'] = training_set_adds_comb['Description'] + ' '+ training_set_adds_comb['Title']+\" \" + training_set_adds_comb['tokenized_url']\n",
    "training_set_adds_comb.drop(['tokenized_url', 'Title', 'Description', 'URL'], inplace=True, axis=1)\n",
    "\n",
    "#same for even set\n",
    "#create tokenized URL field\n",
    "training_set_comb_even['tokenized_url']=training_set_comb_even['URL'].apply(lambda x:tokenize_url(x))\n",
    "#description + tokenized url\n",
    "training_set_comb_even['text_desc_headline_url'] = training_set_comb_even['Description'] + ' '+ training_set_comb_even['Title']+\" \" + training_set_comb_even['tokenized_url']\n",
    "training_set_comb_even.drop(['tokenized_url', 'Title', 'Description', 'URL'], inplace=True, axis=1)\n",
    "\n",
    "training_set_comb.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_comb.pkl')\n",
    "training_set_adds_comb.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_ext_comb.pkl')\n",
    "training_set_comb_even.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_even_comb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print some base stats to keep track of changes: training set size, avg length of description (main feature)\n",
    "\n",
    "print('Size of base set:', len(training_set.index), '\\nMean length of description in base set is:', training_set['Description'].str.len().mean(), '\\nSize of extended set:', len(training_set_adds.index), '\\nMean length of description in extended set is:', training_set_adds['Description'].str.len().mean(), '\\nSize of even set:', len(training_set_even.index), '\\nMean length of description in even set is:', training_set_even['Description'].str.len().mean())\n",
    "\n",
    "#print some base stats to keep track of changes: avg length of combined desc+title+url (main feature)\n",
    "\n",
    "print('\\nMean length of description in base set combined is:', training_set_comb['text_desc_headline_url'].str.len().mean(), '\\nMean length of description in extended set combined is:', training_set_adds_comb['text_desc_headline_url'].str.len().mean(), '\\nMean length of description in even set combined is:', training_set_comb_even['text_desc_headline_url'].str.len().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2:\n",
    "- Try different unsupervised encoding approaches for categorical variables including OrdinalEncoder, Label Encoding, Tf-Idf, BackwardDifferenceEncoder \n",
    "- Ordinal and Label encoding use title, desc, and url + combined title/desc and title/desc/url as features depending on source version of training set\n",
    "- BackwardDifference uses title, desc, and url + separated title/desc and title/desc/url as features (in order to see if there's any diff btw the two different combination of strings approach)\n",
    "- Tf-Idf uses only desc, title/desc, and title/desc/url as features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode categorial variables w/ OrdinalEncoder, pickle\n",
    "ord_enc = OrdinalEncoder()\n",
    "\n",
    "#base set \n",
    "training_set_ordinal = training_set.copy()\n",
    "training_set_ordinal[\"Title_encoded\"] = ord_enc.fit_transform(training_set_ordinal[[\"Title\"]])\n",
    "training_set_ordinal[\"Desc_encoded\"] = ord_enc.fit_transform(training_set_ordinal[[\"Description\"]])\n",
    "training_set_ordinal[\"URL_encoded\"] = ord_enc.fit_transform(training_set_ordinal[[\"URL\"]])\n",
    "training_set_ordinal.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_ordinal.pkl')\n",
    "\n",
    "#base set comb\n",
    "training_set_ordinal_comb = training_set_comb.copy()\n",
    "training_set_ordinal_comb[\"text_desc_headline_url_encoded\"] = ord_enc.fit_transform(training_set_ordinal_comb[[\"text_desc_headline_url\"]])\n",
    "training_set_ordinal_comb.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_comb_ordinal.pkl')\n",
    "\n",
    "#extended set \n",
    "training_set_ordinal_adds = training_set_adds.copy()\n",
    "training_set_ordinal_adds[\"Title_encoded\"] = ord_enc.fit_transform(training_set_ordinal_adds[[\"Title\"]])\n",
    "training_set_ordinal_adds[\"Desc_encoded\"] = ord_enc.fit_transform(training_set_ordinal_adds[[\"Description\"]])\n",
    "training_set_ordinal_adds[\"URL_encoded\"] = ord_enc.fit_transform(training_set_ordinal_adds[[\"URL\"]])\n",
    "training_set_ordinal_adds.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_extended_ordinal.pkl')\n",
    "\n",
    "#extended set comb \n",
    "training_set_ordinal_adds_comb = training_set_adds_comb.copy()\n",
    "training_set_ordinal_adds_comb[\"text_desc_headline_url_encoded\"] = ord_enc.fit_transform(training_set_ordinal_adds_comb[[\"text_desc_headline_url\"]])\n",
    "training_set_ordinal_adds_comb.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_extended_comb_ordinal.pkl')\n",
    "\n",
    "#even set \n",
    "training_set_ordinal_even = training_set_even.copy()\n",
    "training_set_ordinal_even[\"Title_encoded\"] = ord_enc.fit_transform(training_set_ordinal_even[[\"Title\"]])\n",
    "training_set_ordinal_even[\"Desc_encoded\"] = ord_enc.fit_transform(training_set_ordinal_even[[\"Description\"]])\n",
    "training_set_ordinal_even[\"URL_encoded\"] = ord_enc.fit_transform(training_set_ordinal_even[[\"URL\"]])\n",
    "training_set_ordinal_even.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_even_ordinal.pkl')\n",
    "\n",
    "#even set comb \n",
    "training_set_ordinal_even_comb = training_set_comb_even.copy()\n",
    "training_set_ordinal_even_comb[\"text_desc_headline_url_encoded\"] = ord_enc.fit_transform(training_set_ordinal_even_comb[[\"text_desc_headline_url\"]])\n",
    "training_set_ordinal_even_comb.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_even_comb_ordinal.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode cat variables with label encoding, pickle \n",
    "\n",
    "#base set \n",
    "training_set_label = training_set.copy()\n",
    "training_set_label[\"Title\"] = training_set_label[\"Title\"].astype('category')\n",
    "training_set_label[\"Description\"] = training_set_label[\"Description\"].astype('category')\n",
    "training_set_label[\"URL\"] = training_set_label[\"URL\"].astype('category')\n",
    "training_set_label[\"Title_cat\"] = training_set_label[\"Title\"].cat.codes\n",
    "training_set_label[\"Desc_cat\"] = training_set_label[\"Description\"].cat.codes\n",
    "training_set_label[\"URL_cat\"] = training_set_label[\"URL\"].cat.codes\n",
    "training_set_label.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_label.pkl')\n",
    "\n",
    "#base set comb\n",
    "training_set_label_comb = training_set_comb.copy()\n",
    "training_set_label_comb[\"text_desc_headline_url\"] = training_set_label_comb[\"text_desc_headline_url\"].astype('category')\n",
    "training_set_label_comb[\"text_desc_headline_url_cat\"] = training_set_label_comb[\"text_desc_headline_url\"].cat.codes\n",
    "training_set_label_comb.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_comb_label.pkl')\n",
    "\n",
    "#extended set \n",
    "training_set_label_adds = training_set_adds.copy()\n",
    "training_set_label_adds[\"Title\"] = training_set_label_adds[\"Title\"].astype('category')\n",
    "training_set_label_adds[\"Description\"] = training_set_label_adds[\"Description\"].astype('category')\n",
    "training_set_label_adds[\"URL\"] = training_set_label_adds[\"URL\"].astype('category')\n",
    "training_set_label_adds[\"Title_cat\"] = training_set_label_adds[\"Title\"].cat.codes\n",
    "training_set_label_adds[\"Desc_cat\"] = training_set_label_adds[\"Description\"].cat.codes\n",
    "training_set_label_adds[\"URL_cat\"] = training_set_label_adds[\"URL\"].cat.codes\n",
    "training_set_label_adds.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_extended_label.pkl')\n",
    "\n",
    "#extended set comb\n",
    "training_set_label_adds_comb = training_set_adds_comb.copy()\n",
    "training_set_label_adds_comb[\"text_desc_headline_url\"] = training_set_label_adds_comb[\"text_desc_headline_url\"].astype('category')\n",
    "training_set_label_adds_comb[\"text_desc_headline_url_cat\"] = training_set_label_adds_comb[\"text_desc_headline_url\"].cat.codes\n",
    "training_set_label_adds_comb.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_extended_comb_label.pkl')\n",
    "\n",
    "#even set \n",
    "training_set_label_even = training_set_even.copy()\n",
    "training_set_label_even[\"Title\"] = training_set_label_even[\"Title\"].astype('category')\n",
    "training_set_label_even[\"Description\"] = training_set_label_even[\"Description\"].astype('category')\n",
    "training_set_label_even[\"URL\"] = training_set_label_even[\"URL\"].astype('category')\n",
    "training_set_label_even[\"Title_cat\"] = training_set_label_even[\"Title\"].cat.codes\n",
    "training_set_label_even[\"Desc_cat\"] = training_set_label_even[\"Description\"].cat.codes\n",
    "training_set_label_even[\"URL_cat\"] = training_set_label_even[\"URL\"].cat.codes\n",
    "training_set_label_even.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_even_label.pkl')\n",
    "\n",
    "#even set comb \n",
    "training_set_label_even_comb = training_set_comb_even.copy()\n",
    "training_set_label_even_comb[\"text_desc_headline_url\"] = training_set_label_even_comb[\"text_desc_headline_url\"].astype('category')\n",
    "training_set_label_even_comb[\"text_desc_headline_url_cat\"] = training_set_label_even_comb[\"text_desc_headline_url\"].cat.codes\n",
    "training_set_label_even_comb.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_even_comb_label.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode w/ CE's BackwardDifferenceEncoder, pickle \n",
    "\n",
    "#base set \n",
    "training_set_ce = training_set.copy()\n",
    "# Specify the columns to encode then fit and transform\n",
    "encoder_base = ce.BackwardDifferenceEncoder(cols=['Title', 'Description', 'URL'])\n",
    "training_set_ce = encoder_base.fit_transform(training_set_ce, verbose=1)\n",
    "training_set_ce.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_ce.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_BD.pkl')\n",
    "\n",
    "#base set desc only\n",
    "training_set_ce_desc = training_set.copy()\n",
    "encoder_base_desc = ce.BackwardDifferenceEncoder(cols=['Description'])\n",
    "training_set_ce_desc = encoder_base_desc.fit_transform(training_set_ce_desc, verbose=1)\n",
    "training_set_ce_desc.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_ce_desc.to_pickle(path+'LOGREG_RELEVANCE/desc_trainingset_BD_encode.pkl')\n",
    "\n",
    "#extended set \n",
    "training_set_adds_ce = training_set_adds.copy()\n",
    "training_set_adds_ce = encoder_base.fit_transform(training_set_adds_ce, verbose=1)\n",
    "training_set_adds_ce.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_adds_ce.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_extended_BD.pkl')\n",
    "\n",
    "#extended set desc only \n",
    "training_set_adds_ce_desc = training_set_adds.copy()\n",
    "training_set_adds_ce_desc = encoder_base_desc.fit_transform(training_set_adds_ce_desc, verbose=1)\n",
    "training_set_adds_ce_desc.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_adds_ce_desc.to_pickle(path+'LOGREG_RELEVANCE/desc_trainingset_extended_BD.pkl')\n",
    "\n",
    "#base set comb\n",
    "training_set_comb_ce = training_set_comb.copy()\n",
    "encoder_comb = ce.BackwardDifferenceEncoder(cols=['text_desc_headline_url'])\n",
    "training_set_comb_ce = encoder_comb.fit_transform(training_set_comb_ce, verbose=1)\n",
    "training_set_comb_ce.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_comb_ce.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_comb_BD.pkl')\n",
    "\n",
    "#extended set comb\n",
    "training_set_adds_comb_ce = training_set_adds_comb.copy()\n",
    "training_set_adds_comb_ce = encoder_comb.fit_transform(training_set_adds_comb_ce, verbose=1)\n",
    "training_set_adds_comb_ce.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_adds_comb_ce.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_extended_comb_BD.pkl')\n",
    "\n",
    "#even set \n",
    "training_set_ce_even = training_set_even.copy()\n",
    "training_set_ce_even = encoder_base.fit_transform(training_set_ce_even, verbose=1)\n",
    "training_set_ce_even.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_ce_even.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_even_BD.pkl')\n",
    "\n",
    "#even set comb\n",
    "training_set_ce_even_comb = training_set_comb_even.copy()\n",
    "training_set_ce_even_comb = encoder_comb.fit_transform(training_set_ce_even_comb, verbose=1)\n",
    "training_set_ce_even_comb.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_ce_even_comb.to_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_even_comb_BD.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD PICKLE READING CHUNK HERE TO NOT RERUN ALL ABOVE CODE\n",
    "#base sets \n",
    "training_set = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset.pkl')\n",
    "training_set_adds = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_extended.pkl')\n",
    "training_set_even = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_even.pkl')\n",
    "training_set_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_comb.pkl')\n",
    "training_set_adds_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_ext_comb.pkl')\n",
    "training_set_comb_even = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_even_comb.pkl')\n",
    "\n",
    "#ordinal sets \n",
    "training_set_ordinal = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_ordinal.pkl')\n",
    "training_set_ordinal_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_comb_ordinal.pkl')\n",
    "training_set_ordinal_adds = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_extended_ordinal.pkl')\n",
    "training_set_ordinal_adds_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_extended_comb_ordinal.pkl')\n",
    "training_set_ordinal_even = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_even_ordinal.pkl')\n",
    "training_set_ordinal_even_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_even_comb_ordinal.pkl')\n",
    "\n",
    "#label sets \n",
    "training_set_label = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_label.pkl')\n",
    "training_set_label_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_comb_label.pkl')\n",
    "training_set_label_adds = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_extended_label.pkl')\n",
    "training_set_label_adds_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_extended_comb_label.pkl')\n",
    "training_set_label_even = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_even_label.pkl')\n",
    "training_set_label_even_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_even_comb_label.pkl')\n",
    "\n",
    "#BD sets \n",
    "training_set_ce = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_BD.pkl')\n",
    "training_set_ce_desc = pd.read_pickle(path+'LOGREG_RELEVANCE/desc_trainingset_BD_encode.pkl')\n",
    "training_set_adds_ce = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_extended_BD.pkl')\n",
    "training_set_adds_ce_desc = pd.read_pickle(path+'LOGREG_RELEVANCE/desc_trainingset_extended_BD.pkl')\n",
    "training_set_comb_ce = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_comb_BD.pkl')\n",
    "training_set_adds_comb_ce = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_extended_comb_BD.pkl')\n",
    "training_set_ce_even = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_even_BD.pkl')\n",
    "training_set_ce_even_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/title_desc_url_trainingset_even_comb_BD.pkl')\n",
    "\n",
    "#prediction set\n",
    "prediction_set = pd.read_pickle(path+'LOGREG_RELEVANCE/base_prediction_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode w/ tf-idf\n",
    "\n",
    "#create vectorizers \n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True, max_df=0.95)\n",
    "\n",
    "#base set description \n",
    "tfidf_vectorizer.fit_transform(training_set['Description'].values)\n",
    "training_set_tfidf = tfidf_vectorizer.transform(training_set['Description'].values)\n",
    "\n",
    "#extended set description \n",
    "tfidf_vectorizer.fit_transform(training_set_adds['Description'].values)\n",
    "training_set_adds_tfidf = tfidf_vectorizer.transform(training_set_adds['Description'].values)\n",
    "\n",
    "#even set description \n",
    "tfidf_vectorizer.fit_transform(training_set_even['Description'].values)\n",
    "training_set_tfidf_even = tfidf_vectorizer.transform(training_set_even['Description'].values)\n",
    "\n",
    "#base set comb\n",
    "tfidf_vectorizer.fit_transform(training_set_comb['text_desc_headline_url'].values)\n",
    "training_set_comb_tfidf = tfidf_vectorizer.transform(training_set_comb['text_desc_headline_url'].values)\n",
    "\n",
    "#extended set comb\n",
    "tfidf_vectorizer.fit_transform(training_set_adds_comb['text_desc_headline_url'].values)\n",
    "training_set_adds_comb_tfidf = tfidf_vectorizer.transform(training_set_adds_comb['text_desc_headline_url'].values)\n",
    "\n",
    "#even set comb\n",
    "tfidf_vectorizer.fit_transform(training_set_comb_even['text_desc_headline_url'].values)\n",
    "training_set_even_comb_tfidf = tfidf_vectorizer.transform(training_set_comb_even['text_desc_headline_url'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf encoding w/ count vect \n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "#base set\n",
    "base_train_counts = count_vect.fit_transform(training_set['Description'])\n",
    "training_set_tfidf = tfidf_transformer.fit_transform(base_train_counts)\n",
    "\n",
    "#extended set description \n",
    "extended_train_counts = count_vect.fit_transform(training_set_adds['Description'])\n",
    "training_set_adds_tfidf = tfidf_transformer.fit_transform(extended_train_counts)\n",
    "\n",
    "#even set description \n",
    "even_train_counts = count_vect.fit_transform(training_set_even['Description'])\n",
    "training_set_tfidf_even = tfidf_transformer.fit_transform(even_train_counts)\n",
    "\n",
    "#base set comb\n",
    "base_comb_train_counts = count_vect.fit_transform(training_set_comb['text_desc_headline_url'])\n",
    "training_set_comb_tfidf = tfidf_transformer.fit_transform(base_comb_train_counts)\n",
    "\n",
    "#extended set comb\n",
    "extended_comb_train_counts = count_vect.fit_transform(training_set_adds_comb['text_desc_headline_url'])\n",
    "training_set_adds_comb_tfidf = tfidf_transformer.fit_transform(extended_comb_train_counts)\n",
    "\n",
    "#even set comb\n",
    "even_comb_train_counts = count_vect.fit_transform(training_set_comb_even['text_desc_headline_url'])\n",
    "training_set_even_comb_tfidf = tfidf_transformer.fit_transform(even_comb_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set features and targets for all encoding types \n",
    "\n",
    "#ORDINAL  \n",
    "features_ord = ['Title_encoded', 'Desc_encoded', 'URL_encoded']\n",
    "features_ord_comb = ['text_desc_headline_url_encoded']\n",
    "\n",
    "#base set\n",
    "x_ord = training_set_ordinal[features_ord] # Features\n",
    "y_ord = training_set_ordinal.Target # Target variable\n",
    "\n",
    "#base set desc only \n",
    "training_set_ordinal_desc = training_set_ordinal.copy()\n",
    "training_set_ordinal_desc.drop(['Title', 'URL', 'Title_encoded', 'Description', 'URL_encoded'], inplace=True, axis=1)\n",
    "single_feat = ['Desc_encoded']\n",
    "x_ord_desc = training_set_ordinal_desc[single_feat] # Features\n",
    "y_ord_desc = training_set_ordinal_desc.Target # Target variable\n",
    "\n",
    "#extended set \n",
    "x_ord_adds = training_set_ordinal_adds[features_ord] # Features\n",
    "y_ord_adds = training_set_ordinal_adds.Target # Target variable\n",
    "\n",
    "#extended set desc only\n",
    "training_set_ordinal_adds_desc = training_set_ordinal_adds.copy()\n",
    "training_set_ordinal_adds_desc.drop(['Title', 'URL', 'Title_encoded', 'Description', 'URL_encoded'], inplace=True, axis=1)\n",
    "x_ord_adds_desc = training_set_ordinal_adds_desc[single_feat] # Features\n",
    "y_ord_adds_desc = training_set_ordinal_adds_desc.Target # Target variable\n",
    "\n",
    "#base set comb \n",
    "x_ord_comb = training_set_ordinal_comb[features_ord_comb] # Features\n",
    "y_ord_comb = training_set_ordinal_comb.Target # Target variable\n",
    "\n",
    "#extended set comb \n",
    "x_ord_adds_comb = training_set_ordinal_adds_comb[features_ord_comb] # Features\n",
    "y_ord_adds_comb = training_set_ordinal_adds_comb.Target # Target variable\n",
    "\n",
    "#even set \n",
    "x_ord_even = training_set_ordinal_even[features_ord] # Features\n",
    "y_ord_even = training_set_ordinal_even.Target # Target variable\n",
    "\n",
    "#even set comb\n",
    "x_ord_even_comb = training_set_ordinal_even_comb[features_ord_comb] # Features\n",
    "y_ord_even_comb = training_set_ordinal_even_comb.Target # Target variable\n",
    "\n",
    "#LABEL \n",
    "features_label = ['Title_cat', 'Desc_cat', 'URL_cat']\n",
    "features_label_comb = ['text_desc_headline_url_cat']\n",
    "\n",
    "#base set\n",
    "x_label = training_set_label[features_label] # Features\n",
    "y_label = training_set_label.Target # Target variable\n",
    "\n",
    "#base set desc only \n",
    "training_set_label_2 = training_set_label.copy()\n",
    "training_set_label_2.drop(['Title', 'Description', 'URL', 'Title_cat', 'URL_cat'], inplace=True, axis=1)\n",
    "single_feat_label = ['Desc_cat']\n",
    "x_label_desc = training_set_label_2[single_feat_label] # Features\n",
    "y_label_desc = training_set_label_2.Target # Target variable\n",
    "\n",
    "#extended set\n",
    "x_label_adds = training_set_label_adds[features_label] # Features\n",
    "y_label_adds = training_set_label_adds.Target # Target variable\n",
    "\n",
    "#extended set desc only \n",
    "training_set_label_adds_2 = training_set_label_adds.copy()\n",
    "training_set_label_adds_2.drop(['Title', 'Description', 'URL', 'Title_cat', 'URL_cat'], inplace=True, axis=1)\n",
    "x_label_adds_desc = training_set_label_adds_2[single_feat_label] # Features\n",
    "y_label_adds_desc = training_set_label_adds_2.Target # Target variable\n",
    "\n",
    "#base set comb\n",
    "x_label_comb = training_set_label_comb[features_label_comb] # Features\n",
    "y_label_comb = training_set_label_comb.Target # Target variable\n",
    "\n",
    "#extended set comb \n",
    "x_label_adds_comb = training_set_label_adds_comb[features_label_comb] # Features\n",
    "y_label_adds_comb = training_set_label_adds_comb.Target # Target variable\n",
    "\n",
    "#even set\n",
    "x_label_even = training_set_label_even[features_label] # Features\n",
    "y_label_even = training_set_label_even.Target # Target variable\n",
    "\n",
    "#even set comb \n",
    "x_label_even_comb = training_set_label_even_comb[features_label_comb] # Features\n",
    "y_label_even_comb = training_set_label_even_comb.Target # Target variable\n",
    "\n",
    "#BACKWARDSDIFFERENCE\n",
    "\n",
    "#base set \n",
    "x_backwards = training_set_ce.iloc[:,0:1825]\n",
    "y_backwards = training_set_ce.Target\n",
    "\n",
    "#base set desc only \n",
    "x_backwards_desc = training_set_ce_desc.iloc[:,1:611]\n",
    "y_backwards_desc = training_set_ce_desc.Target\n",
    "\n",
    "#extended set \n",
    "x_backwards_adds = training_set_adds_ce.iloc[:,0:2293]\n",
    "y_backwards_adds = training_set_adds_ce.Target\n",
    "\n",
    "#extended set desc only \n",
    "x_backwards_adds_desc = training_set_adds_ce_desc.iloc[:,1:766]\n",
    "y_backwards_adds_desc = training_set_adds_ce_desc.Target\n",
    "\n",
    "#base set comb \n",
    "x_backwards_comb = training_set_comb_ce.iloc[:,2:627]\n",
    "y_backwards_comb = training_set_comb_ce.Target\n",
    "\n",
    "#extended comb\n",
    "x_backwards_adds_comb = training_set_adds_comb_ce.iloc[:,2:782]\n",
    "y_backwards_adds_comb = training_set_adds_comb_ce.Target\n",
    "\n",
    "#even set \n",
    "x_backwards_even = training_set_ce_even.iloc[:,0:1575]\n",
    "y_backwards_even = training_set_ce_even.Target\n",
    "\n",
    "#even set comb\n",
    "x_backwards_even_comb = training_set_ce_even_comb.iloc[:,1:534]\n",
    "y_backwards_even_comb = training_set_ce_even_comb.Target\n",
    "\n",
    "#TFIDF\n",
    "#base set \n",
    "x_tfidf = training_set_tfidf\n",
    "y_tfidf = training_set['Target'].values\n",
    "\n",
    "#extended set \n",
    "x_adds_tfidf = training_set_adds_tfidf\n",
    "y_adds_tfidf = training_set_adds['Target'].values\n",
    "\n",
    "#base set comb \n",
    "x_tfidf_comb = training_set_comb_tfidf\n",
    "y_tfidf_comb = training_set['Target'].values\n",
    "\n",
    "#extended set comb \n",
    "x_adds_tfidf_comb = training_set_adds_comb_tfidf\n",
    "y_adds_tfidf_comb = training_set_adds['Target'].values\n",
    "\n",
    "#even set \n",
    "x_adds_tfidf_even = training_set_tfidf_even \n",
    "y_adds_tfidf_even = training_set_even['Target'].values\n",
    "\n",
    "#even set comb \n",
    "x_adds_tfidf_even_comb = training_set_even_comb_tfidf\n",
    "y_adds_tfidf_even_comb = training_set_even['Target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: \n",
    "- run all variations of training set on two different types of Log Reg: cross eval and train/test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Reg cross eval \n",
    "def lr(x,y,title):  \n",
    "    \"\"\" logistic regression\"\"\"\n",
    "    model = LogisticRegression(solver='liblinear', C=10.0,random_state=44)\n",
    "    y_pred = cross_val_predict(model, x, y, cv=5)\n",
    "    acc = cross_val_score(model, x, y, cv=5, scoring='precision')    \n",
    "    report = classification_report(y, y_pred)\n",
    "    return print(f'{title}\\n''MEAN PRECISION', np.mean(acc), 'report:', report, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordinal Encoding results with crossval\n",
    "print(lr(x_ord, y_ord, 'Base Set Ordinal:'), lr(x_ord_desc, y_ord_desc, 'Base Set Ordinal Desc Only:'), lr(x_ord_adds, y_ord_adds, 'Extended Set Ordinal:'), lr(x_ord_adds_desc, y_ord_adds_desc, 'Extended Set Ordinal Desc Only:'), lr(x_ord_comb, y_ord_comb, 'Base Set Ordinal Combined:'), lr(x_ord_adds_comb, y_ord_adds_comb, 'Extended Set Ordinal Combined:'), lr(x_ord_even, y_ord_even, 'Even Set Ordinal:'), lr(x_ord_even_comb, y_ord_even_comb, 'Even Set Combined Ordinal:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding results with crossval\n",
    "print(lr(x_label, y_label, 'Base Set Label:'), lr(x_label_desc, y_label_desc, 'Base Set Label Desc Only:'), lr(x_label_adds, y_label_adds, 'Extended Set Label:'), lr(x_label_adds_desc, y_label_adds_desc, 'Extended Set Label Desc Only:'), lr(x_label_comb, y_label_comb, 'Base Set Xtra Col Label:'), lr(x_label_adds_comb, y_label_adds_comb, 'Extended Set Xtra Col Label:'), lr(x_label_even, y_label_even, 'Even Set Label:'), lr(x_label_even_comb, y_label_even_comb, 'Even Set Combined Label:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BD Encoding results with crossval\n",
    "print(lr(x_backwards, y_backwards, 'Base Set BD:'), lr(x_backwards_desc, y_backwards_desc, 'Base Set BD Desc Only:'), lr(x_backwards_adds, y_backwards_adds, 'Extended Set BD:'), lr(x_backwards_adds_desc, y_backwards_adds_desc, 'Extended Set Label Desc Only:'), lr(x_backwards_comb, y_backwards_comb, 'Base Set BD Combined:'), lr(x_backwards_adds_comb, y_backwards_adds_comb, 'Extended Set BD Combined:'), lr(x_backwards_even, y_backwards_even, 'Even Set BD:'), lr(x_backwards_even_comb, y_backwards_even_comb, 'Even Set Combined BD:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf Encoding results with crossval\n",
    "print(lr(x_tfidf, y_tfidf, 'Base Set TF:'), lr(x_adds_tfidf, y_adds_tfidf, 'Extended Set TF:'), lr(x_tfidf_comb, y_tfidf_comb, 'Base Set Xtra Col TF URL:'), lr(x_adds_tfidf_comb, y_adds_tfidf_comb, 'Extended Set Xtra Col TF URL:'), lr(x_adds_tfidf_even, y_adds_tfidf_even, 'Even Set BD:'), lr(x_adds_tfidf_even_comb, y_adds_tfidf_even_comb, 'Even Set Combined BD:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogReg train/test \n",
    "\n",
    "# import required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "\n",
    "def lr_training(x,y,title):\n",
    "    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.33,random_state=0)\n",
    "    logreg = LogisticRegression(solver='liblinear')\n",
    "    # fit the model with data\n",
    "    logreg.fit(x_train,y_train)\n",
    "    y_pred=logreg.predict(x_test)\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    # create heatmap for cfn matrix \n",
    "    class_names=[0,1] # name  of classes\n",
    "    fig, ax = plt.subplots()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    # create heatmap\n",
    "    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.tight_layout()\n",
    "    plt.title(f'Confusion matrix:{title}', y=1.1)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return print(f'{title}\\n' \"Accuracy:\", accuracy_score(y_test, y_pred), \"\\nPrecision:\", precision_score(y_test, y_pred), \"\\nRecall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordinal encoding results w/ train/test \n",
    "print(lr_training(x_ord, y_ord, 'Base Set Ordinal:'), lr_training(x_ord_desc, y_ord_desc, 'Base Set Ordinal Desc Only:'), lr_training(x_ord_adds, y_ord_adds, 'Extended Set Ordinal:'), lr_training(x_ord_adds_desc, y_ord_adds_desc, 'Extended Set Ordinal Desc Only:'), lr_training(x_ord_comb, y_ord_comb, 'Base Set Ordinal Combined:'), lr_training(x_ord_adds_comb, y_ord_adds_comb, 'Extended Set Ordinal Combined:'), lr_training(x_ord_even, y_ord_even, 'Even Set Ordinal:'), lr_training(x_ord_even_comb, y_ord_even_comb, 'Even Set Combined Ordinal:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding results w/ train/test\n",
    "print(lr_training(x_label, y_label, 'Base Set Label:'), lr_training(x_label_desc, y_label_desc, 'Base Set Label Desc Only:'), lr_training(x_label_adds, y_label_adds, 'Extended Set Label:'), lr_training(x_label_adds_desc, y_label_adds_desc, 'Extended Set Label Desc Only:'), lr_training(x_label_comb, y_label_comb, 'Base Set Xtra Col Label:'), lr_training(x_label_adds_comb, y_label_adds_comb, 'Extended Set Xtra Col Label:'), lr_training(x_label_even, y_label_even, 'Even Set Label:'), lr_training(x_label_even_comb, y_label_even_comb, 'Even Set Combined Label:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BD Encoding results with crossval\n",
    "print(lr_training(x_backwards, y_backwards, 'Base Set BD:'), lr_training(x_backwards_desc, y_backwards_desc, 'Base Set BD Desc Only:'), lr_training(x_backwards_adds, y_backwards_adds, 'Extended Set BD:'), lr_training(x_backwards_adds_desc, y_backwards_adds_desc, 'Extended Set Label Desc Only:'), lr_training(x_backwards_comb, y_backwards_comb, 'Base Set BD Combined:'), lr_training(x_backwards_adds_comb, y_backwards_adds_comb, 'Extended Set BD Combined:'), lr_training(x_backwards_even, y_backwards_even, 'Even Set BD:'), lr_training(x_backwards_even_comb, y_backwards_even_comb, 'Even Set Combined BD:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf Encoding results with crossval\n",
    "print(lr_training(x_tfidf, y_tfidf, 'Base Set TF:'), lr_training(x_adds_tfidf, y_adds_tfidf, 'Extended Set TF:'), lr_training(x_tfidf_comb, y_tfidf_comb, 'Base Set Xtra Col TF URL:'), lr_training(x_adds_tfidf_comb, y_adds_tfidf_comb, 'Extended Set Xtra Col TF URL:'), lr_training(x_adds_tfidf_even, y_adds_tfidf_even, 'Even Set BD:'), lr_training(x_adds_tfidf_even_comb, y_adds_tfidf_even_comb, 'Even Set Combined BD:')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: \n",
    "- save model\n",
    "- test on base predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_even = count_vect.fit_transform(training_set_even['Description'])\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf_even = tfidf_transformer.fit_transform(X_train_even)\n",
    "Y_train_tfidf_even = training_set_even['Target'].values\n",
    "model = LogisticRegression(solver='liblinear', C=10.0,random_state=44)\n",
    "model.fit(X_train_tfidf_even, Y_train_tfidf_even)\n",
    "prediction_set = pd.read_pickle(path+'LOGREG_RELEVANCE/base_prediction_set.pkl')\n",
    "X_new_counts = count_vect.transform(prediction_set['Description'])\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = model.predict(X_new_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'chordify annotator subjectivity dataset' => 1\n",
      "'the acousticbrainz genre dataset' => 1\n",
      "'aligned scores and performances (asap) dataset' => 1\n",
      "'ballroom beat and bar annotations' => 1\n",
      "'welcome to the dali dataset: a large dataset of synchronised audio, lyrics and vocal notes. news: tutorial: 2- getting the audio. 3- working with dali. 4- correcting annotations.' => 1\n",
      "'da-tacos' => 1\n",
      "'desed dataset' => 1\n",
      "'fma: a dataset for music analysis' => 1\n",
      "'guitarsolodetection' => 1\n",
      "'the harmonix set' => 1\n",
      "'musooevaluator' => 1\n",
      "\"master's thesis data\" => 1\n",
      "'workflow and guidelines for corpus creation' => 1\n",
      "'m-djcue' => 1\n",
      "'mastmelody_dataset' => 1\n",
      "'musical onset database and library (modal)' => 1\n",
      "'the mtg-jamendo dataset' => 1\n",
      "'the nes music database' => 1\n",
      "'seils dataset' => 1\n",
      "'african american historical newspapers online' => 0\n",
      "'zappa books' => 0\n",
      "'folkstreams' => 0\n",
      "'the kirby collection' => 0\n",
      "'ethnologisches museum' => 0\n",
      "'phonogrammarchiv' => 0\n",
      "'the archive of folk culture' => 0\n",
      "'american radio archives' => 0\n",
      "'arabic music archives' => 0\n",
      "'armenian sound archive' => 0\n",
      "'digital library of appalachia' => 0\n",
      "'orwig music library at brown university' => 0\n",
      "'asu sheet music collection' => 0\n",
      "'cancioneros' => 0\n",
      "'latin american music center at indiana university' => 0\n",
      "'electronic musicological review' => 0\n",
      "'digital library of the caribbean' => 0\n",
      "'chatham music archive' => 0\n",
      "'ccny music library' => 0\n",
      "'the cylinder archive' => 0\n",
      "'dartmouth jewish sound archive' => 0\n",
      "'the dutch song database' => 0\n",
      "'stanford university music library and archive of recorded sound' => 0\n",
      "'irving s. gilmore music library' => 0\n",
      "'oz magazine archive' => 0\n",
      "'glasgow miracle archives' => 0\n",
      "'new york folklore' => 0\n",
      "'swem library’s special collections' => 0\n",
      "'recorded sound archives at florida atlantic university' => 0\n",
      "'foundation for iranian studies' => 0\n",
      "'free music archive' => 0\n",
      "'footage berlin' => 0\n",
      "'glenn miller archives' => 0\n",
      "'crocodile café collection' => 0\n",
      "'al magazine' => 0\n",
      "'rave flyers uk' => 0\n",
      "'chicago music forever' => 0\n",
      "'american music institute' => 0\n",
      "'american music research center at university of colorado, boulder' => 0\n",
      "'center for american music' => 0\n",
      "'center for american music at butler school of music' => 0\n",
      "'center for black music research' => 0\n",
      "'center for popular music at middle tennessee state university' => 0\n",
      "'h. wiley hitchcock institute for studies in american music at brooklyn college' => 0\n",
      "'institute of jazz studies at rutgers' => 0\n",
      "'john jacob niles center for american music' => 0\n",
      "'marta and austin weeks music library' => 0\n",
      "'moravian music foundation' => 0\n",
      "'music library & bill schurk sound archives' => 0\n",
      "'the national ragtime and jazz archive' => 0\n",
      "'sarasota music archive' => 0\n",
      "'sousa archives and center for american music' => 0\n",
      "'michelle smith performing arts library' => 0\n",
      "'international piano archives at maryland' => 0\n",
      "'ucla library special collections' => 0\n",
      "'blues archive at the university of mississippi' => 0\n",
      "'wisconsin music archive' => 0\n",
      "'world music archives' => 0\n",
      "'music collection at university of hawaii' => 0\n",
      "'le collezioni di casa ricordi' => 0\n",
      "'sound collections database' => 0\n",
      "'go-set magazine archive' => 0\n",
      "'studs terkel radio archive' => 0\n",
      "'egyptian cassette archive' => 0\n",
      "'betty nolting collection' => 0\n",
      "'eda kuhn loeb music library' => 0\n",
      "'contextual dissemination' => 0\n",
      "'mayrent institute for yiddish culture' => 0\n",
      "'sounding spirit digital library' => 0\n",
      "'lilian voudouri music library' => 0\n",
      "'burns antiphoner' => 0\n",
      "'gilmore music library' => 0\n",
      "'woody guthrie publications' => 0\n",
      "'wende museum music collections' => 0\n",
      "'walter p. reuther library' => 0\n",
      "'big heavy world' => 0\n",
      "'charles h. mills music library' => 0\n",
      "'callaway centre archive' => 0\n",
      "'uw ethnomusicology archives' => 0\n",
      "'faulkner at virginia: an audio archive' => 0\n",
      "'the western soundscape archive' => 0\n",
      "'briscoe center for american history' => 0\n",
      "'the jazz collections at the university library of southern denmark' => 0\n",
      "'freedman jewish sound archive' => 0\n",
      "'otto e. albrecht music library collection' => 0\n",
      "'uo libraries music collection' => 0\n",
      "'zine collection at university of missouri-kansas city' => 0\n",
      "'raymond scott collection at university of missouri-kansas city' => 0\n",
      "'women composers collection' => 0\n",
      "'aldemaro romero archive' => 0\n",
      "'grainger museum' => 0\n",
      "'larry benicewicz collection' => 0\n",
      "'audio-visual archives at the university of kentucky' => 0\n",
      "'the university of kansas sound archive' => 0\n",
      "'music & performing arts library at university of illinois' => 0\n",
      "'the international jazz collections' => 0\n",
      "'the chicago jazz archive' => 0\n",
      "'ucla music library' => 0\n",
      "'ucla ethnomusicology archive' => 0\n",
      "'center for texas music history' => 0\n",
      "'tennessee archive of moving image and sound' => 0\n",
      "'belfer audio laboratory and archive' => 0\n",
      "'belfer cylinders digital connection' => 0\n",
      "'music library collections at university of buffalo' => 0\n",
      "'florida folklife collection' => 0\n",
      "'the monterey jazz festival collection' => 0\n",
      "'nga taonga sound & vision' => 0\n",
      "'pennsound' => 0\n",
      "'northwestern university music library' => 0\n",
      "'new york public radio archives' => 0\n",
      "'moving image and recorded sound division schomburg center for research in black culture' => 0\n",
      "'new hampshire library of traditional jazz' => 0\n",
      "'national archives and records service of south africa' => 0\n",
      "'the australian archive of jewish music' => 0\n",
      "'the max hunter collection' => 0\n",
      "'the milken archive' => 0\n",
      "'louisiana music archive' => 0\n",
      "'polley music library' => 0\n",
      "'the kentucky center for traditional music' => 0\n",
      "'jazz music archives' => 0\n",
      "'jazzinstitut darmstadt' => 0\n",
      "'hate5six' => 0\n",
      "'circulation zero' => 0\n",
      "'the luke rowell digital music collection' => 0\n",
      "'the beta lounge archive' => 0\n",
      "'klaus wachsmann uganda collection' => 0\n",
      "'the audio foundation' => 0\n",
      "'centro library and archives digital collections' => 0\n",
      "'alabama folklife collection' => 0\n",
      "'the arhoolie foundation' => 0\n",
      "'the berkeley folk music festival & the folk revival on the us west coast—an introduction' => 0\n",
      "'alton abraham collection of sun ra 1822-2008' => 0\n",
      "'scottish jazz archive' => 0\n",
      "\"it's only rock 'n' roll digital collection\" => 0\n",
      "'roctober archive' => 0\n",
      "'metopera database' => 0\n",
      "'the new york philharmonic leon levy digital archives' => 0\n",
      "'the boston symphony orchestra archives' => 0\n",
      "'the dancehall archive & research initiative' => 0\n"
     ]
    }
   ],
   "source": [
    "for doc, target in zip(prediction_set['Title'], predicted):\n",
    "    print('%r => %s' % (doc, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re encode models w/ tfidf \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "count_vect = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "#even set\n",
    "X_train_even = count_vect.fit_transform(training_set_even['Description'])\n",
    "X_train_tfidf_even = tfidf_transformer.fit_transform(X_train_even)\n",
    "Y_train_tfidf_even = training_set_even['Target'].values\n",
    "\n",
    "#base set\n",
    "X_train_base = count_vect.fit_transform(training_set['Description'])\n",
    "X_train_tfidf_base = tfidf_transformer.fit_transform(X_train_base)\n",
    "Y_train_tfidf_base = training_set['Target'].values\n",
    "\n",
    "#extended set \n",
    "X_train_extended = count_vect.fit_transform(training_set_adds['Description'])\n",
    "X_train_tfidf_extended = tfidf_transformer.fit_transform(X_train_extended)\n",
    "Y_train_tfidf_extended = training_set_adds['Target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re encode models w/ BD \n",
    "\n",
    "#base set \n",
    "base_model_ce = training_set.copy()\n",
    "encoder_base = ce.BackwardDifferenceEncoder(cols=['Title', 'Description', 'URL'])\n",
    "base_model_ce = encoder_base.fit_transform(base_model_ce, verbose=1)\n",
    "base_model_ce.drop(['intercept'], inplace=True, axis=1)\n",
    "x_train_bd_base = base_model_ce.iloc[:,0:1825]\n",
    "y_train_bd_base = base_model_ce.Target\n",
    "\n",
    "#extended set\n",
    "extended_model_ce = training_set_adds.copy()\n",
    "extended_model_ce = encoder_base.fit_transform(extended_model_ce, verbose=1)\n",
    "extended_model_ce.drop(['intercept'], inplace=True, axis=1)\n",
    "x_train_bd_extended = extended_model_ce.iloc[:,0:2293]\n",
    "y_train_bd_extended = extended_model_ce.Target\n",
    "\n",
    "#even set\n",
    "even_model_ce = training_set_even.copy()\n",
    "even_model_ce = encoder_base.fit_transform(even_model_ce, verbose=1)\n",
    "even_model_ce.drop(['intercept'], inplace=True, axis=1)\n",
    "x_train_bd_extended = even_model_ce.iloc[:,0:1575]\n",
    "y_train_bd_even = even_model_ce.Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a cross val LogReg model, save them\n",
    "\n",
    "#test_size = 0.33\n",
    "#seed = 7\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X_train_tfidf, X_train_tfidf, test_size=test_size, random_state=seed)\n",
    "\n",
    "# Fit the model on training set\n",
    "model = LogisticRegression(solver='liblinear', C=10.0,random_state=44)\n",
    "model.fit(X_train_tfidf_even, Y_train_tfidf_even)\n",
    "# save the model to disk\n",
    "#filename_even = 'crossval_tfidf_even.sav'\n",
    "#pickle.dump(model_tfidf_even, open(filename_even, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_set = pd.read_pickle(path+'LOGREG_RELEVANCE/base_prediction_set.pkl')\n",
    "X_new_counts = count_vect.transform(prediction_set['Description'])\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_2 = pd.read_pickle(path+'GH_PICKLES/music_archive.pkl')\n",
    "prediction_2['Name'] = prediction_2['Name'].str.lower().str.strip()\n",
    "prediction_2 = prediction_2[~prediction_2['Name'].isin(training_set_even['Title'])].dropna()\n",
    "X_new_counts_2 = count_vect.transform(prediction_2['Description'])\n",
    "X_new_tfidf_2 = tfidf_transformer.transform(X_new_counts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_set_ce = prediction_set.copy()\n",
    "prediction_set_ce['Dummy'] = ''\n",
    "prediction_set_ce = encoder_base.transform(prediction_set_ce)\n",
    "prediction_set_ce.drop(['intercept'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_backwards = prediction_set_ce.iloc[:,0:1825]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr(X_new_tfidf_2, predicted, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, target in zip(prediction_2['Name'], predicted):\n",
    "    print('%r => %s' % (doc, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5: \n",
    "- The mysteries of Twitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_neg = pd.read_pickle(path+'TWITTER_SEARCHES/MJI BIGRAMS (NEGATIVE)/twitter_music_magazine.pkl')\n",
    "twitter_neg_2 = pd.read_pickle(path+'TWITTER_SEARCHES/MJI BIGRAMS (NEGATIVE)/twitter_music_oral_history.pkl')\n",
    "twitter_neg['Target'] = '0'\n",
    "twitter_neg_2['Target'] = '0'\n",
    "twitter_neg = pd.concat([twitter_neg[['tweet', 'Target']], twitter_neg_2[['tweet', 'Target']]])\n",
    "twitter_pos = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS (POSITIVE)/twitter_music_archive.pkl')\n",
    "twitter_pos['Target'] = '1'\n",
    "twitter_pos = twitter_pos[['tweet', 'Target']]\n",
    "twitter_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_set = pd.concat([twitter_pos, twitter_neg])\n",
    "twitter_set['Target'] = twitter_set['Target'].astype('int')\n",
    "twitter_set = twitter_set.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base set \n",
    "twitter_set_ce = twitter_set.copy()\n",
    "# Specify the columns to encode then fit and transform\n",
    "encoder_base = ce.BackwardDifferenceEncoder(cols=['tweet'])\n",
    "twitter_set_ce = encoder_base.fit_transform(twitter_set_ce, verbose=1)\n",
    "twitter_set_ce.drop(['intercept'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_set_ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_twit_bd = twitter_set_ce.iloc[:,0:1158]\n",
    "y_twit_bd = twitter_set_ce.Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_training(x_twit_bd, y_twit_bd, 'BD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_idf = twitter_set.copy()\n",
    "tfidf_vectorizer.fit_transform(twitter_idf['tweet'].values)\n",
    "twitter_set_tfidf = tfidf_vectorizer.transform(twitter_idf['tweet'].values)\n",
    "x_twit_idf = twitter_set_tfidf\n",
    "y_twit_idf = twitter_idf['Target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_training(x_twit_idf, y_twit_idf, 'IDF')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
