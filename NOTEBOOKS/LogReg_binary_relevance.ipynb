{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- supervised ce? \n",
    "- replicate w/ twitter dfs? \n",
    "- try different types of scoring in logreg?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports + path\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pandas_profiling import ProfileReport\n",
    "path = '/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/LOGREG_RELEVANCE/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: \n",
    "- create DFs for trainin set using musoW and MJI spreadsheets\n",
    "- two versions: a base one and one w/ additions from ismir dataset (for musoW) and from additional research (for MJI)\n",
    "- the baseline difference we want to train the LogReg model on is that musoW is focused on music archives w/ datasets while MJI is focused on all sorts of music archives, regardless of dataset inclusion\n",
    "- to avoid polluting the two sets we check for duplicates at all possible stages to ensure that the MJI set does not have anything that currently exists in musoW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read mji csv and grab needed columns\n",
    "df_mji = pd.read_csv('/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/MJI/MJI_data.csv', keep_default_na=False, dtype='string')\n",
    "df_mji_small = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "df_mji_small['Title'] = df_mji['Title'].str.lower().str.strip()\n",
    "df_mji_small['Description'] = df_mji['Description'].str.lower().str.strip()\n",
    "df_mji_small['URL'] = df_mji['URL'].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read musow json dump and grab needed columns\n",
    "with open('/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/MUSOW/musow_name_desc_url_cat.json') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "musow_names = [result['name']['value'].strip().lower() for result in data['results']['bindings']]\n",
    "musow_desc = [result['description']['value'].strip().lower() for result in data['results']['bindings']]\n",
    "musow_url = [result['url']['value'].strip().lower() for result in data['results']['bindings']]\n",
    "df_musow = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "df_musow['Title'] = musow_names\n",
    "df_musow['Description'] = musow_desc\n",
    "df_musow['URL'] = musow_url\n",
    "df_musow = df_musow.astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove musow duplicates from MJI set \n",
    "mji_training_set = df_mji_small[~df_mji_small['Title'].isin(df_musow['Title'])].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create positive and negative sets w/o additions, add target column \n",
    "positive_df = df_musow.copy()\n",
    "positive_df['Target'] = '1'\n",
    "negative_df = mji_training_set.copy()\n",
    "negative_df['Target'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create positive and negative sets w/ additions, add target column \n",
    "ismir_df = pd.read_pickle('/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/GH_PICKLES/ismir.pkl')\n",
    "ismir_df = ismir_df[~ismir_df['Title'].isin(df_musow['Title'])].dropna() \n",
    "positive_df_adds = pd.concat([df_musow, ismir_df]).reset_index(drop=True)\n",
    "positive_df_adds = positive_df_adds.drop_duplicates(['Title'], keep='last')\n",
    "positive_df_adds['Target'] = '1'\n",
    "mji_additions_1 = pd.read_csv('/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/MJI/MJI_additions_for_LR.csv')\n",
    "mji_additions_1['Title'] = mji_additions_1['Title'].str.lower().str.strip()\n",
    "mji_additions_1['Description'] = mji_additions_1['Description'].str.lower().str.strip()\n",
    "mji_additions_1['URL'] = mji_additions_1['URL'].str.lower().str.strip()\n",
    "mji_additions_1 = mji_additions_1[~mji_additions_1['Title'].isin(df_musow['Title'])].dropna()\n",
    "mji_additions_1 = mji_additions_1[~mji_additions_1['Title'].isin(mji_training_set['Title'])].dropna()\n",
    "negative_df_adds = pd.concat([mji_training_set, mji_additions_1]).reset_index(drop=True)\n",
    "negative_df_adds = negative_df_adds.drop_duplicates(['Title'], keep='last')\n",
    "negative_df_adds['Target'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge both sets into one, create two training sets one base, one extended w/ additional data, pickle for reuse\n",
    "training_set = pd.concat([positive_df, negative_df])\n",
    "training_set['Target'] = training_set['Target'].astype('int')\n",
    "training_set = training_set.reset_index(drop=True)\n",
    "training_set_adds = pd.concat([positive_df_adds, negative_df_adds])\n",
    "training_set_adds['Target'] = training_set_adds['Target'].astype('int')\n",
    "training_set_adds = training_set_adds.reset_index(drop=True)\n",
    "training_set.to_pickle(path+'title_desc_url_trainingset.pkl')\n",
    "training_set_adds.to_pickle(path+'title_desc_url_trainingset_extended.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine three feature column into one, create new DFs, pickle for reuse\n",
    "\n",
    "training_set_xtra = training_set.copy()\n",
    "training_set_adds_xtra = training_set_adds.copy()\n",
    "\n",
    "#create combined columns for desc+headline and desc+headline_url\n",
    "def tokenize_url(url:str):\n",
    "    url=url.replace(\"https\",\"\")\n",
    "    url=url.replace(\"http\",\"\")\n",
    "    url=url.replace(\"www\",\"\")   \n",
    "    url=re.sub(\"(\\W|_)+\",\" \",url)\n",
    "    return url\n",
    "\n",
    "#create tokenized URL field\n",
    "training_set_xtra['tokenized_url']=training_set_xtra['URL'].apply(lambda x:tokenize_url(x))\n",
    "#description + tokenized url\n",
    "training_set_xtra['text_desc_headline_url'] = training_set_xtra['Description'] + ' '+ training_set_xtra['Title']+\" \" + training_set_xtra['tokenized_url']\n",
    "training_set_xtra.drop(['tokenized_url', 'Title', 'Description', 'URL'], inplace=True, axis=1)\n",
    "\n",
    "#create tokenized URL field\n",
    "training_set_adds_xtra['tokenized_url']=training_set_adds_xtra['URL'].apply(lambda x:tokenize_url(x))\n",
    "#description + tokenized url\n",
    "training_set_adds_xtra['text_desc_headline_url'] = training_set_adds_xtra['Description'] + ' '+ training_set_adds_xtra['Title']+\" \" + training_set_adds_xtra['tokenized_url']\n",
    "training_set_adds_xtra.drop(['tokenized_url', 'Title', 'Description', 'URL'], inplace=True, axis=1)\n",
    "\n",
    "training_set_xtra.to_pickle(path+'title_desc_url_trainingset_xtracols.pkl')\n",
    "training_set_adds_xtra.to_pickle(path+'title_desc_url_trainingset_extended_xtracols.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of base set: 626 \n",
      "Median length of description in base set is: 291.0 \n",
      "Mean length of description in base set is: 343.55271565495207 \n",
      "Size of extended set: 781 \n",
      "Median length of description in extended set is: 355.0 \n",
      "Mean length of description in extended set is: 535.2394366197183\n",
      "\n",
      "Median length of description in base set w/ xtra cols is: 360.5 \n",
      "Mean length of description in base set w/ xtra cols is: 408.073482428115 \n",
      "Median length of description in extended set w/ xtra cols is: 419.0 \n",
      "Mean length of description in extended set w/ xtra cols is: 601.8143405889884\n"
     ]
    }
   ],
   "source": [
    "#print some base stats to keep track of changes: training set size, avg length of description (main feature)\n",
    "\n",
    "print('Size of base set:', len(training_set.index), '\\nMedian length of description in base set is:', training_set['Description'].str.len().median(), '\\nMean length of description in base set is:', training_set['Description'].str.len().mean(), '\\nSize of extended set:', len(training_set_adds.index), '\\nMedian length of description in extended set is:', training_set_adds['Description'].str.len().median(), '\\nMean length of description in extended set is:', training_set_adds['Description'].str.len().mean())\n",
    "\n",
    "#print some base stats to keep track of changes: avg length of combined desc+title+url (main feature)\n",
    "\n",
    "print('\\nMedian length of description in base set w/ xtra cols is:', training_set_xtra['text_desc_headline_url'].str.len().median(), '\\nMean length of description in base set w/ xtra cols is:', training_set_xtra['text_desc_headline_url'].str.len().mean(), '\\nMedian length of description in extended set w/ xtra cols is:', training_set_adds_xtra['text_desc_headline_url'].str.len().median(), '\\nMean length of description in extended set w/ xtra cols is:', training_set_adds_xtra['text_desc_headline_url'].str.len().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2:\n",
    "- Try different unsupervised encoding approaches for categorical variables including OrdinalEncoder, Label Encoding, Tf-Idf, BackwardDifferenceEncoder \n",
    "- Ordinal and Label encoding use title, desc, and url + combined title/desc and title/desc/url as features depending on source version of training set\n",
    "- BackwardDifference uses title, desc, and url + separated title/desc and title/desc/url as features (in order to see if there's any diff btw the two different combination of strings approach)\n",
    "- Tf-Idf uses only desc, title/desc, and title/desc/url as features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode categorial variables w/ OrdinalEncoder, pickle\n",
    "ord_enc = OrdinalEncoder()\n",
    "\n",
    "#base set \n",
    "training_set_ordinal = training_set.copy()\n",
    "training_set_ordinal[\"Title_encoded\"] = ord_enc.fit_transform(training_set_ordinal[[\"Title\"]])\n",
    "training_set_ordinal[\"Desc_encoded\"] = ord_enc.fit_transform(training_set_ordinal[[\"Description\"]])\n",
    "training_set_ordinal[\"URL_encoded\"] = ord_enc.fit_transform(training_set_ordinal[[\"URL\"]])\n",
    "training_set_ordinal.to_pickle(path+'title_desc_url_trainingset_ordinal_encode.pkl')\n",
    "\n",
    "#base set xtra columns\n",
    "training_set_ordinal_xtra = training_set_xtra.copy()\n",
    "training_set_ordinal_xtra[\"text_desc_headline_url_encoded\"] = ord_enc.fit_transform(training_set_ordinal_xtra[[\"text_desc_headline_url\"]])\n",
    "training_set_ordinal_xtra.to_pickle(path+'title_desc_url_trainingset_xtracols_ordinal_encode.pkl')\n",
    "\n",
    "#extended set \n",
    "training_set_ordinal_adds = training_set_adds.copy()\n",
    "training_set_ordinal_adds[\"Title_encoded\"] = ord_enc.fit_transform(training_set_ordinal_adds[[\"Title\"]])\n",
    "training_set_ordinal_adds[\"Desc_encoded\"] = ord_enc.fit_transform(training_set_ordinal_adds[[\"Description\"]])\n",
    "training_set_ordinal_adds[\"URL_encoded\"] = ord_enc.fit_transform(training_set_ordinal_adds[[\"URL\"]])\n",
    "training_set_ordinal_adds.to_pickle(path+'title_desc_url_trainingset_extended_ordinal_encode.pkl')\n",
    "\n",
    "#extended set xtra columns \n",
    "training_set_ordinal_adds_xtra = training_set_adds_xtra.copy()\n",
    "training_set_ordinal_adds_xtra[\"text_desc_headline_url_encoded\"] = ord_enc.fit_transform(training_set_ordinal_adds_xtra[[\"text_desc_headline_url\"]])\n",
    "training_set_ordinal_adds_xtra.to_pickle(path+'title_desc_url_trainingset_extended_xtracols_ordinal_encode.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode cat variables with label encoding, pickle \n",
    "\n",
    "#base set \n",
    "training_set_label = training_set.copy()\n",
    "training_set_label[\"Title\"] = training_set_label[\"Title\"].astype('category')\n",
    "training_set_label[\"Description\"] = training_set_label[\"Description\"].astype('category')\n",
    "training_set_label[\"URL\"] = training_set_label[\"URL\"].astype('category')\n",
    "training_set_label[\"Title_cat\"] = training_set_label[\"Title\"].cat.codes\n",
    "training_set_label[\"Desc_cat\"] = training_set_label[\"Description\"].cat.codes\n",
    "training_set_label[\"URL_cat\"] = training_set_label[\"URL\"].cat.codes\n",
    "training_set_label.to_pickle(path+'title_desc_url_trainingset_label_encode.pkl')\n",
    "\n",
    "#base set xtra cols \n",
    "training_set_label_xtra = training_set_xtra.copy()\n",
    "training_set_label_xtra[\"text_desc_headline_url\"] = training_set_label_xtra[\"text_desc_headline_url\"].astype('category')\n",
    "training_set_label_xtra[\"text_desc_headline_url_cat\"] = training_set_label_xtra[\"text_desc_headline_url\"].cat.codes\n",
    "training_set_label_xtra.to_pickle(path+'title_desc_url_trainingset_xtracols_label_encode.pkl')\n",
    "\n",
    "#extended set \n",
    "training_set_label_adds = training_set_adds.copy()\n",
    "training_set_label_adds[\"Title\"] = training_set_label_adds[\"Title\"].astype('category')\n",
    "training_set_label_adds[\"Description\"] = training_set_label_adds[\"Description\"].astype('category')\n",
    "training_set_label_adds[\"URL\"] = training_set_label_adds[\"URL\"].astype('category')\n",
    "training_set_label_adds[\"Title_cat\"] = training_set_label_adds[\"Title\"].cat.codes\n",
    "training_set_label_adds[\"Desc_cat\"] = training_set_label_adds[\"Description\"].cat.codes\n",
    "training_set_label_adds[\"URL_cat\"] = training_set_label_adds[\"URL\"].cat.codes\n",
    "training_set_label_adds.to_pickle(path+'title_desc_url_trainingset_extended_label_encode.pkl')\n",
    "\n",
    "#extended set xtra cols \n",
    "training_set_label_adds_xtra = training_set_adds_xtra.copy()\n",
    "training_set_label_adds_xtra[\"text_desc_headline_url\"] = training_set_label_adds_xtra[\"text_desc_headline_url\"].astype('category')\n",
    "training_set_label_adds_xtra[\"text_desc_headline_url_cat\"] = training_set_label_adds_xtra[\"text_desc_headline_url\"].cat.codes\n",
    "training_set_label_adds_xtra.to_pickle(path+'title_desc_url_trainingset_extended_xtracols_label_encode.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode w/ CE's BackwardDifferenceEncoder, pickle \n",
    "\n",
    "#base set \n",
    "training_set_ce = training_set.copy()\n",
    "# Specify the columns to encode then fit and transform\n",
    "encoder_base = ce.BackwardDifferenceEncoder(cols=['Title', 'Description', 'URL'])\n",
    "training_set_ce = encoder_base.fit_transform(training_set_ce, verbose=1)\n",
    "training_set_ce.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_ce.to_pickle(path+'title_desc_url_trainingset_BD_encode.pkl')\n",
    "\n",
    "#extended set \n",
    "training_set_adds_ce = training_set_adds.copy()\n",
    "training_set_adds_ce = encoder_base.fit_transform(training_set_adds_ce, verbose=1)\n",
    "training_set_adds_ce.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_adds_ce.to_pickle(path+'title_desc_url_trainingset_extended_BD_encode.pkl')\n",
    "\n",
    "#base set xtra cols, two types (urls and no urls)\n",
    "training_set_xtra_ce = training_set_xtra.copy()\n",
    "encoder_xtra_url = ce.BackwardDifferenceEncoder(cols=['text_desc_headline_url'])\n",
    "training_set_xtra_ce_url = encoder_xtra_url.fit_transform(training_set_xtra_ce, verbose=1)\n",
    "training_set_xtra_ce_url.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_xtra_ce_url.to_pickle(path+'title_desc_url_trainingset_xtracols_BD_encode_url.pkl')\n",
    "\n",
    "#extended set xtra cols \n",
    "training_set_adds_xtra_ce = training_set_adds_xtra.copy()\n",
    "training_set_adds_xtra_ce_url = encoder_xtra_url.fit_transform(training_set_adds_xtra_ce, verbose=1)\n",
    "training_set_adds_xtra_ce_url.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_adds_xtra_ce_url.to_pickle(path+'title_desc_url_trainingset_extended_xtracols_BD_encode_url.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD PICKLE READING CHUNK HERE TO NOT RERUN ALL ABOVE CODE\n",
    "training_set = pd.read_pickle(path+'title_desc_url_trainingset.pkl')\n",
    "training_set_adds = pd.read_pickle(path+'title_desc_url_trainingset_extended.pkl')\n",
    "training_set_xtra = pd.read_pickle(path+'title_desc_url_trainingset_xtracols.pkl')\n",
    "training_set_adds_xtra = pd.read_pickle(path+'title_desc_url_trainingset_extended_xtracols.pkl')\n",
    "training_set_ordinal = pd.read_pickle(path+'title_desc_url_trainingset_ordinal_encode.pkl')\n",
    "training_set_ordinal_xtra = pd.read_pickle(path+'title_desc_url_trainingset_xtracols_ordinal_encode.pkl')\n",
    "training_set_ordinal_adds = pd.read_pickle(path+'title_desc_url_trainingset_extended_ordinal_encode.pkl')\n",
    "training_set_ordinal_adds_xtra = pd.read_pickle(path+'title_desc_url_trainingset_extended_xtracols_ordinal_encode.pkl')\n",
    "training_set_label = pd.read_pickle(path+'title_desc_url_trainingset_label_encode.pkl')\n",
    "training_set_label_xtra = pd.read_pickle(path+'title_desc_url_trainingset_xtracols_label_encode.pkl')\n",
    "training_set_label_adds = pd.read_pickle(path+'title_desc_url_trainingset_extended_label_encode.pkl')\n",
    "training_set_label_adds_xtra = pd.read_pickle(path+'title_desc_url_trainingset_extended_xtracols_label_encode.pkl')\n",
    "training_set_ce = pd.read_pickle(path+'title_desc_url_trainingset_BD_encode.pkl')\n",
    "training_set_adds_ce = pd.read_pickle(path+'title_desc_url_trainingset_extended_BD_encode.pkl')\n",
    "training_set_xtra_ce_url = pd.read_pickle(path+'title_desc_url_trainingset_xtracols_BD_encode_url.pkl')\n",
    "training_set_adds_xtra_ce_url = pd.read_pickle(path+'title_desc_url_trainingset_extended_xtracols_BD_encode_url.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode w/ tf-idf\n",
    "\n",
    "#create vectorizer \n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True, max_df=0.95)\n",
    "\n",
    "#base set description \n",
    "tfidf_vectorizer.fit_transform(training_set['Description'].values)\n",
    "training_set_tfidf = tfidf_vectorizer.transform(training_set['Description'].values)\n",
    "\n",
    "#extended set description \n",
    "tfidf_vectorizer.fit_transform(training_set_adds['Description'].values)\n",
    "training_set_adds_tfidf = tfidf_vectorizer.transform(training_set_adds['Description'].values)\n",
    "\n",
    "#base set xtra cols description+title+url\n",
    "tfidf_vectorizer.fit_transform(training_set_xtra['text_desc_headline_url'].values)\n",
    "training_set_xtra_url_tfidf = tfidf_vectorizer.transform(training_set_xtra['text_desc_headline_url'].values)\n",
    "\n",
    "#base set xtra cols description+title+url\n",
    "tfidf_vectorizer.fit_transform(training_set_adds_xtra['text_desc_headline_url'].values)\n",
    "training_set_adds_xtra_url_tfidf = tfidf_vectorizer.transform(training_set_adds_xtra['text_desc_headline_url'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set features and targets for all encoding types \n",
    "\n",
    "#ordinal encoding \n",
    "feature_cols_ord = ['Title_encoded', 'Desc_encoded', 'URL_encoded']\n",
    "feature_cols_ord_comb = ['text_desc_headline_url_encoded']\n",
    "\n",
    "#base set\n",
    "x_ord = training_set_ordinal[feature_cols_ord] # Features\n",
    "y_ord = training_set_ordinal.Target # Target variable\n",
    "\n",
    "#extended set \n",
    "x_ord_adds = training_set_ordinal_adds[feature_cols_ord] # Features\n",
    "y_ord_adds = training_set_ordinal_adds.Target # Target variable\n",
    "\n",
    "#base set xtra cols \n",
    "x_ord_xtra = training_set_ordinal_xtra[feature_cols_ord_comb] # Features\n",
    "y_ord_xtra = training_set_ordinal_xtra.Target # Target variable\n",
    "\n",
    "#extended set xtra cols \n",
    "x_ord_xtra_adds = training_set_ordinal_adds_xtra[feature_cols_ord_comb] # Features\n",
    "y_ord_xtra_adds = training_set_ordinal_adds_xtra.Target # Target variable\n",
    "\n",
    "#feature encoding \n",
    "feature_cols_label = ['Title_cat', 'Desc_cat', 'URL_cat']\n",
    "feature_cols_label_comb = ['text_desc_headline_url_cat']\n",
    "\n",
    "#base set\n",
    "x_label = training_set_label[feature_cols_label] # Features\n",
    "y_label = training_set_label.Target # Target variable\n",
    "\n",
    "#extended set\n",
    "x_label_adds = training_set_label_adds[feature_cols_label] # Features\n",
    "y_label_adds = training_set_label_adds.Target # Target variable\n",
    "\n",
    "#base set xtra cols \n",
    "x_label_xtra = training_set_label_xtra[feature_cols_label_comb] # Features\n",
    "y_label_xtra = training_set_label_xtra.Target # Target variable\n",
    "\n",
    "#extended set xtra cols \n",
    "x_label_xtra_adds = training_set_label_adds_xtra[feature_cols_label_comb] # Features\n",
    "y_label_xtra_adds = training_set_label_adds_xtra.Target # Target variable\n",
    "\n",
    "#BD encoding \n",
    "\n",
    "#base set \n",
    "x_backwards = training_set_ce.iloc[:,0:1825]\n",
    "y_backwards = training_set_ce.Target\n",
    "\n",
    "#extended set \n",
    "x_backwards_adds = training_set_adds_ce.iloc[:,0:2293]\n",
    "y_backwards_adds = training_set_adds_ce.Target\n",
    "\n",
    "#base xtra url \n",
    "x_backwards_xtra_url = training_set_xtra_ce_url.iloc[:,2:627]\n",
    "y_backwards_xtra_url = training_set_xtra_ce_url.Target\n",
    "\n",
    "#extended xtra url \n",
    "x_backwards_adds_xtra_url = training_set_adds_xtra_ce_url.iloc[:,2:782]\n",
    "y_backwards_adds_xtra_url = training_set_adds_xtra_ce_url.Target\n",
    "\n",
    "#tf-idf encoding \n",
    "\n",
    "#base set \n",
    "x_tfidf = training_set_tfidf\n",
    "y_tfidf = training_set['Target'].values\n",
    "\n",
    "#extended set \n",
    "x_adds_tfidf = training_set_adds_tfidf\n",
    "y_adds_tfidf = training_set_adds['Target'].values\n",
    "\n",
    "#base set xtra url \n",
    "x_tfidf_xtra_url = training_set_xtra_url_tfidf\n",
    "y_tfidf_xtra_url = training_set_xtra['Target'].values\n",
    "\n",
    "#extended set xtra url \n",
    "x_adds_tfidf_xtra_url = training_set_adds_xtra_url_tfidf\n",
    "y_adds_tfidf_xtra_url = training_set_adds_xtra['Target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: \n",
    "- run all variations of training set on two different types of Log Reg: cross eval and train/test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Reg cross eval \n",
    "def lr(x,y,title):  \n",
    "    \"\"\" logistic regression\"\"\"\n",
    "    model = LogisticRegression(solver='liblinear', C=10.0,random_state=44)\n",
    "    y_pred = cross_val_predict(model, x, y, cv=10)\n",
    "    acc = cross_val_score(model, x, y, cv=10, scoring='precision')    \n",
    "    report = classification_report(y, y_pred)\n",
    "    return print(f'{title}\\n''MEAN PRECISION', np.mean(acc), 'report:', report, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Set Ordinal:\n",
      "MEAN PRECISION\n",
      "0.7930343559137847\n",
      "report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.02      0.03       128\n",
      "           1       0.79      0.97      0.87       498\n",
      "\n",
      "    accuracy                           0.78       626\n",
      "   macro avg       0.46      0.49      0.45       626\n",
      "weighted avg       0.66      0.78      0.70       626\n",
      "\n",
      "Extended Set Ordinal:\n",
      "MEAN PRECISION\n",
      "0.7504776818976107\n",
      "report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.47      0.53       267\n",
      "           1       0.75      0.83      0.79       514\n",
      "\n",
      "    accuracy                           0.71       781\n",
      "   macro avg       0.67      0.65      0.66       781\n",
      "weighted avg       0.70      0.71      0.70       781\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Set Xtra Col Ordinal:\n",
      "MEAN PRECISION\n",
      "0.7955453149001535\n",
      "report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       128\n",
      "           1       0.80      1.00      0.89       498\n",
      "\n",
      "    accuracy                           0.80       626\n",
      "   macro avg       0.40      0.50      0.44       626\n",
      "weighted avg       0.63      0.80      0.70       626\n",
      "\n",
      "Extended Set Xtra Col Ordinal:\n",
      "MEAN PRECISION\n",
      "0.6581304771178189\n",
      "report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       267\n",
      "           1       0.66      1.00      0.79       514\n",
      "\n",
      "    accuracy                           0.66       781\n",
      "   macro avg       0.33      0.50      0.40       781\n",
      "weighted avg       0.43      0.66      0.52       781\n",
      "\n",
      "None None None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Ordinal Encoding results with crossval\n",
    "print(lr(x_ord, y_ord, 'Base Set Ordinal:'), lr(x_ord_adds, y_ord_adds, 'Extended Set Ordinal:'), lr(x_ord_xtra, y_ord_xtra, 'Base Set Xtra Col Ordinal:'), lr(x_ord_xtra_adds, y_ord_xtra_adds, 'Extended Set Xtra Col Ordinal:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Set Label:\n",
      "MEAN PRECISION\n",
      "0.7930343559137847\n",
      "report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.02      0.03       128\n",
      "           1       0.79      0.97      0.87       498\n",
      "\n",
      "    accuracy                           0.78       626\n",
      "   macro avg       0.46      0.49      0.45       626\n",
      "weighted avg       0.66      0.78      0.70       626\n",
      "\n",
      "Extended Set Label:\n",
      "MEAN PRECISION\n",
      "0.7504776818976107\n",
      "report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.47      0.53       267\n",
      "           1       0.75      0.83      0.79       514\n",
      "\n",
      "    accuracy                           0.71       781\n",
      "   macro avg       0.67      0.65      0.66       781\n",
      "weighted avg       0.70      0.71      0.70       781\n",
      "\n",
      "Base Set Xtra Col Label:\n",
      "MEAN PRECISION\n",
      "0.7955453149001535\n",
      "report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       128\n",
      "           1       0.80      1.00      0.89       498\n",
      "\n",
      "    accuracy                           0.80       626\n",
      "   macro avg       0.40      0.50      0.44       626\n",
      "weighted avg       0.63      0.80      0.70       626\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Set Xtra Col Label:\n",
      "MEAN PRECISION\n",
      "0.6581304771178189\n",
      "report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       267\n",
      "           1       0.66      1.00      0.79       514\n",
      "\n",
      "    accuracy                           0.66       781\n",
      "   macro avg       0.33      0.50      0.40       781\n",
      "weighted avg       0.43      0.66      0.52       781\n",
      "\n",
      "None None None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Label Encoding results with crossval\n",
    "print(lr(x_label, y_label, 'Base Set Label:'), lr(x_label_adds, y_label_adds, 'Extended Set Label:'), lr(x_label_xtra, y_label_xtra, 'Base Set Xtra Col Label:'), lr(x_label_xtra_adds, y_label_xtra_adds, 'Extended Set Xtra Col Label:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Set BD:\n",
      "MEAN PRECISION\n",
      "0.9883720930232558\n",
      "report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.96      0.82       128\n",
      "           1       0.99      0.90      0.94       498\n",
      "\n",
      "    accuracy                           0.91       626\n",
      "   macro avg       0.85      0.93      0.88       626\n",
      "weighted avg       0.93      0.91      0.92       626\n",
      "\n",
      "Extended Set BD:\n",
      "MEAN PRECISION\n",
      "0.9800000000000001\n",
      "report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93       267\n",
      "           1       0.97      0.95      0.96       514\n",
      "\n",
      "    accuracy                           0.95       781\n",
      "   macro avg       0.94      0.95      0.94       781\n",
      "weighted avg       0.95      0.95      0.95       781\n",
      "\n",
      "Base Set Xtra Col BD URL:\n",
      "MEAN PRECISION\n",
      "0.9892857142857142\n",
      "report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.95      0.86       128\n",
      "           1       0.99      0.93      0.96       498\n",
      "\n",
      "    accuracy                           0.93       626\n",
      "   macro avg       0.88      0.94      0.91       626\n",
      "weighted avg       0.94      0.93      0.94       626\n",
      "\n",
      "Extended Set Xtra Col BD URL:\n",
      "MEAN PRECISION\n",
      "0.9800000000000001\n",
      "report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93       267\n",
      "           1       0.97      0.95      0.96       514\n",
      "\n",
      "    accuracy                           0.95       781\n",
      "   macro avg       0.94      0.95      0.94       781\n",
      "weighted avg       0.95      0.95      0.95       781\n",
      "\n",
      "None None None None\n"
     ]
    }
   ],
   "source": [
    "#BD Encoding results with crossval\n",
    "print(lr(x_backwards, y_backwards, 'Base Set BD:'), lr(x_backwards_adds, y_backwards_adds, 'Extended Set BD:'), lr(x_backwards_xtra_url, y_backwards_xtra_url, 'Base Set Xtra Col BD URL:'), lr(x_backwards_adds_xtra_url, y_backwards_adds_xtra_url, 'Extended Set Xtra Col BD URL:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf Encoding results with crossval\n",
    "print(lr(x_tfidf, y_tfidf, 'Base Set TF:'), lr(x_adds_tfidf, y_adds_tfidf, 'Extended Set TF:'), lr(x_tfidf_xtra_url, y_tfidf_xtra_url, 'Base Set Xtra Col TF URL:'), lr(x_adds_tfidf_xtra_url, y_adds_tfidf_xtra_url, 'Extended Set Xtra Col TF URL:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogReg train/test \n",
    "\n",
    "def lr_training(x,y,title):\n",
    "    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=0)\n",
    "    logreg = LogisticRegression()\n",
    "    # fit the model with data\n",
    "    logreg.fit(x_train,y_train)\n",
    "    y_pred=logreg.predict(x_test)\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    return print(f'{title}\\n' \"Accuracy:\", accuracy_score(y_test, y_pred), \"\\nPrecision:\", precision_score(y_test, y_pred), \"\\nRecall:\", recall_score(y_test, y_pred)), cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Set Ordinal:\n",
      "Accuracy: 0.7834394904458599 \n",
      "Precision: 0.7973856209150327 \n",
      "Recall: 0.976\n",
      "Extended Set Ordinal:\n",
      "Accuracy: 0.6938775510204082 \n",
      "Precision: 0.7445255474452555 \n",
      "Recall: 0.8031496062992126\n",
      "Base Set Xtra Col Ordinal:\n",
      "Accuracy: 0.7961783439490446 \n",
      "Precision: 0.7961783439490446 \n",
      "Recall: 1.0\n",
      "Extended Set Xtra Col Ordinal:\n",
      "Accuracy: 0.6479591836734694 \n",
      "Precision: 0.6479591836734694 \n",
      "Recall: 1.0\n",
      "(None, array([[  1,  31],\n",
      "       [  3, 122]])) (None, array([[ 34,  35],\n",
      "       [ 25, 102]])) (None, array([[  0,  32],\n",
      "       [  0, 125]])) (None, array([[  0,  69],\n",
      "       [  0, 127]]))\n"
     ]
    }
   ],
   "source": [
    "#Ordinal encoding results w/ train/test \n",
    "print(lr_training(x_ord, y_ord, 'Base Set Ordinal:'), lr_training(x_ord_adds, y_ord_adds, 'Extended Set Ordinal:'), lr_training(x_ord_xtra, y_ord_xtra, 'Base Set Xtra Col Ordinal:'), lr_training(x_ord_xtra_adds, y_ord_xtra_adds, 'Extended Set Xtra Col Ordinal:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Set Label:\n",
      "Accuracy: 0.7834394904458599 \n",
      "Precision: 0.7973856209150327 \n",
      "Recall: 0.976\n",
      "Extended Set Label:\n",
      "Accuracy: 0.6938775510204082 \n",
      "Precision: 0.7445255474452555 \n",
      "Recall: 0.8031496062992126\n",
      "Base Set Xtra Col Label:\n",
      "Accuracy: 0.7961783439490446 \n",
      "Precision: 0.7961783439490446 \n",
      "Recall: 1.0\n",
      "Extended Set Xtra Col Label:\n",
      "Accuracy: 0.6479591836734694 \n",
      "Precision: 0.6479591836734694 \n",
      "Recall: 1.0\n",
      "(None, array([[  1,  31],\n",
      "       [  3, 122]])) (None, array([[ 34,  35],\n",
      "       [ 25, 102]])) (None, array([[  0,  32],\n",
      "       [  0, 125]])) (None, array([[  0,  69],\n",
      "       [  0, 127]]))\n"
     ]
    }
   ],
   "source": [
    "#Label Encoding results w/ train/test\n",
    "print(lr_training(x_label, y_label, 'Base Set Label:'), lr_training(x_label_adds, y_label_adds, 'Extended Set Label:'), lr_training(x_label_xtra, y_label_xtra, 'Base Set Xtra Col Label:'), lr_training(x_label_xtra_adds, y_label_xtra_adds, 'Extended Set Xtra Col Label:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Set BD:\n",
      "Accuracy: 1.0 \n",
      "Precision: 1.0 \n",
      "Recall: 1.0\n",
      "Extended Set BD:\n",
      "Accuracy: 1.0 \n",
      "Precision: 1.0 \n",
      "Recall: 1.0\n",
      "Base Set Xtra Col BD URL:\n",
      "Accuracy: 1.0 \n",
      "Precision: 1.0 \n",
      "Recall: 1.0\n",
      "Extended Set Xtra Col BD URL:\n",
      "Accuracy: 1.0 \n",
      "Precision: 1.0 \n",
      "Recall: 1.0\n",
      "(None, array([[ 32,   0],\n",
      "       [  0, 125]])) (None, array([[ 69,   0],\n",
      "       [  0, 127]])) (None, array([[ 32,   0],\n",
      "       [  0, 125]])) (None, array([[ 69,   0],\n",
      "       [  0, 127]]))\n"
     ]
    }
   ],
   "source": [
    "#BD Encoding results with crossval\n",
    "print(lr_training(x_backwards, y_backwards, 'Base Set BD:'), lr_training(x_backwards_adds, y_backwards_adds, 'Extended Set BD:'), lr_training(x_backwards_xtra_url, y_backwards_xtra_url, 'Base Set Xtra Col BD URL:'), lr_training(x_backwards_adds_xtra_url, y_backwards_adds_xtra_url, 'Extended Set Xtra Col BD URL:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Set TF:\n",
      "Accuracy: 0.8089171974522293 \n",
      "Precision: 0.8064516129032258 \n",
      "Recall: 1.0\n",
      "Extended Set TF:\n",
      "Accuracy: 0.7806122448979592 \n",
      "Precision: 0.78 \n",
      "Recall: 0.9212598425196851\n",
      "Base Set Xtra Col TF URL:\n",
      "Accuracy: 0.8089171974522293 \n",
      "Precision: 0.8064516129032258 \n",
      "Recall: 1.0\n",
      "Extended Set Xtra Col TF URL:\n",
      "Accuracy: 0.7602040816326531 \n",
      "Precision: 0.7666666666666667 \n",
      "Recall: 0.905511811023622\n",
      "(None, array([[  2,  30],\n",
      "       [  0, 125]])) (None, array([[ 36,  33],\n",
      "       [ 10, 117]])) (None, array([[  2,  30],\n",
      "       [  0, 125]])) (None, array([[ 34,  35],\n",
      "       [ 12, 115]]))\n"
     ]
    }
   ],
   "source": [
    "#Tf-Idf Encoding results with crossval\n",
    "print(lr_training(x_tfidf, y_tfidf, 'Base Set TF:'), lr_training(x_adds_tfidf, y_adds_tfidf, 'Extended Set TF:'), lr_training(x_tfidf_xtra_url, y_tfidf_xtra_url, 'Base Set Xtra Col TF URL:'), lr_training(x_adds_tfidf_xtra_url, y_adds_tfidf_xtra_url, 'Extended Set Xtra Col TF URL:')) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
