{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TWITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe function\n",
    "\n",
    "def append_to_pd(json):\n",
    "    \n",
    "    #username setup\n",
    "    username = {user['id']: user['username'] for user in json_response['includes']['users']}\n",
    "    \n",
    "    #1. username\n",
    "    author_id = [tweet['author_id'] for tweet in json['data']]\n",
    "    user = [username[id] for id in author_id]\n",
    "\n",
    "    # 2. Time created\n",
    "    created_at = [dateutil.parser.parse(tweet['created_at']) for tweet in json['data']]\n",
    "\n",
    "    # 3. Language\n",
    "    lang = [tweet['lang'] for tweet in json_response['data']]\n",
    "\n",
    "    # 4. Tweet metrics\n",
    "    retweet_count = [tweet['public_metrics']['retweet_count'] for tweet in json['data']]\n",
    "    reply_count = [tweet['public_metrics']['reply_count'] for tweet in json['data']]\n",
    "    like_count = [tweet['public_metrics']['like_count'] for tweet in json['data']]\n",
    "    quote_count = [tweet['public_metrics']['quote_count'] for tweet in json['data']]\n",
    "\n",
    "    # 5. Tweet text\n",
    "    text = [tweet['text'] for tweet in json['data']]\n",
    "    \n",
    "    # 6. URL \n",
    "    url = []\n",
    "    for tweet in json['data']:\n",
    "        if ('entities' in tweet) and ('urls' in tweet['entities']):\n",
    "            for link in tweet['entities']['urls']:\n",
    "                url.append(link['expanded_url'])\n",
    "        else:\n",
    "            url.append('')\n",
    "    \n",
    "    # Create df and append everything \n",
    "    dataframe = pd.DataFrame(columns=['User', 'Created', 'Language', 'Likes', 'Quotes', 'Replies', 'RTs', 'Tweet', 'URL'])\n",
    "    dataframe['User'] = pd.Series(user).astype('string')\n",
    "    dataframe['Created'] = pd.Series(created_at)\n",
    "    dataframe['Language'] = pd.Series(lang).astype('string')\n",
    "    dataframe['Likes'] = pd.Series(like_count)\n",
    "    dataframe['Quotes'] = pd.Series(quote_count)\n",
    "    dataframe['Replies'] = pd.Series(reply_count)\n",
    "    dataframe['RTs'] = pd.Series(retweet_count)\n",
    "    dataframe['Tweet'] = pd.Series(text).astype('string')\n",
    "    dataframe['URL'] = pd.Series(url).astype('string')   \n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and path\n",
    "from __future__ import print_function\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import pandas as pd\n",
    "path = '/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape URLs for title and text \n",
    "\n",
    "def scrape_links(link_list):\n",
    "    links = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "    for link in link_list:\n",
    "        URL = link\n",
    "        try:\n",
    "            page = requests.get(URL)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            pass\n",
    "        except Exception:\n",
    "            continue\n",
    "        try:\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            if soup and soup.find('head') and soup.find('body') is not None:\n",
    "                title = ' '.join([t.text for t in soup.find('head').find_all('title')]).strip()\n",
    "                text = ' '.join([p.text for p in soup.find('body').find_all('p')]).strip()\n",
    "                new_row = {'Title': title, 'Description': text, 'URL': URL.strip()}\n",
    "                links = links.append(new_row, ignore_index=True)\n",
    "        except AssertionError:\n",
    "            pass\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_co = pd.read_pickle(path+'TWITTER_SEARCHES/NEGATIVE/music_company_2021.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_add = music_co.url.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_add_5= scrape_links(links_to_add[301:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_add_5.to_excel(path+'scrape5.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_add_18 = scrape_links(link_list[1800:1850])\n",
    "links_to_add_18.to_csv(path+'scrape18.csv')\n",
    "links_to_add = links_to_add[links_to_add.Description != ''].reset_index(drop=True)\n",
    "#links_to_add.to_csv(path+'test_2.csv')\n",
    "#links_to_add.to_pickle('/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/DATA/TWITTER_SEARCHES/MJI BIGRAMS (NEGATIVE)/twitter_music_oral_history_link_scrape.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLIT PREDICTION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model(t_input, t_feature, target, score_type, filename, path):\n",
    "    count_vect = CountVectorizer()\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    x_count = count_vect.fit_transform(t_input[t_feature])\n",
    "    x_train = tfidf_transformer.fit_transform(x_count)\n",
    "    y_train = t_input[target].values\n",
    "    model = LogisticRegressionCV(solver='liblinear', random_state=44, cv=2, scoring=score_type)\n",
    "    model.fit(x_train, y_train)\n",
    "    saved_model = f'LOGREG_RELEVANCE/{filename}_model.pkl'\n",
    "    vectorizer = f'LOGREG_RELEVANCE/{filename}_vectorizer.pkl'\n",
    "    pickle.dump(model, open(path+saved_model, 'wb'))\n",
    "    pickle.dump(vectorizer, open(path+vectorizer, 'wb'))\n",
    "\n",
    "def lr_predict(p_input, p_feature, filename, path):\n",
    "    model = pickle.load(open(path+f'LOGREG_RELEVANCE/{filename}_model.pkl', 'rb'))\n",
    "    vectorizer = pickle.load(open(path+f'LOGREG_RELEVANCE/{filename}_vectorizer.pkl', 'rb'))\n",
    "    x_new_count = vectorizer.transform(p_input[p_feature])\n",
    "    x_new_train = tfidf_transformer.transform(x_new_count)\n",
    "    y_predict = model.predict(x_new_train)\n",
    "    scores = model.decision_function(x_new_train, y_predict)\n",
    "    probability = model.predict_log_proba(x_new_train)\n",
    "    results = [r for r in y_predict]\n",
    "    result = p_input.copy()\n",
    "    result['Prediction'] = results\n",
    "    result['Score'] = [s for s in scores]\n",
    "    result['Probability'] = [p for p in probability]\n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW TRAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_even = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even.pkl')\n",
    "training_set_even['Target'] = '1'\n",
    "negative_set = pd.read_csv(path+'LOGREG_RELEVANCE/negative_set.csv')\n",
    "negative_set['Target'] = '0'\n",
    "new_training_set = pd.concat([training_set_even, negative_set])\n",
    "new_training_set['Target'] = new_training_set['Target'].astype('int')\n",
    "new_training_set = new_training_set.reset_index(drop=True)\n",
    "new_training_set.to_pickle(path+'new_training_set.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
