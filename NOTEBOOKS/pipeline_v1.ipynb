{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import os , json , csv , datetime , dateutil.parser , unicodedata , time\n",
    "from datetime import datetime , date , timedelta \n",
    "# classifier\n",
    "import pandas as pd\n",
    "from pandas import Timestamp as timestamp\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#!pip3 install trafilatura\n",
    "import trafilatura\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../'\n",
    "\n",
    "# descriptions training set\n",
    "new_training_set = pd.read_pickle(path+'LOGREG_RELEVANCE/TRAINING_SETS/new_training_set.pkl')\n",
    "\n",
    "# negative twitter training set\n",
    "dh = pd.read_pickle(path+'TWITTER_SEARCHES/NEGATIVE/digital_humanities_2021.pkl')\n",
    "music_company = pd.read_pickle(path+'TWITTER_SEARCHES/NEGATIVE/music_company_2021.pkl')\n",
    "twitter_neg = pd.concat([dh, music_company])\n",
    "twitter_neg = twitter_neg.loc[twitter_neg['lang'] == 'en']\n",
    "twitter_neg['Target'] = '0'\n",
    "twitter_neg = twitter_neg.sample(n=4379, random_state=56)\n",
    "twitter_neg = twitter_neg[['tweet', 'Target']].reset_index(drop=True)\n",
    "\n",
    "#positive twitter training set \n",
    "music_collection = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_music_collection.pkl')\n",
    "song_dataset = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_song_dataset.pkl')\n",
    "sound_archive = pd.read_pickle(path+'TWITTER_SEARCHES/MJI BIGRAMS/twitter_sound_archive.pkl')\n",
    "digital_archive = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_digital_archive.pkl')\n",
    "music_archive = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_music_archive.pkl')\n",
    "digi_music_archive = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_digital_music_archive.pkl')\n",
    "midi_file = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_midi_file.pkl')\n",
    "music_data = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_music_data.pkl')\n",
    "music_research = pd.read_pickle(path+'TWITTER_SEARCHES/MJI BIGRAMS/twitter_music_research.pkl')\n",
    "music_dataset = pd.read_pickle(path+'TWITTER_SEARCHES/MUSOW BIGRAMS/twitter_music_dataset.pkl')\n",
    "twitter_pos = pd.concat([sound_archive, music_collection, digital_archive, music_archive, song_dataset, digi_music_archive, midi_file, music_data, music_research, music_dataset])\n",
    "twitter_pos = twitter_pos.loc[twitter_pos['lang'] == 'en']\n",
    "twitter_pos['Target'] = '1'\n",
    "twitter_pos = twitter_pos[['tweet', 'Target']].reset_index(drop=True)\n",
    "\n",
    "# create the twitter training set\n",
    "twitter_set = pd.concat([twitter_pos, twitter_neg])\n",
    "twitter_set['Target'] = twitter_set['Target'].astype('int')\n",
    "twitter_set = twitter_set.reset_index(drop=True)\n",
    "\n",
    "#kw and sites to remove from url and title strings \n",
    "discard = ['youtu', '404', 'Not Found', 'bandcamp', 'ebay', 'It needs a human touch', 'Page not found', 'open.spotify.com', 'We\\'re sorry...', 'Not Acceptable!', 'Access denied', '412 Error', 'goo.gl', 'instagr.am', 'soundcloud', 'apple.co', 'amzn', 'masterstillmusic', 'Facebook', 'facebook', 'sheetmusiclibrary.website', 'Unsupported browser', 'Last.fm', 'last.fm', 'amazon.com', 'tidal.com', 'tmblr.co', 'blogspot', 'dailymusicroll', 'PortalTaxiMusic', 'apple.news', 'yahoo.com', 'sheetmusicplus.com', 'musicnotes.com', 'musescore.com', 'etsy', 'nts.live', 'twitch.tv', 'YouTube', 'radiosparx.com', 'freemusicarchive.org', 'blastradio', 'opensea', 'mixcloud', 'catalog.works', 'nft', 'NFT', 'allmusic.com', 'foundation.app', 'Robot or human?', 'heardle', 'insession.agency', 'jobvite', 'career']\n",
    "\n",
    "# twitter prediction set \n",
    "#prediction_twitter = pd.read_pickle(path+'TWITTER_SEARCHES/PREDICTIONS/digital_archive_22.pkl')\n",
    "#prediction_twitter = prediction_twitter.loc[prediction_twitter['lang'] == 'en']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_training(t_input, t_feature, target, cv_int, score_type, filename, path):\n",
    "    \"\"\" Create a text classifier based on Logistic regression and TF-IDF. Use cross validation \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t_input: list\n",
    "        dataframe including the training set\n",
    "    t_feature: list\n",
    "        df column, text of tweet or description of the resource\n",
    "    target: list\n",
    "        df column, [0,1] values\n",
    "    cv_int: int\n",
    "        the number of cross validation folding\n",
    "    score_type: str\n",
    "        precision or recall\n",
    "    filename: str\n",
    "        model file name\n",
    "    path: str\n",
    "        parent folder\n",
    "    \"\"\"\n",
    "    # TODO eda to define max_features=1000\n",
    "      \n",
    "    #count_vect = CountVectorizer()\n",
    "    #tfidf_transformer = TfidfTransformer() \n",
    "    #x_train = tfidf_transformer.fit_transform(x_count)\n",
    "    tfidf_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=1000) \n",
    "    x_train = tfidf_transformer.fit_transform(t_input[t_feature])\n",
    "    y_train = t_input[target].values\n",
    "    model = LogisticRegressionCV(solver='liblinear', random_state=44, cv=cv_int, scoring=score_type)\n",
    "    \n",
    "    # export\n",
    "    model.fit(x_train, y_train)\n",
    "    export_model = f'MODELS/{filename}_model.pkl'\n",
    "    export_vectorizer = f'MODELS/{filename}_vectorizer.pkl'\n",
    "    pickle.dump(model, open(path+export_model, 'wb'))\n",
    "    pickle.dump(tfidf_transformer, open(path+export_vectorizer, 'wb'))\n",
    "    \n",
    "    # report\n",
    "    y_pred = cross_val_predict(model, x_train, y_train, cv=cv_int)\n",
    "    report = classification_report(y_train, y_pred)\n",
    "    print('report:', report, sep='\\n')\n",
    "    return model\n",
    "    \n",
    "    \n",
    "def lr_predict(path, filename, p_input, p_feature):\n",
    "    \"\"\" Classify text using a pickled model based on Logistic regression and TF-IDF.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_input: list\n",
    "        dataframe including the prediction set\n",
    "    p_feature: list\n",
    "        df column, text of tweet or description of the resource\n",
    "    filename: str\n",
    "        model file name\n",
    "    path: str\n",
    "        parent folder\n",
    "    \"\"\"\n",
    "    export_model = f'{path}MODELS/{filename}_model.pkl'\n",
    "    export_vectorizer = f'{path}MODELS/{filename}_vectorizer.pkl'\n",
    "    model = pickle.load(open(export_model, 'rb'))\n",
    "    tfidf_transformer = pickle.load(open(export_vectorizer, 'rb'))\n",
    "  \n",
    "    #result = loaded_model.score(X_test, Y_test)\n",
    "    #x_new_count = count_vect.transform(p_input[p_feature])\n",
    "    x_predict = tfidf_transformer.transform(p_input[p_feature])\n",
    "    y_predict = model.predict(x_predict)\n",
    "    scores = model.decision_function(x_predict)\n",
    "    probability = model.predict_proba(x_predict)\n",
    "    \n",
    "    #results = [r for r in y_predict]\n",
    "    result = p_input.copy()\n",
    "    result['Prediction'] = y_predict\n",
    "    result['Score'] = scores\n",
    "    result['Probability'] = probability[:,1]\n",
    "    result['Input Length'] = result[p_feature].str.len()\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_url(keyword, start_date, end_date, max_results):\n",
    "        search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
    "        #change params based on the endpoint you are using\n",
    "        query_params = {'query': keyword,\n",
    "                        'start_time': start_date,\n",
    "                        'end_time': end_date,\n",
    "                        'max_results': max_results,\n",
    "                        'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                        'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source,entities',\n",
    "                        'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                        'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                        'next_token': {}}\n",
    "        return (search_url, query_params)\n",
    "    \n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    " \n",
    "def append_to_csv(json_response, fileName):\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    \n",
    "    #setup usernames via includes\n",
    "    username = {user['id']: user['username'] for user in json_response['includes']['users']}\n",
    "    \n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "\n",
    "        # 1. Username\n",
    "        author_id = tweet['author_id']\n",
    "        user = username[author_id]\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 4. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        #5. URLs \n",
    "        if ('entities' in tweet) and ('urls' in tweet['entities']):\n",
    "            for url in tweet['entities']['urls']:\n",
    "                url = [url['expanded_url'] for url in tweet['entities']['urls'] if 'twitter.com' not in url['expanded_url']]\n",
    "                url = ', '.join(url)\n",
    "        else:\n",
    "            url = \" \"\n",
    "        \n",
    "        #6. Tweet text\n",
    "        text = tweet['text'] \n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [user, created_at, lang, like_count, quote_count, reply_count, retweet_count, text, url]\n",
    "\n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1    \n",
    "    \n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    print(\"# of Tweets added from this response: \", counter) \n",
    "    \n",
    "def twitter_search(token, input_keywords, start, end, mresults, mcount, path='../'):\n",
    "    \n",
    "    # TODO filter tweets in english only OR tweak TF-IDF stopwords (lang detection)\n",
    "    # TODO clean tweets from @ and emoji\n",
    "    bearer_token = token\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    input_keywords   \n",
    "    start_list = start\n",
    "    end_list =  end\n",
    "    max_results = mresults\n",
    "    total_tweets = 0\n",
    "\n",
    "    # Create file\n",
    "    file_name = str(end[0]).replace(':','-').replace('/','-')\n",
    "    csvFile = open(f'{path}TWITTER_SEARCHES/{file_name}.csv', \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "    csvWriter.writerow(['user', 'created_at', 'lang', 'like_count', 'quote_count', 'reply_count','retweet_count','tweet', 'url'])\n",
    "    csvFile.close()\n",
    "\n",
    "    for i in range(0,len(start_list)):\n",
    "        # Inputs\n",
    "        count = 0 # Counting tweets per time period\n",
    "        max_count = mcount # Max tweets per time period\n",
    "        flag = True\n",
    "        next_token = None\n",
    "        \n",
    "        while flag:\n",
    "            # Check if max_count reached\n",
    "            if count >= max_count:\n",
    "                break\n",
    "            print(\"-------------------\")\n",
    "            print(\"Token: \", next_token)\n",
    "            for keyword in input_keywords:\n",
    "                url = create_url(keyword, start_list[i],end_list[i], max_results)\n",
    "                json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "                result_count = json_response['meta']['result_count']\n",
    "\n",
    "                if 'next_token' in json_response['meta']:\n",
    "                    # Save the token to use for next call\n",
    "                    next_token = json_response['meta']['next_token']\n",
    "                    print(\"Next Token: \", next_token)\n",
    "                    if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                        print(\"Start Date: \", start_list[i])\n",
    "                        append_to_csv(json_response, f'{path}TWITTER_SEARCHES/{file_name}.csv')\n",
    "                        count += result_count\n",
    "                        total_tweets += result_count\n",
    "                        print(\"Total # of Tweets added: \", total_tweets)\n",
    "                        print(\"-------------------\")\n",
    "                        time.sleep(5)                \n",
    "                # If no next token exists\n",
    "                else:\n",
    "                    if result_count is not None and result_count > 0:\n",
    "                        print(\"-------------------\")\n",
    "                        print(\"Start Date: \", start_list[i])\n",
    "                        append_to_csv(json_response, f'{path}TWITTER_SEARCHES/{file_name}.csv')\n",
    "                        count += result_count\n",
    "                        total_tweets += result_count\n",
    "                        print(\"Total # of Tweets added: \", total_tweets)\n",
    "                        print(\"-------------------\")\n",
    "                        time.sleep(5)\n",
    "\n",
    "                    #Since this is the final request, turn flag to false to move to the next time period.\n",
    "                    flag = False\n",
    "                    next_token = None\n",
    "                time.sleep(5)\n",
    "    print(\"Total number of results: \", total_tweets)\n",
    "    \n",
    "    df = pd.read_csv(f'{path}TWITTER_SEARCHES/{file_name}.csv', keep_default_na=False, dtype={\"user\": \"string\", \"lang\": \"string\", \"tweet\": \"string\", \"url\": \"string\"})\n",
    "    \n",
    "    # clean the tweet from meentions and hashtags\n",
    "    df['tweet'].replace( { r\"@[A-Za-z0-9_]+\" : '' }, inplace= True, regex = True)\n",
    "    df['tweet'].replace( { r\"#[A-Za-z0-9_]+\" : '' }, inplace= True, regex = True)\n",
    "    \n",
    "    # remove tweets that are not in english\n",
    "    df = df[df['lang'].isin(['en'])]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def scrape_links(link_list):\n",
    "    links = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    \n",
    "    for link in link_list:\n",
    "        URL = link\n",
    "        page = None\n",
    "        ARTICLE = ''\n",
    "        try:\n",
    "            x = requests.head(URL)\n",
    "            content_type = x.headers[\"Content-Type\"] if \"Content-Type\" in x.headers else \"None\"\n",
    "            if (\"text/html\" in content_type.lower()):\n",
    "                page = requests.get(URL)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        if page:\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            title = ' '.join([t.text for t in soup.find('head').find_all('title')]).strip() \\\n",
    "                if soup and soup.find('head') and soup.find('body') is not None \\\n",
    "                else URL\n",
    "            \n",
    "            try:\n",
    "                downloaded = trafilatura.fetch_url(URL)\n",
    "                ARTICLE = trafilatura.extract(downloaded, include_comments=False, include_tables=True)\n",
    "            except Exception:\n",
    "                results = soup.find_all(['h1', 'p'])\n",
    "                text = [result.text for result in results]\n",
    "                ARTICLE = ' '.join(text)\n",
    "            \n",
    "            if len(ARTICLE) > 200:\n",
    "                # text summarisation\n",
    "                max_chunk = 500\n",
    "                #removing special characters and replacing with end of sentence\n",
    "                ARTICLE = ARTICLE.replace('.', '.<eos>')\n",
    "                ARTICLE = ARTICLE.replace('?', '?<eos>')\n",
    "                ARTICLE = ARTICLE.replace('!', '!<eos>')\n",
    "                sentences = ARTICLE.split('<eos>')\n",
    "                current_chunk = 0 \n",
    "                chunks = []\n",
    "\n",
    "                # split text to process\n",
    "                for sentence in sentences:\n",
    "                    if len(chunks) == current_chunk + 1: \n",
    "                        if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n",
    "                            chunks[current_chunk].extend(sentence.split(' '))\n",
    "                        else:\n",
    "                            current_chunk += 1\n",
    "                            chunks.append(sentence.split(' '))\n",
    "                    else:\n",
    "                        chunks.append(sentence.split(' '))\n",
    "\n",
    "                for chunk_id in range(len(chunks)):\n",
    "                    chunks[chunk_id] = ' '.join(chunks[chunk_id])\n",
    "                try:\n",
    "                    res = summarizer(chunks, max_length=120, min_length=30, do_sample=False)\n",
    "                    # summary\n",
    "                    text = ' '.join([summ['summary_text'] for summ in res])\n",
    "                except Exception:\n",
    "                    text = ARTICLE\n",
    "                    continue\n",
    "            else:\n",
    "                text = ARTICLE\n",
    "            print(URL,title,'\\n',text)\n",
    "            new_row = {'Title': title, 'Description': text, 'URL': URL.strip()}\n",
    "            links = links.append(new_row, ignore_index=True)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training twitter and descriptions classifiers\n",
    "\n",
    "This is a ONE TIME operation. The models are pickled and loaded later to predict new results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one time training on twitter\n",
    "#twitter_training_model = lr_training(twitter_set, 'tweet', 'Target', 10, 'precision', 'twitter', path)\n",
    "\n",
    "# one time training on resources\n",
    "#resource_training_model = lr_training(new_training_set, 'Description', 'Target', 10, 'f1','resources',path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Twitter\n",
    "\n",
    "Calls Twitter API with the list of keywords and returns the table `prediction_twitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpytnyptfnmp8crqdvug4yd43o3sl9\n",
      "Start Date:  2022-04-15T00:00:00.000Z\n",
      "# of Tweets added from this response:  50\n",
      "Total # of Tweets added:  50\n",
      "-------------------\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpytnylm2odhy36cglhy9aujv0ndrx\n",
      "Start Date:  2022-04-15T00:00:00.000Z\n",
      "# of Tweets added from this response:  50\n",
      "Total # of Tweets added:  100\n",
      "-------------------\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpytnylls3idscmqvzus92fl40lv25\n",
      "Start Date:  2022-04-15T00:00:00.000Z\n",
      "# of Tweets added from this response:  50\n",
      "Total # of Tweets added:  150\n",
      "-------------------\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  b26v89c19zqg8o3fpytnyljdy63dd64qt5m2eplcb0131\n",
      "Start Date:  2022-04-15T00:00:00.000Z\n",
      "# of Tweets added from this response:  50\n",
      "Total # of Tweets added:  200\n",
      "-------------------\n",
      "Total number of results:  200\n"
     ]
    }
   ],
   "source": [
    "token = 'AAAAAAAAAAAAAAAAAAAAAJgsNAEAAAAAQcsgbUnOJJmqmU483%2F8x6n9V1i8%3Df0qaEo9cV1sWP4eyNQ6E9s8BiRjvFTSN9mSqithe8uIXSNP68x'\n",
    "\n",
    "# a selection of keywords from KEYWORDS/bg_summary.csv\n",
    "# keywords = ['sheet music','music archive','music collection','music library','black music','sound recording','midi file','early music','sound archive','music information','music history','music research','musical score','song dataset','library music','music oral','score collection','digitized score']\n",
    "keywords = ['sheet music','music archive','music collection','music library']\n",
    "input_keywords = [k+\" -is:retweet\" for k in keywords] \n",
    "\n",
    "today = date.today()\n",
    "week_ago = today - timedelta(days=7)\n",
    "start = [week_ago.strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")]\n",
    "end = [today.strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")]\n",
    "\n",
    "mresults = 50 # for each keyword\n",
    "mcount = 50 # for each timespan (only one, last week, here)\n",
    "path='../'\n",
    "\n",
    "prediction_twitter = twitter_search(token, input_keywords, start, end, mresults, mcount, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LogisticRegressionCV from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/var/folders/d1/gjlb4c2x5qld93w00hwqj3yh0000gn/T/ipykernel_6072/4144673739.py:62: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  tfidf_transformer = pickle.load(open(export_vectorizer, 'rb'))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Score</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Input Length</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Black Music Archive is so fucking SHADY!!!!!!!...</td>\n",
       "      <td>1</td>\n",
       "      <td>10.983706</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>70</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you identify this song with no music? Chec...</td>\n",
       "      <td>1</td>\n",
       "      <td>9.511414</td>\n",
       "      <td>0.999926</td>\n",
       "      <td>105</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Black Feminity TV ()\n",
       "Black Music Archive \n",
       "Calv...</td>\n",
       "      <td>1</td>\n",
       "      <td>9.382190</td>\n",
       "      <td>0.999916</td>\n",
       "      <td>113</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Christian Music Archive\" Artist of the Day: P...</td>\n",
       "      <td>1</td>\n",
       "      <td>7.917846</td>\n",
       "      <td>0.999636</td>\n",
       "      <td>158</td>\n",
       "      <td>http://www.ccmrewound.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who says pony music is dead? I've got a fuck...</td>\n",
       "      <td>1</td>\n",
       "      <td>7.191593</td>\n",
       "      <td>0.999248</td>\n",
       "      <td>189</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Grammy Museum To Feature 'The Power Of Women I...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.156079</td>\n",
       "      <td>0.538941</td>\n",
       "      <td>118</td>\n",
       "      <td>http://dlvr.it/SP0VZf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>I could have roleplayed tradwife if caught yo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.151471</td>\n",
       "      <td>0.537795</td>\n",
       "      <td>232</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>This 2012  project may interest you. It's lon...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.131311</td>\n",
       "      <td>0.532781</td>\n",
       "      <td>270</td>\n",
       "      <td>http://RadioOccupy.tv, https://web.archive.org...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>YouTube music has a bigger library than both ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.110714</td>\n",
       "      <td>0.527650</td>\n",
       "      <td>145</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>🎊 Got plans for this weekend? FERIA DE ABRIL i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.081926</td>\n",
       "      <td>0.520470</td>\n",
       "      <td>299</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet  Prediction      Score  \\\n",
       "0    Black Music Archive is so fucking SHADY!!!!!!!...           1  10.983706   \n",
       "1    Can you identify this song with no music? Chec...           1   9.511414   \n",
       "2    Black Feminity TV ()\n",
       "Black Music Archive \n",
       "Calv...           1   9.382190   \n",
       "3    \"Christian Music Archive\" Artist of the Day: P...           1   7.917846   \n",
       "4      Who says pony music is dead? I've got a fuck...           1   7.191593   \n",
       "..                                                 ...         ...        ...   \n",
       "117  Grammy Museum To Feature 'The Power Of Women I...           1   0.156079   \n",
       "118   I could have roleplayed tradwife if caught yo...           1   0.151471   \n",
       "119   This 2012  project may interest you. It's lon...           1   0.131311   \n",
       "120   YouTube music has a bigger library than both ...           1   0.110714   \n",
       "121  🎊 Got plans for this weekend? FERIA DE ABRIL i...           1   0.081926   \n",
       "\n",
       "     Probability  Input Length  \\\n",
       "0       0.999983            70   \n",
       "1       0.999926           105   \n",
       "2       0.999916           113   \n",
       "3       0.999636           158   \n",
       "4       0.999248           189   \n",
       "..           ...           ...   \n",
       "117     0.538941           118   \n",
       "118     0.537795           232   \n",
       "119     0.532781           270   \n",
       "120     0.527650           145   \n",
       "121     0.520470           299   \n",
       "\n",
       "                                                   url  \n",
       "0                                                       \n",
       "1                                                       \n",
       "2                                                       \n",
       "3                            http://www.ccmrewound.com  \n",
       "4                                                       \n",
       "..                                                 ...  \n",
       "117                              http://dlvr.it/SP0VZf  \n",
       "118                                                     \n",
       "119  http://RadioOccupy.tv, https://web.archive.org...  \n",
       "120                                                     \n",
       "121                                                     \n",
       "\n",
       "[122 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions\n",
    "twitter_predictions = lr_predict(path, 'twitter', prediction_twitter, 'tweet')\n",
    "\n",
    "tweet_predict_cv_df = twitter_predictions.copy().drop_duplicates()\n",
    "tweet_predict_cv_df = tweet_predict_cv_df.loc[tweet_predict_cv_df['Prediction'] == 1]\n",
    "tweet_predict_cv_df = tweet_predict_cv_df[~tweet_predict_cv_df.url.str.contains('|'.join(discard))]\n",
    "tweet_predict_cv_df = tweet_predict_cv_df.sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
    "tweet_predict_cv_df = tweet_predict_cv_df[['tweet', 'Prediction', 'Score', 'Probability', 'Input Length', 'url']]\n",
    "tweet_predict_cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7feb91fab0c4445b058e68cb1afbb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/NOTEBOOKS/pipeline_v1.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurentfintoni/Desktop/University/COURSE%20DOCS/THESIS/Internship/musow-pipeline/NOTEBOOKS/pipeline_v1.ipynb#ch0000013?line=1'>2</a>\u001b[0m twitter_link_list \u001b[39m=\u001b[39m [link \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m tweet_predict_cv_df[\u001b[39m'\u001b[39m\u001b[39murl\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtwitter\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m link]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurentfintoni/Desktop/University/COURSE%20DOCS/THESIS/Internship/musow-pipeline/NOTEBOOKS/pipeline_v1.ipynb#ch0000013?line=3'>4</a>\u001b[0m \u001b[39m# scrape URL list\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/laurentfintoni/Desktop/University/COURSE%20DOCS/THESIS/Internship/musow-pipeline/NOTEBOOKS/pipeline_v1.ipynb#ch0000013?line=4'>5</a>\u001b[0m links_to_add \u001b[39m=\u001b[39m scrape_links(twitter_link_list)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurentfintoni/Desktop/University/COURSE%20DOCS/THESIS/Internship/musow-pipeline/NOTEBOOKS/pipeline_v1.ipynb#ch0000013?line=6'>7</a>\u001b[0m \u001b[39m# remove empty descriptions \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/laurentfintoni/Desktop/University/COURSE%20DOCS/THESIS/Internship/musow-pipeline/NOTEBOOKS/pipeline_v1.ipynb#ch0000013?line=7'>8</a>\u001b[0m links_to_add \u001b[39m=\u001b[39m links_to_add[links_to_add\u001b[39m.\u001b[39mDescription \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/NOTEBOOKS/pipeline_v1.ipynb Cell 6'\u001b[0m in \u001b[0;36mscrape_links\u001b[0;34m(link_list)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/laurentfintoni/Desktop/University/COURSE%20DOCS/THESIS/Internship/musow-pipeline/NOTEBOOKS/pipeline_v1.ipynb#ch0000005?line=234'>235</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscrape_links\u001b[39m(link_list):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/laurentfintoni/Desktop/University/COURSE%20DOCS/THESIS/Internship/musow-pipeline/NOTEBOOKS/pipeline_v1.ipynb#ch0000005?line=235'>236</a>\u001b[0m     links \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mTitle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDescription\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mURL\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/laurentfintoni/Desktop/University/COURSE%20DOCS/THESIS/Internship/musow-pipeline/NOTEBOOKS/pipeline_v1.ipynb#ch0000005?line=236'>237</a>\u001b[0m     summarizer \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39msummarization\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/laurentfintoni/Desktop/University/COURSE%20DOCS/THESIS/Internship/musow-pipeline/NOTEBOOKS/pipeline_v1.ipynb#ch0000005?line=238'>239</a>\u001b[0m     \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m link_list:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/laurentfintoni/Desktop/University/COURSE%20DOCS/THESIS/Internship/musow-pipeline/NOTEBOOKS/pipeline_v1.ipynb#ch0000005?line=239'>240</a>\u001b[0m         URL \u001b[39m=\u001b[39m link\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/__init__.py:549\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/__init__.py?line=544'>545</a>\u001b[0m \u001b[39m# Infer the framework from the model\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/__init__.py?line=545'>546</a>\u001b[0m \u001b[39m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/__init__.py?line=546'>547</a>\u001b[0m \u001b[39m# Will load the correct model if possible\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/__init__.py?line=547'>548</a>\u001b[0m model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/__init__.py?line=548'>549</a>\u001b[0m framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/__init__.py?line=549'>550</a>\u001b[0m     model,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/__init__.py?line=550'>551</a>\u001b[0m     model_classes\u001b[39m=\u001b[39;49mmodel_classes,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/__init__.py?line=551'>552</a>\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/__init__.py?line=552'>553</a>\u001b[0m     framework\u001b[39m=\u001b[39;49mframework,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/__init__.py?line=553'>554</a>\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/__init__.py?line=554'>555</a>\u001b[0m     task\u001b[39m=\u001b[39;49mtask,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/__init__.py?line=555'>556</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/__init__.py?line=556'>557</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/__init__.py?line=558'>559</a>\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/__init__.py?line=560'>561</a>\u001b[0m load_tokenizer \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(model_config) \u001b[39min\u001b[39;00m TOKENIZER_MAPPING \u001b[39mor\u001b[39;00m model_config\u001b[39m.\u001b[39mtokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/base.py:198\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/base.py?line=171'>172</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/base.py?line=172'>173</a>\u001b[0m \u001b[39mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/base.py?line=173'>174</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/base.py?line=194'>195</a>\u001b[0m \u001b[39m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/base.py?line=195'>196</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/base.py?line=196'>197</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tf_available() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_available():\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/base.py?line=197'>198</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/base.py?line=198'>199</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/base.py?line=199'>200</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/base.py?line=200'>201</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/base.py?line=201'>202</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/base.py?line=202'>203</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/base.py?line=203'>204</a>\u001b[0m     model_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_from_pipeline\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m task\n",
      "\u001b[0;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "# get links from positive tweets results\n",
    "twitter_link_list = [link for link in tweet_predict_cv_df['url'] if 'twitter' not in link]\n",
    "\n",
    "# scrape URL list\n",
    "links_to_add = scrape_links(twitter_link_list)\n",
    "\n",
    "# remove empty descriptions \n",
    "links_to_add = links_to_add[links_to_add.Description != ''].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify web resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>URL</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Score</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Input Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Third Eye TV | CCA Glasgow</td>\n",
       "      <td>New music, sound poetry, archival video, inte...</td>\n",
       "      <td>https://www.cca-glasgow.com/programme/third-ey...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.886536</td>\n",
       "      <td>0.979896</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journals</td>\n",
       "      <td>The National Jazz Archive journal collection ...</td>\n",
       "      <td>https://tinyurl.com/jazz-journals</td>\n",
       "      <td>1</td>\n",
       "      <td>2.931182</td>\n",
       "      <td>0.949367</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Title  \\\n",
       "0  Third Eye TV | CCA Glasgow   \n",
       "1                    Journals   \n",
       "\n",
       "                                         Description  \\\n",
       "0   New music, sound poetry, archival video, inte...   \n",
       "1   The National Jazz Archive journal collection ...   \n",
       "\n",
       "                                                 URL  Prediction     Score  \\\n",
       "0  https://www.cca-glasgow.com/programme/third-ey...           1  3.886536   \n",
       "1                  https://tinyurl.com/jazz-journals           1  2.931182   \n",
       "\n",
       "   Probability  Input Length  \n",
       "0     0.979896           187  \n",
       "1     0.949367           210  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resources_predictions = lr_predict(path, 'resources', links_to_add, 'Description')\n",
    "\n",
    "resources_preds_cv_df = resources_predictions.copy()\n",
    "resources_preds_cv_df = resources_preds_cv_df.loc[resources_preds_cv_df['Prediction'] == 1]\n",
    "resources_preds_cv_df = resources_preds_cv_df[~resources_preds_cv_df.Title.str.contains('|'.join(discard))]\n",
    "resources_preds_cv_df = resources_preds_cv_df[~resources_preds_cv_df.URL.str.contains('|'.join(discard))]\n",
    "resources_preds_cv_df.sort_values(by='Score', ascending=False).reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
