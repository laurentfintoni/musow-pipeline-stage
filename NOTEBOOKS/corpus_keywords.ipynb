{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- version w/o lemmatizing?\n",
    "- reusable item for importing into pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and set your path \n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import PlaintextCorpusReader, wordnet\n",
    "path = '/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: \n",
    "- extract needed info from MJI and musoW dumps \n",
    "- format, align and remove dupes\n",
    "- bounce to text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read mji csv and turn into two column df for names and desc \n",
    "df_mji = pd.read_csv(path+'MJI/MJI_data.csv', keep_default_na=False, dtype='string')\n",
    "df_mji_small = df_mji.iloc[:, [0, 5]].copy()\n",
    "df_mji_small['Title'] = df_mji_small['Title'].str.lower().str.strip()\n",
    "df_mji_small['Description'] = df_mji_small['Description'].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = pd.read_csv(path+'negative_set.csv', keep_default_na=False, dtype='string')\n",
    "negative['Title'] = negative['Title'].str.lower().str.strip()\n",
    "negative['Description'] = negative['Description'].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read musow json dump and turn into df w/ same columns \n",
    "with open(path+'MUSOW/musow_name_desc_url_cat.json') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "musow_names = [result['name']['value'].strip().lower() for result in data['results']['bindings']]\n",
    "musow_desc = [result['description']['value'].strip().lower() for result in data['results']['bindings']]\n",
    "df_musow = pd.DataFrame(columns=['Title', 'Description'])\n",
    "df_musow['Title'] = musow_names\n",
    "df_musow['Description'] = musow_desc\n",
    "df_musow = df_musow.astype('string')\n",
    "df_musow.to_pickle(path+'KEYWORDS/musoW_keywords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates from MJI set based on title field \n",
    "df_mji_small[~df_mji_small['Title'].isin(df_musow['Title'])].dropna()\n",
    "df_mji_small.to_pickle(path+'KEYWORDS/MJI_keywords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save each df to a single text file for processing \n",
    "with open(path+'KEYWORDS/mji_corpus.txt', 'a') as f:\n",
    "    dfAsString = df_mji_small.to_string(header=False, index=False)\n",
    "    f.write(dfAsString)\n",
    "with open(path+'KEYWORDS/musow_corpus.txt', 'a') as f:\n",
    "    dfAsString = df_musow.to_string(header=False, index=False)\n",
    "    f.write(dfAsString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'KEYWORDS/negative.txt', 'a') as f:\n",
    "    dfAsString = negative.to_string(header=False, index=False)\n",
    "    f.write(dfAsString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2:\n",
    "- text processing using nltk: remove punctuations, tokenize, remove nltk stopwords + custom list, lemmatize \n",
    "- get top 35 keywords \n",
    "- get bigrams in which each word is in the top 35 kws "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set corpus variables + vars for cleaning \n",
    "mji_corpus = PlaintextCorpusReader(path+'KEYWORDS/mji_corpus.txt', '.*\\.txt')\n",
    "musow_corpus = PlaintextCorpusReader(path+'KEYWORDS/musow_corpus.txt', '.*\\.txt')\n",
    "negative_corpus = PlaintextCorpusReader(path+'KEYWORDS/negative.txt', '.*\\.txt')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "punct_tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "custom_stopwords = ['available', '000', 'including', 'also', 'includes', 'website', 'new', 'include', 'well', 'based', 'source', 'sources', 'contains', 'search']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract top 35 keywords and return them alongside bigrams that include two words within top 35\n",
    "def keywords(input_corpus):\n",
    "    string = input_corpus.raw('')\n",
    "    tokenised = punct_tokenizer.tokenize(string)\n",
    "    clean = [w for w in tokenised if w not in stopwords]\n",
    "    clean_2 = [w for w in clean if w not in custom_stopwords]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w) for w in clean_2]\n",
    "    freqdist_lem = nltk.FreqDist(lemmatized)\n",
    "    most_common_lem = freqdist_lem.most_common(35)\n",
    "    most_common_lem_list = []\n",
    "    for t in most_common_lem:\n",
    "        most_common_lem_list.append(t[0])\n",
    "    bigrams = nltk.bigrams(lemmatized)\n",
    "    freqdist_bg = nltk.FreqDist(bigrams)\n",
    "    search_bigrams = []\n",
    "    for k, v in freqdist_bg.items():\n",
    "        if k[0] in most_common_lem_list:\n",
    "            if k[1] in most_common_lem_list:\n",
    "                if v > 5:\n",
    "                    k = ' '.join(k)\n",
    "                    search_bigrams.append([k, v])\n",
    "    kw_pd = pd.DataFrame(columns=['Most Common KW', 'KW Freq', 'Bigrams', 'Bigram Freq'])\n",
    "    kw_pd['Most Common KW'] = pd.Series([w[0] for w in most_common_lem])\n",
    "    kw_pd['KW Freq'] = pd.Series([w[1] for w in most_common_lem])\n",
    "    kw_pd['Bigrams'] = pd.Series([b[0] for b in search_bigrams])\n",
    "    kw_pd['Bigram Freq'] = pd.Series([b[1] for b in search_bigrams])\n",
    "    return kw_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process each and pickle \n",
    "musowkw = keywords(musow_corpus)\n",
    "musowkw.to_pickle(path+'KEYWORDS/musowkw.pkl')\n",
    "mjikw = keywords(mji_corpus)\n",
    "mjikw.to_pickle(path+'KEYWORDS/mjikw.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativekw = keywords(negative_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativekw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3:\n",
    "- Analyze results, differences \n",
    "- Create reusable item for needed results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "musowkw['Source'] = 'musow'\n",
    "mjikw['Source'] = 'mji'\n",
    "different = pd.concat([musowkw, mjikw]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_kw = different.copy()\n",
    "different_kw = different_kw[['Most Common KW', 'KW Freq', 'Source']]\n",
    "different_kw = different_kw.sort_values(by='KW Freq', ascending=False)\n",
    "different_kw.to_csv(path+'KEYWORDS/kw_summary.csv')\n",
    "different_bg = different.copy()\n",
    "different_bg = different_bg[['Bigrams', 'Bigram Freq', 'Source']]\n",
    "different_bg = different_bg.sort_values(by='Bigram Freq', ascending=False)\n",
    "different_bg.to_csv(path+'KEYWORDS/bg_summary.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
