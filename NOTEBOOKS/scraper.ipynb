{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://github.com/cwu307/DrumPtDataset/blob/master/README.txt\"\n",
    "page = requests.get(URL)\n",
    "github_positive = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "title = ' '.join([h1.text for h1 in soup.find(id='readme').find_all('h1')])\n",
    "text = ' '.join([p.text for p in soup.find(id='readme').find_all('p')])\n",
    "new_row = {'Title': title, 'Description': text, 'URL': URL.strip()}\n",
    "github_positive = github_positive.append(new_row, ignore_index=True)\n",
    "\n",
    "github_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.nippon.com/en/people/e00181/\"\n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "title = ' '.join([t.text for t in soup.find('head').find_all('title')])\n",
    "text = ' '.join([p.text for p in soup.find('body').find_all('p')])\n",
    "text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape github read me files \n",
    "def scrape_github(link_list):\n",
    "    github_pd = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "    for link in link_list:\n",
    "        URL = link\n",
    "        page = requests.get(URL)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        if soup and soup.find('head') and soup.find('body') is not None:\n",
    "            title = ' '.join([h1.text for h1 in soup.find(id='readme').find_all('h1')])\n",
    "            text = ' '.join([p.text for p in soup.find(id='readme').find_all('p')]) \n",
    "            new_row = {'Title': title, 'Description': text, 'URL': URL.strip()}\n",
    "            github_pd = github_pd.append(new_row, ignore_index=True)\n",
    "    return github_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ismir_list = ['https://github.com/chordify/CASD/blob/master/readme.md', 'https://github.com/MTG/acousticbrainz-genre-dataset/blob/master/README.md', 'https://github.com/fosfrancesco/asap-dataset/blob/master/README.md', 'https://github.com/Tsung-Ping/functional-harmony/blob/master/README.md', 'https://github.com/CPJKU/BallroomAnnotations/blob/master/README.md', 'https://github.com/gabolsgabs/DALI/blob/master/README.md', 'https://github.com/MTG/da-tacos/blob/master/README.md', 'https://github.com/turpaultn/DESED/blob/master/README.md', 'https://github.com/mdeff/fma/blob/master/README.md', 'https://github.com/ashispati/GuitarSoloDetection/blob/master/README.md', 'https://github.com/urinieto/harmonixset/blob/master/README.md', 'https://github.com/MTG/MusOOEvaluator/blob/master/README.md', 'https://github.com/jblsmith/ma-thesis/blob/master/README.md', 'https://github.com/ELVIS-Project/mass-duos-corpus-josquin-larue/blob/Methodologies-for-Creating-Symbolic-Music-Corpora/README.md', 'https://github.com/MZehren/M-DJCUE/blob/master/README.md', 'https://github.com/barisbozkurt/MASTmelody_dataset/blob/master/README.md', 'https://github.com/johnglover/modal/blob/master/README.md', 'https://github.com/MTG/mtg-jamendo-dataset/blob/master/README.md', 'https://github.com/chrisdonahue/nesmdb/blob/master/README.md', 'https://github.com/SEILSdataset/SEILSdataset/blob/master/README.md']\n",
    "ismir_df = scrape_github(ismir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ismir_df['Title'] = ismir_df['Title'].str.lower().str.strip()\n",
    "ismir_df['Description'] = ismir_df['Description'].str.lower().str.strip()\n",
    "ismir_df['URL'] = ismir_df['URL'].str.lower().str.strip()\n",
    "ismir_df.loc[6, 'Title'] = 'da-tacos'\n",
    "ismir_df.loc[9, 'Title'] = 'guitarsolodetection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape URLs for title and text \n",
    "\n",
    "def scrape_links(link_list):\n",
    "    links = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "    for link in link_list:\n",
    "        URL = link\n",
    "        try:\n",
    "            page = requests.get(URL)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            pass\n",
    "        except Exception:\n",
    "            continue\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        if soup and soup.find('head') and soup.find('body') is not None:\n",
    "            title = ' '.join([t.text for t in soup.find('head').find_all('title')]).strip()\n",
    "            text = ' '.join([p.text for p in soup.find('body').find_all('p')]).strip()\n",
    "            new_row = {'Title': title, 'Description': text, 'URL': URL.strip()}\n",
    "            links = links.append(new_row, ignore_index=True)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mji_additions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mji_new = mji_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mji_new['Title'] = mji_new['Title'].str.lower().str.strip()\n",
    "mji_new['Description'] = mji_new['Description'].str.lower().str.strip()\n",
    "mji_new['URL'] = mji_new['URL'].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = pd.read_pickle('/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/DATA/TWITTER_SEARCHES/MJI BIGRAMS (NEGATIVE)/twitter_music_research.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_link_list = [link for link in twitter['url'] if 'twitter' and 'demd.ro' not in link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_add = scrape_links(twitter_link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_add = links_to_add[links_to_add.Description != ''].reset_index(drop=True)\n",
    "links_to_add = links_to_add[links_to_add.Title != '404: Page Not Found'].reset_index(drop=True)\n",
    "links_to_add = links_to_add[links_to_add.Title != 'Not Acceptable!'].reset_index(drop=True)\n",
    "links_to_add = links_to_add[links_to_add.Title != 'Attention Required! | Cloudflare'].reset_index(drop=True)\n",
    "links_to_add\n",
    "#links_to_add.to_pickle('/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/DATA/TWITTER_SEARCHES/MJI BIGRAMS (NEGATIVE)/twitter_music_oral_history_link_scrape.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
