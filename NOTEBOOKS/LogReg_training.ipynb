{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**README**\n",
    "- This notebook includes code to test different training sets, encoding, and logistric regression functions with a focus on binary choice for relevance \n",
    "- The primary use was to evaluate which would be best to move forward \n",
    "- See results in part 3 below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports + path\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "path = '/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: \n",
    "- Create a training set using musoW and MJI datasets. \n",
    "- Three versions of the training set: a base version containing the datasets as of Feb 2022 (unbalanced, musoW has 3 times as many inputs as MJI); an extended version containing additional inputs yet to be included in original datasets (reduces inbalance closer to 2/1); an even set w/ random sampling of the musoW set to match length of the MJI set.\n",
    "- The additional inputs come from two sources: for musoW they are taken from the Ismir list and all github repos, for MJI they are taken from a list of resources to be added to the database and come from various sources. \n",
    "- There are three training features: Title, Description, URL. \n",
    "- We also create some permutations for testing: two versions of the even set (one from the base set, one from extended set), and additional versions of the base, extended, and even sets that combine all three features (title, description, URL) into one. \n",
    "- The baseline difference for the model to pick up is that musoW includes only music archives w/ datasets while MJI includes all sorts of music archives, regardless of whether or not there is a dataset. This distinction is quite subtle and in order to avoid confusing the model we check for duplicates at various stages of the training set creation to ensure that MJI and musoW contain different inputs. \n",
    "- We also create a prediction set using the additions to the base set to help test the model (predictions done in a separate notebook). Note that we do keep a few inputs in the MJI additions that would technically fit within musoW to see if they end up correctly predicted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read mji csv and grab needed columns\n",
    "df_mji = pd.read_csv(path+'MJI/MJI_data.csv', keep_default_na=False, dtype='string')\n",
    "df_mji_small = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "df_mji_small['Title'] = df_mji['Title'].str.lower().str.strip()\n",
    "df_mji_small['Description'] = df_mji['Description'].str.lower().str.strip()\n",
    "df_mji_small['URL'] = df_mji['URL'].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read musow json dump and grab needed columns\n",
    "with open(path+'MUSOW/musow_name_desc_url_cat.json') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "musow_names = [result['name']['value'].strip().lower() for result in data['results']['bindings']]\n",
    "musow_desc = [result['description']['value'].strip().lower() for result in data['results']['bindings']]\n",
    "musow_url = [result['url']['value'].strip().lower() for result in data['results']['bindings']]\n",
    "df_musow = pd.DataFrame(columns=['Title', 'Description', 'URL'])\n",
    "df_musow['Title'] = musow_names\n",
    "df_musow['Description'] = musow_desc\n",
    "df_musow['URL'] = musow_url\n",
    "df_musow = df_musow.astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove musow duplicates from MJI set \n",
    "mji_training_set = df_mji_small[~df_mji_small['Title'].isin(df_musow['Title'])].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create positive and negative base sets, add target column \n",
    "positive_df = df_musow.copy()\n",
    "positive_df['Target'] = '1'\n",
    "negative_df = mji_training_set.copy()\n",
    "negative_df['Target'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create positive and negative sets w/ additions, add target column \n",
    "ismir_df = pd.read_pickle(path+'GH_PICKLES/ismir.pkl')\n",
    "ismir_df = ismir_df[~ismir_df['Title'].isin(df_musow['Title'])].dropna() \n",
    "positive_df_adds = pd.concat([df_musow, ismir_df]).reset_index(drop=True)\n",
    "positive_df_adds = positive_df_adds.drop_duplicates(['Title'], keep='last')\n",
    "positive_df_adds['Target'] = '1'\n",
    "mji_additions_1 = pd.read_csv(path+'MJI/MJI_additions_for_LR.csv')\n",
    "mji_additions_1['Title'] = mji_additions_1['Title'].str.lower().str.strip()\n",
    "mji_additions_1['Description'] = mji_additions_1['Description'].str.lower().str.strip()\n",
    "mji_additions_1['URL'] = mji_additions_1['URL'].str.lower().str.strip()\n",
    "mji_additions_1 = mji_additions_1[~mji_additions_1['Title'].isin(df_musow['Title'])].dropna()\n",
    "mji_additions_1 = mji_additions_1[~mji_additions_1['Title'].isin(mji_training_set['Title'])].dropna()\n",
    "negative_df_adds = pd.concat([mji_training_set, mji_additions_1]).reset_index(drop=True)\n",
    "negative_df_adds = negative_df_adds.drop_duplicates(['Title'], keep='last')\n",
    "negative_df_adds['Target'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a prediction set for later using additions to base set \n",
    "prediction_set = pd.concat([ismir_df, mji_additions_1]).reset_index(drop=True)\n",
    "prediction_set.to_pickle(path+'LOGREG_RELEVANCE/base_prediction_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the base and extended training sets, pickle for reuse\n",
    "training_set = pd.concat([positive_df, negative_df])\n",
    "training_set['Target'] = training_set['Target'].astype('int')\n",
    "training_set = training_set.reset_index(drop=True)\n",
    "training_set_adds = pd.concat([positive_df_adds, negative_df_adds])\n",
    "training_set_adds['Target'] = training_set_adds['Target'].astype('int')\n",
    "training_set_adds = training_set_adds.reset_index(drop=True)\n",
    "training_set.to_pickle(path+'LOGREG_RELEVANCE/trainingset.pkl')\n",
    "training_set_adds.to_pickle(path+'LOGREG_RELEVANCE/trainingset_extended.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the even set, one base and one extended, pickle for reuse \n",
    "positive_df_2 = positive_df.sample(n=128, random_state=1)\n",
    "training_set_even = pd.concat([positive_df_2, negative_df])\n",
    "training_set_even['Target'] = training_set_even['Target'].astype('int')\n",
    "training_set_even = training_set_even.reset_index(drop=True)\n",
    "training_set_even.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even.pkl')\n",
    "\n",
    "positive_df_3 = positive_df.sample(n=272, random_state=1)\n",
    "training_set_even_adds = pd.concat([positive_df_3, negative_df_adds])\n",
    "training_set_even_adds['Target'] = training_set_even_adds['Target'].astype('int')\n",
    "training_set_even_adds = training_set_even_adds.reset_index(drop=True)\n",
    "training_set_even_adds.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create another version of base/extended sets with combined features, pickle for reuse\n",
    "\n",
    "training_set_comb = training_set.copy()\n",
    "training_set_adds_comb = training_set_adds.copy()\n",
    "training_set_comb_even = training_set_even.copy()\n",
    "training_set_adds_comb_even = training_set_even_adds.copy()\n",
    "\n",
    "#create combined columns for desc+headline and desc+headline_url\n",
    "def tokenize_url(url:str):\n",
    "    url=url.replace(\"https\",\"\")\n",
    "    url=url.replace(\"http\",\"\")\n",
    "    url=url.replace(\"www\",\"\")   \n",
    "    url=re.sub(\"(\\W|_)+\",\" \",url)\n",
    "    return url\n",
    "\n",
    "#create tokenized URL field\n",
    "training_set_comb['tokenized_url']=training_set_comb['URL'].apply(lambda x:tokenize_url(x))\n",
    "#description + tokenized url\n",
    "training_set_comb['text_desc_headline_url'] = training_set_comb['Description'] + ' '+ training_set_comb['Title']+\" \" + training_set_comb['tokenized_url']\n",
    "training_set_comb.drop(['tokenized_url', 'Title', 'Description', 'URL'], inplace=True, axis=1)\n",
    "\n",
    "#create tokenized URL field\n",
    "training_set_adds_comb['tokenized_url']=training_set_adds_comb['URL'].apply(lambda x:tokenize_url(x))\n",
    "#description + tokenized url\n",
    "training_set_adds_comb['text_desc_headline_url'] = training_set_adds_comb['Description'] + ' '+ training_set_adds_comb['Title']+\" \" + training_set_adds_comb['tokenized_url']\n",
    "training_set_adds_comb.drop(['tokenized_url', 'Title', 'Description', 'URL'], inplace=True, axis=1)\n",
    "\n",
    "#create tokenized URL field\n",
    "training_set_comb_even['tokenized_url']=training_set_comb_even['URL'].apply(lambda x:tokenize_url(x))\n",
    "#description + tokenized url\n",
    "training_set_comb_even['text_desc_headline_url'] = training_set_comb_even['Description'] + ' '+ training_set_comb_even['Title']+\" \" + training_set_comb_even['tokenized_url']\n",
    "training_set_comb_even.drop(['tokenized_url', 'Title', 'Description', 'URL'], inplace=True, axis=1)\n",
    "\n",
    "#create tokenized URL field\n",
    "training_set_adds_comb_even['tokenized_url']=training_set_adds_comb_even['URL'].apply(lambda x:tokenize_url(x))\n",
    "#description + tokenized url\n",
    "training_set_adds_comb_even['text_desc_headline_url'] = training_set_adds_comb_even['Description'] + ' '+ training_set_adds_comb_even['Title']+\" \" + training_set_adds_comb_even['tokenized_url']\n",
    "training_set_adds_comb_even.drop(['tokenized_url', 'Title', 'Description', 'URL'], inplace=True, axis=1)\n",
    "\n",
    "training_set_comb.to_pickle(path+'LOGREG_RELEVANCE/trainingset_comb.pkl')\n",
    "training_set_adds_comb.to_pickle(path+'LOGREG_RELEVANCE/trainingset_extended_comb.pkl')\n",
    "training_set_comb_even.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even_comb.pkl')\n",
    "training_set_adds_comb_even.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even_comb_extended.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of base set: 626 \n",
      "Mean length of description in base set is: 343.55271565495207 \n",
      "Size of extended set: 786 \n",
      "Mean length of description in extended set is: 541.9580152671756 \n",
      "Size of even set: 256 \n",
      "Mean length of description in even set is: 410.11328125 \n",
      "Size of even extended set: 544 \n",
      "Mean length of description in even extended set is: 525.5183823529412\n",
      "\n",
      "Mean length of description in base set combined is: 408.073482428115 \n",
      "Mean length of description in extended set combined is: 608.5534351145038 \n",
      "Mean length of description in even set combined is: 474.6171875 \n",
      "Mean length of description in even set extended combined is: 590.6360294117648\n"
     ]
    }
   ],
   "source": [
    "#print some base stats to keep track of changes: training set size, avg length of description (main feature)\n",
    "\n",
    "print('Size of base set:', len(training_set.index), '\\nMean length of description in base set is:', training_set['Description'].str.len().mean(), '\\nSize of extended set:', len(training_set_adds.index), '\\nMean length of description in extended set is:', training_set_adds['Description'].str.len().mean(), '\\nSize of even set:', len(training_set_even.index), '\\nMean length of description in even set is:', training_set_even['Description'].str.len().mean(), '\\nSize of even extended set:', len(training_set_even_adds.index), '\\nMean length of description in even extended set is:', training_set_even_adds['Description'].str.len().mean())\n",
    "\n",
    "#print some base stats to keep track of changes: avg length of combined desc+title+url (main feature)\n",
    "\n",
    "print('\\nMean length of description in base set combined is:', training_set_comb['text_desc_headline_url'].str.len().mean(), '\\nMean length of description in extended set combined is:', training_set_adds_comb['text_desc_headline_url'].str.len().mean(), '\\nMean length of description in even set combined is:', training_set_comb_even['text_desc_headline_url'].str.len().mean(), '\\nMean length of description in even set extended combined is:', training_set_adds_comb_even['text_desc_headline_url'].str.len().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2:\n",
    "- Try different unsupervised encoding approaches for categorical variables including OrdinalEncoder, Label Encoding, Tf-Idf, BackwardDifferenceEncoder \n",
    "- Ordinal, Label, and BackwardDifference encoding use title, desc, and url + desc only + combined title/desc/url as features depending on source version of training set (the desc only encoding for Ordinal/Label encoding happens at the feature creation step rather than encoding step)\n",
    "- Tf-Idf encoding uses only desc and combined title/desc/url as features (tf-idf encoding happens last as we can't easily pickle the resulting matrices). Earlier instances used tdidfvectorizer but this messed up the prediction so instead we use the countvect + tfidf transformer.\n",
    "- All these variations are intended to help assess if dropped or combined features make any difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode categorial variables w/ OrdinalEncoder, pickle\n",
    "ord_enc = OrdinalEncoder()\n",
    "\n",
    "#base set \n",
    "training_set_ordinal = training_set.copy()\n",
    "training_set_ordinal[\"Title_encoded\"] = ord_enc.fit_transform(training_set_ordinal[[\"Title\"]])\n",
    "training_set_ordinal[\"Desc_encoded\"] = ord_enc.fit_transform(training_set_ordinal[[\"Description\"]])\n",
    "training_set_ordinal[\"URL_encoded\"] = ord_enc.fit_transform(training_set_ordinal[[\"URL\"]])\n",
    "training_set_ordinal.to_pickle(path+'LOGREG_RELEVANCE/trainingset_ordinal.pkl')\n",
    "\n",
    "#base set comb\n",
    "training_set_ordinal_comb = training_set_comb.copy()\n",
    "training_set_ordinal_comb[\"text_desc_headline_url_encoded\"] = ord_enc.fit_transform(training_set_ordinal_comb[[\"text_desc_headline_url\"]])\n",
    "training_set_ordinal_comb.to_pickle(path+'LOGREG_RELEVANCE/trainingset_comb_ordinal.pkl')\n",
    "\n",
    "#extended set \n",
    "training_set_ordinal_adds = training_set_adds.copy()\n",
    "training_set_ordinal_adds[\"Title_encoded\"] = ord_enc.fit_transform(training_set_ordinal_adds[[\"Title\"]])\n",
    "training_set_ordinal_adds[\"Desc_encoded\"] = ord_enc.fit_transform(training_set_ordinal_adds[[\"Description\"]])\n",
    "training_set_ordinal_adds[\"URL_encoded\"] = ord_enc.fit_transform(training_set_ordinal_adds[[\"URL\"]])\n",
    "training_set_ordinal_adds.to_pickle(path+'LOGREG_RELEVANCE/trainingset_extended_ordinal.pkl')\n",
    "\n",
    "#extended set comb \n",
    "training_set_ordinal_adds_comb = training_set_adds_comb.copy()\n",
    "training_set_ordinal_adds_comb[\"text_desc_headline_url_encoded\"] = ord_enc.fit_transform(training_set_ordinal_adds_comb[[\"text_desc_headline_url\"]])\n",
    "training_set_ordinal_adds_comb.to_pickle(path+'LOGREG_RELEVANCE/trainingset_extended_comb_ordinal.pkl')\n",
    "\n",
    "#even set \n",
    "training_set_ordinal_even = training_set_even.copy()\n",
    "training_set_ordinal_even[\"Title_encoded\"] = ord_enc.fit_transform(training_set_ordinal_even[[\"Title\"]])\n",
    "training_set_ordinal_even[\"Desc_encoded\"] = ord_enc.fit_transform(training_set_ordinal_even[[\"Description\"]])\n",
    "training_set_ordinal_even[\"URL_encoded\"] = ord_enc.fit_transform(training_set_ordinal_even[[\"URL\"]])\n",
    "training_set_ordinal_even.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even_ordinal.pkl')\n",
    "\n",
    "#even set comb \n",
    "training_set_ordinal_even_comb = training_set_comb_even.copy()\n",
    "training_set_ordinal_even_comb[\"text_desc_headline_url_encoded\"] = ord_enc.fit_transform(training_set_ordinal_even_comb[[\"text_desc_headline_url\"]])\n",
    "training_set_ordinal_even_comb.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even_comb_ordinal.pkl')\n",
    "\n",
    "#extended even set \n",
    "training_set_ordinal_adds_even = training_set_even_adds.copy()\n",
    "training_set_ordinal_adds_even[\"Title_encoded\"] = ord_enc.fit_transform(training_set_ordinal_adds_even[[\"Title\"]])\n",
    "training_set_ordinal_adds_even[\"Desc_encoded\"] = ord_enc.fit_transform(training_set_ordinal_adds_even[[\"Description\"]])\n",
    "training_set_ordinal_adds_even[\"URL_encoded\"] = ord_enc.fit_transform(training_set_ordinal_adds_even[[\"URL\"]])\n",
    "training_set_ordinal_adds_even.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended_ordinal.pkl')\n",
    "\n",
    "#extended even set comb \n",
    "training_set_ordinal_adds_even_comb = training_set_adds_comb_even.copy()\n",
    "training_set_ordinal_adds_even_comb[\"text_desc_headline_url_encoded\"] = ord_enc.fit_transform(training_set_ordinal_adds_even_comb[[\"text_desc_headline_url\"]])\n",
    "training_set_ordinal_adds_even_comb.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended_comb_ordinal.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode cat variables with label encoding, pickle \n",
    "\n",
    "#base set \n",
    "training_set_label = training_set.copy()\n",
    "training_set_label[\"Title\"] = training_set_label[\"Title\"].astype('category')\n",
    "training_set_label[\"Description\"] = training_set_label[\"Description\"].astype('category')\n",
    "training_set_label[\"URL\"] = training_set_label[\"URL\"].astype('category')\n",
    "training_set_label[\"Title_cat\"] = training_set_label[\"Title\"].cat.codes\n",
    "training_set_label[\"Desc_cat\"] = training_set_label[\"Description\"].cat.codes\n",
    "training_set_label[\"URL_cat\"] = training_set_label[\"URL\"].cat.codes\n",
    "training_set_label.to_pickle(path+'LOGREG_RELEVANCE/trainingset_label.pkl')\n",
    "\n",
    "#base set comb\n",
    "training_set_label_comb = training_set_comb.copy()\n",
    "training_set_label_comb[\"text_desc_headline_url\"] = training_set_label_comb[\"text_desc_headline_url\"].astype('category')\n",
    "training_set_label_comb[\"text_desc_headline_url_cat\"] = training_set_label_comb[\"text_desc_headline_url\"].cat.codes\n",
    "training_set_label_comb.to_pickle(path+'LOGREG_RELEVANCE/trainingset_comb_label.pkl')\n",
    "\n",
    "#extended set \n",
    "training_set_label_adds = training_set_adds.copy()\n",
    "training_set_label_adds[\"Title\"] = training_set_label_adds[\"Title\"].astype('category')\n",
    "training_set_label_adds[\"Description\"] = training_set_label_adds[\"Description\"].astype('category')\n",
    "training_set_label_adds[\"URL\"] = training_set_label_adds[\"URL\"].astype('category')\n",
    "training_set_label_adds[\"Title_cat\"] = training_set_label_adds[\"Title\"].cat.codes\n",
    "training_set_label_adds[\"Desc_cat\"] = training_set_label_adds[\"Description\"].cat.codes\n",
    "training_set_label_adds[\"URL_cat\"] = training_set_label_adds[\"URL\"].cat.codes\n",
    "training_set_label_adds.to_pickle(path+'LOGREG_RELEVANCE/trainingset_extended_label.pkl')\n",
    "\n",
    "#extended set comb\n",
    "training_set_label_adds_comb = training_set_adds_comb.copy()\n",
    "training_set_label_adds_comb[\"text_desc_headline_url\"] = training_set_label_adds_comb[\"text_desc_headline_url\"].astype('category')\n",
    "training_set_label_adds_comb[\"text_desc_headline_url_cat\"] = training_set_label_adds_comb[\"text_desc_headline_url\"].cat.codes\n",
    "training_set_label_adds_comb.to_pickle(path+'LOGREG_RELEVANCE/trainingset_extended_comb_label.pkl')\n",
    "\n",
    "#even set \n",
    "training_set_label_even = training_set_even.copy()\n",
    "training_set_label_even[\"Title\"] = training_set_label_even[\"Title\"].astype('category')\n",
    "training_set_label_even[\"Description\"] = training_set_label_even[\"Description\"].astype('category')\n",
    "training_set_label_even[\"URL\"] = training_set_label_even[\"URL\"].astype('category')\n",
    "training_set_label_even[\"Title_cat\"] = training_set_label_even[\"Title\"].cat.codes\n",
    "training_set_label_even[\"Desc_cat\"] = training_set_label_even[\"Description\"].cat.codes\n",
    "training_set_label_even[\"URL_cat\"] = training_set_label_even[\"URL\"].cat.codes\n",
    "training_set_label_even.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even_label.pkl')\n",
    "\n",
    "#even set comb \n",
    "training_set_label_even_comb = training_set_comb_even.copy()\n",
    "training_set_label_even_comb[\"text_desc_headline_url\"] = training_set_label_even_comb[\"text_desc_headline_url\"].astype('category')\n",
    "training_set_label_even_comb[\"text_desc_headline_url_cat\"] = training_set_label_even_comb[\"text_desc_headline_url\"].cat.codes\n",
    "training_set_label_even_comb.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even_comb_label.pkl')\n",
    "\n",
    "#extended even set \n",
    "training_set_label_adds_even = training_set_even_adds.copy()\n",
    "training_set_label_adds_even[\"Title\"] = training_set_label_adds_even[\"Title\"].astype('category')\n",
    "training_set_label_adds_even[\"Description\"] = training_set_label_adds_even[\"Description\"].astype('category')\n",
    "training_set_label_adds_even[\"URL\"] = training_set_label_adds_even[\"URL\"].astype('category')\n",
    "training_set_label_adds_even[\"Title_cat\"] = training_set_label_adds_even[\"Title\"].cat.codes\n",
    "training_set_label_adds_even[\"Desc_cat\"] = training_set_label_adds_even[\"Description\"].cat.codes\n",
    "training_set_label_adds_even[\"URL_cat\"] = training_set_label_adds_even[\"URL\"].cat.codes\n",
    "training_set_label_adds_even.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended_label.pkl')\n",
    "\n",
    "#extended even set comb \n",
    "training_set_label_adds_even_comb = training_set_adds_comb_even.copy()\n",
    "training_set_label_adds_even_comb[\"text_desc_headline_url\"] = training_set_label_adds_even_comb[\"text_desc_headline_url\"].astype('category')\n",
    "training_set_label_adds_even_comb[\"text_desc_headline_url_cat\"] = training_set_label_adds_even_comb[\"text_desc_headline_url\"].cat.codes\n",
    "training_set_label_adds_even_comb.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended_comb_label.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode w/ CE's BackwardDifferenceEncoder, pickle \n",
    "\n",
    "#base set \n",
    "training_set_ce = training_set.copy()\n",
    "# Specify the columns to encode then fit and transform\n",
    "encoder_base = ce.BackwardDifferenceEncoder(cols=['Title', 'Description', 'URL'])\n",
    "training_set_ce = encoder_base.fit_transform(training_set_ce, verbose=1)\n",
    "training_set_ce.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_ce.to_pickle(path+'LOGREG_RELEVANCE/trainingset_BD.pkl')\n",
    "\n",
    "#base set desc only\n",
    "training_set_ce_desc = training_set.copy()\n",
    "encoder_base_desc = ce.BackwardDifferenceEncoder(cols=['Description'])\n",
    "training_set_ce_desc = encoder_base_desc.fit_transform(training_set_ce_desc, verbose=1)\n",
    "training_set_ce_desc.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_ce_desc.to_pickle(path+'LOGREG_RELEVANCE/trainingset_BD_desc.pkl')\n",
    "\n",
    "#base set comb\n",
    "training_set_comb_ce = training_set_comb.copy()\n",
    "encoder_comb = ce.BackwardDifferenceEncoder(cols=['text_desc_headline_url'])\n",
    "training_set_comb_ce = encoder_comb.fit_transform(training_set_comb_ce, verbose=1)\n",
    "training_set_comb_ce.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_comb_ce.to_pickle(path+'LOGREG_RELEVANCE/trainingset_comb_BD.pkl')\n",
    "\n",
    "#extended set \n",
    "training_set_adds_ce = training_set_adds.copy()\n",
    "training_set_adds_ce = encoder_base.fit_transform(training_set_adds_ce, verbose=1)\n",
    "training_set_adds_ce.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_adds_ce.to_pickle(path+'LOGREG_RELEVANCE/trainingset_extended_BD.pkl')\n",
    "\n",
    "#extended set desc only \n",
    "training_set_adds_ce_desc = training_set_adds.copy()\n",
    "training_set_adds_ce_desc = encoder_base_desc.fit_transform(training_set_adds_ce_desc, verbose=1)\n",
    "training_set_adds_ce_desc.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_adds_ce_desc.to_pickle(path+'LOGREG_RELEVANCE/trainingset_extended_BD_desc.pkl')\n",
    "\n",
    "#extended set comb\n",
    "training_set_adds_comb_ce = training_set_adds_comb.copy()\n",
    "training_set_adds_comb_ce = encoder_comb.fit_transform(training_set_adds_comb_ce, verbose=1)\n",
    "training_set_adds_comb_ce.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_adds_comb_ce.to_pickle(path+'LOGREG_RELEVANCE/trainingset_extended_comb_BD.pkl')\n",
    "\n",
    "#even set \n",
    "training_set_ce_even = training_set_even.copy()\n",
    "training_set_ce_even = encoder_base.fit_transform(training_set_ce_even, verbose=1)\n",
    "training_set_ce_even.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_ce_even.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even_BD.pkl')\n",
    "\n",
    "#even set desc only \n",
    "training_set_ce_even_desc = training_set_even.copy()\n",
    "training_set_ce_even_desc = encoder_base_desc.fit_transform(training_set_ce_even_desc, verbose=1)\n",
    "training_set_ce_even_desc.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_ce_even_desc.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even_BD_desc.pkl')\n",
    "\n",
    "#even set comb\n",
    "training_set_ce_even_comb = training_set_comb_even.copy()\n",
    "training_set_ce_even_comb = encoder_comb.fit_transform(training_set_ce_even_comb, verbose=1)\n",
    "training_set_ce_even_comb.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_ce_even_comb.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even_comb_BD.pkl')\n",
    "\n",
    "#extended even set \n",
    "training_set_ce_even_extended = training_set_even_adds.copy()\n",
    "training_set_ce_even_extended = encoder_base.fit_transform(training_set_ce_even_extended, verbose=1)\n",
    "training_set_ce_even_extended.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_ce_even_extended.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended_BD.pkl')\n",
    "\n",
    "#extended even set desc only \n",
    "training_set_ce_even_extended_desc = training_set_even_adds.copy()\n",
    "training_set_ce_even_extended_desc = encoder_base_desc.fit_transform(training_set_ce_even_extended_desc, verbose=1)\n",
    "training_set_ce_even_extended_desc.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_ce_even_extended_desc.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended_BD_desc.pkl')\n",
    "\n",
    "# extendedeven set comb\n",
    "training_set_ce_even_extended_comb = training_set_adds_comb_even.copy()\n",
    "training_set_ce_even_extended_comb = encoder_comb.fit_transform(training_set_ce_even_extended_comb, verbose=1)\n",
    "training_set_ce_even_extended_comb.drop(['intercept'], inplace=True, axis=1)\n",
    "training_set_ce_even_extended_comb.to_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended_comb_BD.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS CHUNK TO LOAD ALL ABOVE CODE\n",
    "#base sets \n",
    "training_set = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset.pkl')\n",
    "training_set_adds = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_extended.pkl')\n",
    "training_set_even = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even.pkl')\n",
    "training_set_even_adds = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended.pkl')\n",
    "training_set_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_comb.pkl')\n",
    "training_set_adds_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_extended_comb.pkl')\n",
    "training_set_comb_even = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_comb.pkl')\n",
    "training_set_adds_comb_even = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_comb_extended.pkl')\n",
    "\n",
    "#ordinal sets \n",
    "training_set_ordinal = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_ordinal.pkl')\n",
    "training_set_ordinal_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_comb_ordinal.pkl')\n",
    "training_set_ordinal_adds = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_extended_ordinal.pkl')\n",
    "training_set_ordinal_adds_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_extended_comb_ordinal.pkl')\n",
    "training_set_ordinal_even = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_ordinal.pkl')\n",
    "training_set_ordinal_even_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_comb_ordinal.pkl')\n",
    "training_set_ordinal_adds_even = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended_ordinal.pkl')\n",
    "training_set_ordinal_adds_even_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended_comb_ordinal.pkl')\n",
    "\n",
    "#label sets \n",
    "training_set_label = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_label.pkl')\n",
    "training_set_label_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_comb_label.pkl')\n",
    "training_set_label_adds = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_extended_label.pkl')\n",
    "training_set_label_adds_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_extended_comb_label.pkl')\n",
    "training_set_label_even = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_label.pkl')\n",
    "training_set_label_even_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_comb_label.pkl')\n",
    "training_set_label_adds_even = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended_label.pkl')\n",
    "training_set_label_adds_even_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended_comb_label.pkl')\n",
    "\n",
    "#BD sets \n",
    "training_set_ce = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_BD.pkl')\n",
    "training_set_ce_desc = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_BD_desc.pkl')\n",
    "training_set_comb_ce = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_comb_BD.pkl')\n",
    "training_set_adds_ce = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_extended_BD.pkl')\n",
    "training_set_adds_ce_desc = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_extended_BD_desc.pkl')\n",
    "training_set_adds_comb_ce = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_extended_comb_BD.pkl')\n",
    "training_set_ce_even = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_BD.pkl')\n",
    "training_set_ce_even_desc = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_BD_desc.pkl')\n",
    "training_set_ce_even_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_comb_BD.pkl')\n",
    "training_set_ce_even_extended = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended_BD.pkl')\n",
    "training_set_ce_even_extended_desc = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended_BD_desc.pkl')\n",
    "training_set_ce_even_extended_comb = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended_comb_BD.pkl')\n",
    "\n",
    "#prediction set\n",
    "prediction_set = pd.read_pickle(path+'LOGREG_RELEVANCE/base_prediction_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS NEXT - tfidf encoding w/ count vect \n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "#base set\n",
    "base_train_counts = count_vect.fit_transform(training_set['Description'])\n",
    "training_set_tfidf = tfidf_transformer.fit_transform(base_train_counts)\n",
    "\n",
    "#base set comb\n",
    "base_comb_train_counts = count_vect.fit_transform(training_set_comb['text_desc_headline_url'])\n",
    "training_set_comb_tfidf = tfidf_transformer.fit_transform(base_comb_train_counts)\n",
    "\n",
    "#extended set  \n",
    "extended_train_counts = count_vect.fit_transform(training_set_adds['Description'])\n",
    "training_set_adds_tfidf = tfidf_transformer.fit_transform(extended_train_counts)\n",
    "\n",
    "#extended set comb\n",
    "extended_comb_train_counts = count_vect.fit_transform(training_set_adds_comb['text_desc_headline_url'])\n",
    "training_set_adds_comb_tfidf = tfidf_transformer.fit_transform(extended_comb_train_counts)\n",
    "\n",
    "#even set  \n",
    "even_train_counts = count_vect.fit_transform(training_set_even['Description'])\n",
    "training_set_tfidf_even = tfidf_transformer.fit_transform(even_train_counts)\n",
    "\n",
    "#even set comb\n",
    "even_comb_train_counts = count_vect.fit_transform(training_set_comb_even['text_desc_headline_url'])\n",
    "training_set_even_comb_tfidf = tfidf_transformer.fit_transform(even_comb_train_counts)\n",
    "\n",
    "#extended even set  \n",
    "even_adds_train_counts = count_vect.fit_transform(training_set_even_adds['Description'])\n",
    "training_set_tfidf_even_adds = tfidf_transformer.fit_transform(even_adds_train_counts)\n",
    "\n",
    "#extended even set comb\n",
    "even_adds_comb_train_counts = count_vect.fit_transform(training_set_adds_comb_even['text_desc_headline_url'])\n",
    "training_set_even_adds_comb_tfidf = tfidf_transformer.fit_transform(even_adds_comb_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS NEXT THEN YOU CAN MOVE TO LOGREG RUNS - set features and targets for all encoding types \n",
    "\n",
    "#ORDINAL  \n",
    "features_ord = ['Title_encoded', 'Desc_encoded', 'URL_encoded']\n",
    "features_ord_comb = ['text_desc_headline_url_encoded']\n",
    "\n",
    "#base set\n",
    "x_ord = training_set_ordinal[features_ord] # Features\n",
    "y_ord = training_set_ordinal.Target # Target variable\n",
    "\n",
    "#base set desc only \n",
    "training_set_ordinal_desc = training_set_ordinal.copy()\n",
    "training_set_ordinal_desc.drop(['Title', 'URL', 'Title_encoded', 'Description', 'URL_encoded'], inplace=True, axis=1)\n",
    "single_feat = ['Desc_encoded']\n",
    "x_ord_desc = training_set_ordinal_desc[single_feat] # Features\n",
    "y_ord_desc = training_set_ordinal_desc.Target # Target variable\n",
    "\n",
    "#base set comb \n",
    "x_ord_comb = training_set_ordinal_comb[features_ord_comb] # Features\n",
    "y_ord_comb = training_set_ordinal_comb.Target # Target variable\n",
    "\n",
    "#extended set \n",
    "x_ord_adds = training_set_ordinal_adds[features_ord] # Features\n",
    "y_ord_adds = training_set_ordinal_adds.Target # Target variable\n",
    "\n",
    "#extended set desc only\n",
    "training_set_ordinal_adds_desc = training_set_ordinal_adds.copy()\n",
    "training_set_ordinal_adds_desc.drop(['Title', 'URL', 'Title_encoded', 'Description', 'URL_encoded'], inplace=True, axis=1)\n",
    "x_ord_adds_desc = training_set_ordinal_adds_desc[single_feat] # Features\n",
    "y_ord_adds_desc = training_set_ordinal_adds_desc.Target # Target variable\n",
    "\n",
    "#extended set comb \n",
    "x_ord_adds_comb = training_set_ordinal_adds_comb[features_ord_comb] # Features\n",
    "y_ord_adds_comb = training_set_ordinal_adds_comb.Target # Target variable\n",
    "\n",
    "#even set \n",
    "x_ord_even = training_set_ordinal_even[features_ord] # Features\n",
    "y_ord_even = training_set_ordinal_even.Target # Target variable\n",
    "\n",
    "#even set desc only \n",
    "training_set_even_desc = training_set_ordinal_even.copy()\n",
    "training_set_even_desc.drop(['Title', 'URL', 'Title_encoded', 'Description', 'URL_encoded'], inplace=True, axis=1)\n",
    "x_ord_even_desc = training_set_even_desc[single_feat] # Features\n",
    "y_ord_even_desc = training_set_even_desc.Target # Target variable\n",
    "\n",
    "#even set comb\n",
    "x_ord_even_comb = training_set_ordinal_even_comb[features_ord_comb] # Features\n",
    "y_ord_even_comb = training_set_ordinal_even_comb.Target # Target variable\n",
    "\n",
    "#extended even set \n",
    "x_ord_even_adds = training_set_ordinal_adds_even[features_ord] # Features\n",
    "y_ord_even_adds = training_set_ordinal_adds_even.Target # Target variable\n",
    "\n",
    "#extended even set desc only \n",
    "training_set_even_adds_desc = training_set_ordinal_adds_even.copy()\n",
    "training_set_even_adds_desc.drop(['Title', 'URL', 'Title_encoded', 'Description', 'URL_encoded'], inplace=True, axis=1)\n",
    "x_ord_even_adds_desc = training_set_even_adds_desc[single_feat] # Features\n",
    "y_ord_even_adds_desc = training_set_even_adds_desc.Target # Target variable\n",
    "\n",
    "#extended even set comb\n",
    "x_ord_even_adds_comb = training_set_ordinal_adds_even_comb[features_ord_comb] # Features\n",
    "y_ord_even_adds_comb = training_set_ordinal_adds_even_comb.Target # Target variable\n",
    "\n",
    "#LABEL \n",
    "features_label = ['Title_cat', 'Desc_cat', 'URL_cat']\n",
    "features_label_comb = ['text_desc_headline_url_cat']\n",
    "\n",
    "#base set\n",
    "x_label = training_set_label[features_label] # Features\n",
    "y_label = training_set_label.Target # Target variable\n",
    "\n",
    "#base set desc only \n",
    "training_set_label_desc = training_set_label.copy()\n",
    "training_set_label_desc.drop(['Title', 'Description', 'URL', 'Title_cat', 'URL_cat'], inplace=True, axis=1)\n",
    "single_feat_label = ['Desc_cat']\n",
    "x_label_desc = training_set_label_desc[single_feat_label] # Features\n",
    "y_label_desc = training_set_label_desc.Target # Target variable\n",
    "\n",
    "#base set comb\n",
    "x_label_comb = training_set_label_comb[features_label_comb] # Features\n",
    "y_label_comb = training_set_label_comb.Target # Target variable\n",
    "\n",
    "#extended set\n",
    "x_label_adds = training_set_label_adds[features_label] # Features\n",
    "y_label_adds = training_set_label_adds.Target # Target variable\n",
    "\n",
    "#extended set desc only \n",
    "training_set_label_adds_desc = training_set_label_adds.copy()\n",
    "training_set_label_adds_desc.drop(['Title', 'Description', 'URL', 'Title_cat', 'URL_cat'], inplace=True, axis=1)\n",
    "x_label_adds_desc = training_set_label_adds_desc[single_feat_label] # Features\n",
    "y_label_adds_desc = training_set_label_adds_desc.Target # Target variable\n",
    "\n",
    "#extended set comb \n",
    "x_label_adds_comb = training_set_label_adds_comb[features_label_comb] # Features\n",
    "y_label_adds_comb = training_set_label_adds_comb.Target # Target variable\n",
    "\n",
    "#even set\n",
    "x_label_even = training_set_label_even[features_label] # Features\n",
    "y_label_even = training_set_label_even.Target # Target variable\n",
    "\n",
    "#even set desc only \n",
    "training_set_even_desc = training_set_label_even.copy()\n",
    "training_set_even_desc.drop(['Title', 'Description', 'URL', 'Title_cat', 'URL_cat'], inplace=True, axis=1)\n",
    "x_label_even_desc = training_set_even_desc[single_feat_label] # Features\n",
    "y_label_even_desc = training_set_even_desc.Target # Target variable\n",
    "\n",
    "#even set comb \n",
    "x_label_even_comb = training_set_label_even_comb[features_label_comb] # Features\n",
    "y_label_even_comb = training_set_label_even_comb.Target # Target variable\n",
    "\n",
    "#extended even set\n",
    "x_label_even_adds = training_set_label_adds_even[features_label] # Features\n",
    "y_label_even_adds = training_set_label_even.Target # Target variable\n",
    "\n",
    "#extended even set desc only \n",
    "training_set_even_adds_desc = training_set_label_adds_even.copy()\n",
    "training_set_even_adds_desc.drop(['Title', 'Description', 'URL', 'Title_cat', 'URL_cat'], inplace=True, axis=1)\n",
    "x_label_even_adds_desc = training_set_even_adds_desc[single_feat_label] # Features\n",
    "y_label_even_adds_desc = training_set_even_adds_desc.Target # Target variable\n",
    "\n",
    "#extended even set comb \n",
    "x_label_even_adds_comb = training_set_label_adds_even_comb[features_label_comb] # Features\n",
    "y_label_even_adds_comb = training_set_label_adds_even_comb.Target # Target variable\n",
    "\n",
    "#BACKWARDSDIFFERENCE\n",
    "\n",
    "#base set \n",
    "x_backwards = training_set_ce.iloc[:,0:1825]\n",
    "y_backwards = training_set_ce.Target\n",
    "\n",
    "#base set desc only \n",
    "x_backwards_desc = training_set_ce_desc.iloc[:,1:611]\n",
    "y_backwards_desc = training_set_ce_desc.Target\n",
    "\n",
    "#base set comb \n",
    "x_backwards_comb = training_set_comb_ce.iloc[:,2:627]\n",
    "y_backwards_comb = training_set_comb_ce.Target\n",
    "\n",
    "#extended set \n",
    "x_backwards_adds = training_set_adds_ce.iloc[:,0:2293]\n",
    "y_backwards_adds = training_set_adds_ce.Target\n",
    "\n",
    "#extended set desc only \n",
    "x_backwards_adds_desc = training_set_adds_ce_desc.iloc[:,1:766]\n",
    "y_backwards_adds_desc = training_set_adds_ce_desc.Target\n",
    "\n",
    "#extended comb\n",
    "x_backwards_adds_comb = training_set_adds_comb_ce.iloc[:,2:782]\n",
    "y_backwards_adds_comb = training_set_adds_comb_ce.Target\n",
    "\n",
    "#even set \n",
    "x_backwards_even = training_set_ce_even.iloc[:,0:752]\n",
    "y_backwards_even = training_set_ce_even.Target\n",
    "\n",
    "#even set desc only\n",
    "x_backwards_even_desc = training_set_ce_even_desc.iloc[:,1:252]\n",
    "y_backwards_even_desc = training_set_ce_even_desc.Target\n",
    "\n",
    "#even set comb\n",
    "x_backwards_even_comb = training_set_ce_even_comb.iloc[:,1:256]\n",
    "y_backwards_even_comb = training_set_ce_even_comb.Target\n",
    "\n",
    "#extended even set \n",
    "x_backwards_even_adds = training_set_ce_even_extended.iloc[:,0:1604]\n",
    "y_backwards_even_adds = training_set_ce_even_extended.Target\n",
    "\n",
    "#extended even set desc only\n",
    "x_backwards_even_adds_desc = training_set_ce_even_extended_desc.iloc[:,1:536]\n",
    "y_backwards_even_adds_desc = training_set_ce_even_extended_desc.Target\n",
    "\n",
    "#extended even set comb\n",
    "x_backwards_even_adds_comb = training_set_ce_even_extended_comb .iloc[:,1:544]\n",
    "y_backwards_even_adds_comb = training_set_ce_even_extended_comb .Target\n",
    "\n",
    "#TFIDF\n",
    "\n",
    "#base set \n",
    "x_tfidf = training_set_tfidf\n",
    "y_tfidf = training_set['Target'].values\n",
    "\n",
    "#base set comb \n",
    "x_tfidf_comb = training_set_comb_tfidf\n",
    "y_tfidf_comb = training_set['Target'].values\n",
    "\n",
    "#extended set \n",
    "x_adds_tfidf = training_set_adds_tfidf\n",
    "y_adds_tfidf = training_set_adds['Target'].values\n",
    "\n",
    "#extended set comb \n",
    "x_adds_tfidf_comb = training_set_adds_comb_tfidf\n",
    "y_adds_tfidf_comb = training_set_adds['Target'].values\n",
    "\n",
    "#even set \n",
    "x_tfidf_even = training_set_tfidf_even \n",
    "y_tfidf_even = training_set_even['Target'].values\n",
    "\n",
    "#even set comb \n",
    "x_tfidf_even_comb = training_set_even_comb_tfidf\n",
    "y_tfidf_even_comb = training_set_even['Target'].values\n",
    "\n",
    "#extended even set  \n",
    "x_tfidf_even_adds = training_set_tfidf_even_adds\n",
    "y_tfidf_even_adds = training_set_even_adds['Target'].values\n",
    "\n",
    "#extended even set comb\n",
    "x_tfidf_even_adds_comb = training_set_even_adds_comb_tfidf\n",
    "y_tfidf_even_adds_comb = training_set_adds_comb_even['Target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: \n",
    "- Run all variations of the training set on two different types of Logistic Regression: cross eval and train/test, packaged as functions.\n",
    "- To see the results directly, check this spreadsheet https://docs.google.com/spreadsheets/d/1EntGAyJPffpIe_zRlXSpwujFLq1kyLQLJyS_udQsRcE/edit#gid=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Reg crossval function\n",
    "def lr(x,y,title):  \n",
    "    \"\"\" logistic regression\"\"\"\n",
    "    model = LogisticRegression(solver='liblinear', C=10.0,random_state=44)\n",
    "    y_pred = cross_val_predict(model, x, y, cv=5)\n",
    "    acc = cross_val_score(model, x, y, cv=5, scoring='precision')    \n",
    "    report = classification_report(y, y_pred)\n",
    "    return print(f'{title}\\n''MEAN PRECISION', np.mean(acc), 'report:', report, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordinal Encoding results with crossval\n",
    "print(lr(x_ord, y_ord, 'Base Set Ordinal:'), lr(x_ord_desc, y_ord_desc, 'Base Set Ordinal Desc Only:'), lr(x_ord_comb, y_ord_comb, 'Base Set Ordinal Combined:'), lr(x_ord_adds, y_ord_adds, 'Extended Set Ordinal:'), lr(x_ord_adds_desc, y_ord_adds_desc, 'Extended Set Ordinal Desc Only:'), lr(x_ord_adds_comb, y_ord_adds_comb, 'Extended Set Ordinal Combined:'), lr(x_ord_even, y_ord_even, 'Even Set Ordinal:'), lr(x_ord_even_desc, y_ord_even_desc, 'Even Set Ordinal Desc Only:'), lr(x_ord_even_comb, y_ord_even_comb, 'Even Set Combined Ordinal:'), lr(x_ord_even_adds, y_ord_even_adds, 'Extended Even Set Ordinal:'), lr(x_ord_even_adds_desc, y_ord_even_adds_desc, 'Extended Even Set Ordinal Desc Only:'), lr(x_ord_even_adds_comb, y_ord_even_adds_comb, 'Extended Even Set Combined Ordinal:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding results with crossval\n",
    "print(lr(x_label, y_label, 'Base Set Label:'), lr(x_label_desc, y_label_desc, 'Base Set Label Desc Only:'), lr(x_label_comb, y_label_comb, 'Base Set  Label Combined:'), lr(x_label_adds, y_label_adds, 'Extended Set Label:'), lr(x_label_adds_desc, y_label_adds_desc, 'Extended Set Label Desc Only:'), lr(x_label_adds_comb, y_label_adds_comb, 'Extended Set Label Combined:'), lr(x_label_even, y_label_even, 'Even Set Label:'), lr(x_label_even_desc, y_label_even_desc, 'Even Set Label Desc Only:'), lr(x_label_even_comb, y_label_even_comb, 'Even Set Combined Label:'), lr(x_label_even_adds, y_label_even_adds, 'Extended Even Set Label:'), lr(x_label_even_adds_desc, y_label_even_adds_desc, 'Extended Even Set Label Desc Only:'), lr(x_label_even_adds_comb, y_label_even_adds_comb, 'Extended Even Set Combined Label:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BD Encoding results with crossval\n",
    "print(lr(x_backwards, y_backwards, 'Base Set BD:'), lr(x_backwards_desc, y_backwards_desc, 'Base Set BD Desc Only:'), lr(x_backwards_comb, y_backwards_comb, 'Base Set BD Combined:'), lr(x_backwards_adds, y_backwards_adds, 'Extended Set BD:'), lr(x_backwards_adds_desc, y_backwards_adds_desc, 'Extended Set BD Desc Only:'), lr(x_backwards_adds_comb, y_backwards_adds_comb, 'Extended Set BD Combined:'), lr(x_backwards_even, y_backwards_even, 'Even Set BD:'), lr(x_backwards_even_desc, y_backwards_even_desc, 'Even Set BD Desc Only:'), lr(x_backwards_even_comb, y_backwards_even_comb, 'Even Set Combined BD:'), lr(x_backwards_even_adds, y_backwards_even_adds, 'Extended Even Set BD:'), lr(x_backwards_even_adds_desc, y_backwards_even_adds_desc, 'Extended Even Set BD Desc Only:'), lr(x_backwards_even_adds_comb, y_backwards_even_adds_comb, 'Extended Even Set Combined BD:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf Encoding results with crossval\n",
    "print(lr(x_tfidf, y_tfidf, 'Base Set TF:'), lr(x_tfidf_comb, y_tfidf_comb, 'Base Set TF Combined:'), lr(x_adds_tfidf, y_adds_tfidf, 'Extended Set TF:'), lr(x_adds_tfidf_comb, y_adds_tfidf_comb, 'Extended Set TF Combined:'), lr(x_tfidf_even, y_tfidf_even, 'Even Set TF:'), lr(x_tfidf_even_comb, y_tfidf_even_comb, 'Even Set TF Combined:'), lr(x_tfidf_even_adds, y_tfidf_even_adds, 'Extended Even Set TF:'), lr(x_tfidf_even_adds_comb, y_tfidf_even_adds_comb, 'Extended Even Set TF Combined:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogReg train/test function\n",
    "\n",
    "# import required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "\n",
    "def lr_training(x,y,title):\n",
    "    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.33,random_state=0)\n",
    "    logreg = LogisticRegression(solver='liblinear')\n",
    "    # fit the model with data\n",
    "    logreg.fit(x_train,y_train)\n",
    "    y_pred=logreg.predict(x_test)\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    # create heatmap for cfn matrix \n",
    "    class_names=[0,1] # name  of classes\n",
    "    fig, ax = plt.subplots()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    # create heatmap\n",
    "    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.tight_layout()\n",
    "    plt.title(f'Confusion matrix:{title}', y=1.1)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return print(f'{title}\\n' \"Accuracy:\", accuracy_score(y_test, y_pred), \"\\nPrecision:\", precision_score(y_test, y_pred), \"\\nRecall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordinal encoding results w/ train/test \n",
    "print(lr_training(x_ord, y_ord, 'Base Set Ordinal:'), lr_training(x_ord_desc, y_ord_desc, 'Base Set Ordinal Desc Only:'), lr_training(x_ord_comb, y_ord_comb, 'Base Set Ordinal Combined:'), lr_training(x_ord_adds, y_ord_adds, 'Extended Set Ordinal:'), lr_training(x_ord_adds_desc, y_ord_adds_desc, 'Extended Set Ordinal Desc Only:'), lr_training(x_ord_adds_comb, y_ord_adds_comb, 'Extended Set Ordinal Combined:'), lr_training(x_ord_even, y_ord_even, 'Even Set Ordinal:'), lr_training(x_ord_even_desc, y_ord_even_desc, 'Even Set Ordinal Desc Only:'), lr_training(x_ord_even_comb, y_ord_even_comb, 'Even Set Combined Ordinal:'), lr_training(x_ord_even_adds, y_ord_even_adds, 'Extended Even Set Ordinal:'), lr_training(x_ord_even_adds_desc, y_ord_even_adds_desc, 'Extended Even Set Ordinal Desc Only:'), lr_training(x_ord_even_adds_comb, y_ord_even_adds_comb, 'Extended Even Set Combined Ordinal:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding results w/ train/test\n",
    "print(lr_training(x_label, y_label, 'Base Set Label:'), lr_training(x_label_desc, y_label_desc, 'Base Set Label Desc Only:'), lr_training(x_label_comb, y_label_comb, 'Base Set  Label Combined:'), lr_training(x_label_adds, y_label_adds, 'Extended Set Label:'), lr_training(x_label_adds_desc, y_label_adds_desc, 'Extended Set Label Desc Only:'), lr_training(x_label_adds_comb, y_label_adds_comb, 'Extended Set Label Combined:'), lr_training(x_label_even, y_label_even, 'Even Set Label:'), lr_training(x_label_even_desc, y_label_even_desc, 'Even Set Label Desc Only:'), lr_training(x_label_even_comb, y_label_even_comb, 'Even Set Combined Label:'), lr_training(x_label_even_adds, y_label_even_adds, 'Extended Even Set Label:'), lr_training(x_label_even_adds_desc, y_label_even_adds_desc, 'Extended Even Set Label Desc Only:'), lr_training(x_label_even_adds_comb, y_label_even_adds_comb, 'Extended Even Set Combined Label:')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BD Encoding results with crossval\n",
    "print(lr_training(x_backwards, y_backwards, 'Base Set BD:'), lr_training(x_backwards_desc, y_backwards_desc, 'Base Set BD Desc Only:'), lr_training(x_backwards_comb, y_backwards_comb, 'Base Set BD Combined:'), lr_training(x_backwards_adds, y_backwards_adds, 'Extended Set BD:'), lr_training(x_backwards_adds_desc, y_backwards_adds_desc, 'Extended Set BD Desc Only:'), lr_training(x_backwards_adds_comb, y_backwards_adds_comb, 'Extended Set BD Combined:'), lr_training(x_backwards_even, y_backwards_even, 'Even Set BD:'), lr_training(x_backwards_even_desc, y_backwards_even_desc, 'Even Set BD Desc Only:'), lr_training(x_backwards_even_comb, y_backwards_even_comb, 'Even Set Combined BD:'), lr_training(x_backwards_even_adds, y_backwards_even_adds, 'Extended Even Set BD:'), lr_training(x_backwards_even_adds_desc, y_backwards_even_adds_desc, 'Extended Even Set BD Desc Only:'), lr_training(x_backwards_even_adds_comb, y_backwards_even_adds_comb, 'Extended Even Set Combined BD:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf Encoding results with crossval\n",
    "print(lr_training(x_tfidf, y_tfidf, 'Base Set TF:'), lr_training(x_tfidf_comb, y_tfidf_comb, 'Base Set TF Combined:'), lr_training(x_adds_tfidf, y_adds_tfidf, 'Extended Set TF:'), lr_training(x_adds_tfidf_comb, y_adds_tfidf_comb, 'Extended Set TF Combined:'), lr_training(x_tfidf_even, y_tfidf_even, 'Even Set TF:'), lr_training(x_tfidf_even_comb, y_tfidf_even_comb, 'Even Set TF Combined:'), lr_training(x_tfidf_even_adds, y_tfidf_even_adds, 'Extended Even Set TF:'), lr_training(x_tfidf_even_adds_comb, y_tfidf_even_adds_comb, 'Extended Even Set TF Combined:')) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
