{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- how to split the function into two: modelling function, prediction function. It's difficult because you also need to carry over the counts and transforms. Think the answer might be in the log reg classifier tutorial? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports + path\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import pickle\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "path = '/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on LogReg evaluations of the different training sets we focus on three variations for potential final model/predictions: extended set, even set, and extended even set using description only as features. These are the variations that gave us the best precision on negatives during model testing, which should help us to have a model somewhat adept at removing what we don't want from our results. \n",
    "- For the encoding we focus on Tf-IDF as I'm not yet 100% sure if BD encoding is being used correctly, however if it is then there is a case for using it with combined features as it performs better with them. \n",
    "- After some early tests switched the sklearn function to LogisticRegressionCV as it seems to perform better, esp w/ 2 for number of CV folds and average_precision or f1 as scoring parameter. If scoring is set to precision it behaves more drastically but this can be compensated with higher CV values. Re-running model testing w/ LogRegCV shows higher precision on negatives under all set variations using tf-idf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD TRAINING DFs \n",
    "\n",
    "#base DFs \n",
    "training_set_adds = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_extended.pkl')\n",
    "training_set_even = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even.pkl')\n",
    "training_set_even_adds = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended.pkl')\n",
    "\n",
    "#LOAD BD DFs \n",
    "\n",
    "#BD sets \n",
    "training_set_adds_ce_desc = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_extended_BD_desc.pkl')\n",
    "training_set_ce_even_desc = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_BD_desc.pkl')\n",
    "training_set_ce_even_extended_desc = pd.read_pickle(path+'LOGREG_RELEVANCE/trainingset_even_extended_BD_desc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD PREDICTION DFs\n",
    "\n",
    "#additions to base set (164)\n",
    "predictions_1 = pd.read_pickle(path+'LOGREG_RELEVANCE/base_prediction_set.pkl') \n",
    "\n",
    "#test results from twitter (20)\n",
    "predictions_2 = pd.read_csv('/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/TWITTER_SEARCHES/twitter_test.csv', encoding='iso-8859-1')\n",
    "\n",
    "#test results from github (104)\n",
    "prediction_github_1 = pd.read_pickle(path+'GH_PICKLES/music_archive.pkl')\n",
    "prediction_github_2 = pd.read_pickle(path+'GH_PICKLES/digital_score.pkl')\n",
    "prediction_github_3 = pd.read_pickle(path+'GH_PICKLES/library_music.pkl')\n",
    "prediction_github_4 = pd.read_pickle(path+'GH_PICKLES/oral_history.pkl')\n",
    "predictions_3 = pd.concat([prediction_github_1, prediction_github_2, prediction_github_3, prediction_github_4]).reset_index(drop=True)\n",
    "predictions_3 = predictions_3.dropna(how='any').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICTION FUNCTION W/ LOGREGCV \n",
    "\n",
    "def lr_model_predict(t_input, t_feature, target, cv_int, score_type, p_input, p_feature, filename, path):\n",
    "    count_vect = CountVectorizer()\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    x_count = count_vect.fit_transform(t_input[t_feature])\n",
    "    x_train = tfidf_transformer.fit_transform(x_count)\n",
    "    y_train = t_input[target].values\n",
    "    model = LogisticRegressionCV(solver='liblinear', random_state=44, cv=cv_int, scoring=score_type)\n",
    "    model.fit(x_train, y_train)\n",
    "    export = f'LOGREG_RELEVANCE/{filename}.sav'\n",
    "    pickle.dump(model, open(path+export, 'wb'))\n",
    "    x_new_count = count_vect.transform(p_input[p_feature])\n",
    "    x_new_train = tfidf_transformer.transform(x_new_count)\n",
    "    y_predict = model.predict(x_new_train)\n",
    "    scores = model.decision_function(x_new_train)\n",
    "    probability = model.predict_log_proba(x_new_train)\n",
    "    results = [r for r in y_predict]\n",
    "    result = p_input.copy()\n",
    "    result['Prediction'] = results\n",
    "    result['Score'] = [s for s in scores]\n",
    "    result['Probability'] = [p for p in probability]\n",
    "    result['Input Length'] = result['Description'].str.len()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def lr_model(t_input, t_feature, target, score_type, filename, path):\n",
    "    count_vect = CountVectorizer()\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    x_count = count_vect.fit_transform(t_input[t_feature])\n",
    "    x_train = tfidf_transformer.fit_transform(x_count)\n",
    "    y_train = t_input[target].values\n",
    "    model = LogisticRegressionCV(solver='liblinear', random_state=44, cv=2, scoring=score_type)\n",
    "    model.fit(x_train, y_train)\n",
    "    saved_model = f'LOGREG_RELEVANCE/{filename}_model.pkl'\n",
    "    vectorizer = f'LOGREG_RELEVANCE/{filename}_vectorizer.pkl'\n",
    "    pickle.dump(model, open(path+saved_model, 'wb'))\n",
    "    pickle.dump(vectorizer, open(path+vectorizer, 'wb'))\n",
    "\n",
    "def lr_predict(p_input, p_feature, filename, path):\n",
    "    model = pickle.load(open(path+f'LOGREG_RELEVANCE/{filename}_model.pkl', 'rb'))\n",
    "    vectorizer = pickle.load(open(path+f'LOGREG_RELEVANCE/{filename}_vectorizer.pkl', 'rb'))\n",
    "    x_new_count = vectorizer.transform(p_input[p_feature])\n",
    "    x_new_train = tfidf_transformer.transform(x_new_count)\n",
    "    y_predict = model.predict(x_new_train)\n",
    "    scores = model.decision_function(x_new_train, y_predict)\n",
    "    probability = model.predict_log_proba(x_new_train)\n",
    "    results = [r for r in y_predict]\n",
    "    result = p_input.copy()\n",
    "    result['Prediction'] = results\n",
    "    result['Score'] = [s for s in scores]\n",
    "    result['Probability'] = [p for p in probability]\n",
    "    return result \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict even set against extensions\n",
    "even_pred_1 = lr_model_predict(training_set_even, 'Description', 'Target', 2, 'f1_weighted', predictions_1, 'Description', 'extended_even_model', '/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/')\n",
    "even_pred_1 = even_pred_1.loc[even_pred_1['Prediction'] == 1]\n",
    "even_pred_1.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prediction set size is 164 and technically includes 20 positives (ismir list) and some negatives that could be positives (mji list). \n",
    "\n",
    "scoring and cv values:\n",
    "- precision returns no positives.\n",
    "- average_precision returns 16 positives w/ cv = 10, including 6 from ismir \n",
    "- precision_weighted w/ cv 2 returns 16 positives, same as above \n",
    "- f1 w/ cv 2 returns 44 positives, including all ismir -> drops when you increase the cv value \n",
    "- f1_weighted same as above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict even set against twitter tests \n",
    "even_pred_2 = lr_model_predict(training_set_even, 'Description', 'Target', 2, 'precision_weighted', predictions_2, 'Description', 'extended_even_model', '/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/')\n",
    "even_pred_2 = even_pred_2.loc[even_pred_2['Prediction'] == 1]\n",
    "even_pred_2.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prediction set size is 20 and has one entry that's a definite positive a few that are ambiguous. \n",
    "\n",
    "scoring and cv values (2, 5, 10):\n",
    "- precision (all cv values) returns 1 ambiguous \n",
    "- average_precision returns 1 ambigous at cv 2 + 5, 2 ambiguous and definite and wrong at cv 10\n",
    "- precision_weighted same as above for cv 10 but at all cv values \n",
    "- f1/f1_weighted (all cv values) return 5 positives, including the definite, the ambiguous and one wrong "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict even set against github tests \n",
    "even_pred_3 = lr_model_predict(training_set_even, 'Description', 'Target', 2, 'f1_weighted', predictions_3, 'Description', 'extended_even_model', '/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/')\n",
    "even_pred_3 = even_pred_3.loc[even_pred_3['Prediction'] == 1]\n",
    "even_pred_3 = even_pred_3.sort_values(by='Score', ascending=False)\n",
    "even_pred_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prediction set size is 104 entries and TK TK. IT DOES HAVE V SMALL DESC COUNTS THO WHICH I THINK THROWS IT\n",
    "\n",
    "scoring and cv values (2, 5, 10):\n",
    "- precision returns 42 (2, 5) and 62 (10)\n",
    "- average_precision returns 62 (2, 5) and 87 (10)\n",
    "- precision_weighted returns 87 (all values)\n",
    "- f1/f1_weighted returns 93-94 (all values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict extended set against twitter tests \n",
    "even_pred_4 = lr_model_predict(training_set_adds, 'Description', 'Target', 10, 'precision_weighted', predictions_2, 'Description', 'extended_even_model', '/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/')\n",
    "even_pred_4 = even_pred_4.loc[even_pred_4['Prediction'] == 1]\n",
    "even_pred_4 = even_pred_4.sort_values(by='Score', ascending=False)\n",
    "even_pred_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scoring and cv values (2, 5, 10):\n",
    "- precision returns 6 positives including definite, ambiguous and 2 wrong \n",
    "- average_precision returns 6 at cv 2 but 20 at cv 5/10 (no good)\n",
    "- precision_weighted returns 6 positives including definite, ambiguous and 2 wrong \n",
    "- f1/f1_weighted returns 6 positives including definite, ambiguous and 2 wrong "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict extended even set against twitter tests \n",
    "even_pred_5 = lr_model_predict(training_set_even_adds, 'Description', 'Target', 2, 'f1_weighted', predictions_2, 'Description', 'extended_even_model', '/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/')\n",
    "even_pred_5 = even_pred_5.loc[even_pred_5['Prediction'] == 1]\n",
    "even_pred_5 = even_pred_5.sort_values(by='Score', ascending=False)\n",
    "even_pred_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scoring and cv values (2, 5, 10):\n",
    "- precision (all cv values) returns nothing\n",
    "- average_precision returns 1 def, 2 ambiguous, 1 wrong at all cv values w/ the wrong scoring highest \n",
    "- precision_weighted same as above but only 3 results at cv 2, 5 (dropping one ambiguous)\n",
    "- f1/f1_weighted same as av precision w/ 3 results for cv 2 on weighted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict extended even set against github tests \n",
    "even_pred_6 = lr_model_predict(training_set_even_adds, 'Description', 'Target', 2, 'precision', predictions_3, 'Description', 'extended_even_model', '/Users/laurentfintoni/Desktop/University/COURSE DOCS/THESIS/Internship/musow-pipeline/')\n",
    "even_pred_6 = even_pred_6.loc[even_pred_6['Prediction'] == 1]\n",
    "even_pred_6 = even_pred_6.sort_values(by='Score', ascending=False)\n",
    "even_pred_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- precision (all cv values) returns 18 which is a lot better than using the even set only as model. this model w/ these values might work best for github searches? "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
